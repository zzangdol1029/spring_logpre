# 성능 개선 고급 방법

## 문제 상황

100만 개 데이터와 30 epochs로 학습해도 목표 성능에 도달하지 못하는 경우, 근본적인 개선이 필요합니다.

## 1. 데이터 품질 확인 및 개선

### 1.1 데이터 불균형 확인

```python
# 데이터 불균형 확인
import pandas as pd

train_df = pd.read_parquet('split_data/train.parquet')
test_df = pd.read_parquet('split_data/test.parquet')

print("Train 데이터:")
print(f"  정상: {(train_df['is_error'] == False).sum():,}개")
print(f"  에러: {(train_df['is_error'] == True).sum():,}개")
print(f"  비율: {(train_df['is_error'] == True).sum() / len(train_df) * 100:.2f}%")

print("\nTest 데이터:")
print(f"  정상: {(test_df['is_error'] == False).sum():,}개")
print(f"  에러: {(test_df['is_error'] == True).sum():,}개")
print(f"  비율: {(test_df['is_error'] == True).sum() / len(test_df) * 100:.2f}%")
```

**문제**: 정상 로그가 90% 이상이면 불균형이 심함

**해결책**:
- 정상 로그 샘플링 (언더샘플링)
- 에러 로그 증강 (오버샘플링)
- 클래스 가중치 조정

### 1.2 로그 분류 정확도 확인

현재 `is_error` 분류가 정확한지 확인:

```python
# 에러 로그가 실제로 에러인지 확인
error_logs = train_df[train_df['is_error'] == True]
print("에러 로그 샘플:")
for i, row in error_logs.head(10).iterrows():
    print(f"  {row['level']}: {row['message'][:100]}")
```

**문제**: 정상 로그가 에러로 잘못 분류되었을 수 있음

**해결책**: 분류 기준 재검토 및 수정

### 1.3 템플릿 다양성 확인

```python
from log_specific_anomaly_detectors import LogTemplateExtractor

parser = LogTemplateExtractor()
templates = [parser.extract_template(msg) for msg in train_df['message'].head(10000)]
unique_templates = len(set(templates))

print(f"고유 템플릿 수: {unique_templates}개")
print(f"템플릿 다양성: {unique_templates / len(templates) * 100:.2f}%")
```

**문제**: 템플릿이 너무 적으면 패턴 학습이 어려움

**해결책**: 더 다양한 로그 데이터 수집

---

## 2. 모델 구조 개선

### 2.1 DeepLog 하이퍼파라미터 튜닝

현재 설정:
- `embedding_dim=128`
- `hidden_dim=64`
- `num_layers=2`
- `sequence_length=10`

**개선 방안**:

```python
# 더 큰 모델
detector = DeepLogDetector(
    embedding_dim=256,      # 128 → 256
    hidden_dim=128,         # 64 → 128
    num_layers=3,           # 2 → 3
    sequence_length=15      # 10 → 15
)
```

**주의**: 메모리 사용량 증가

### 2.2 LogAnomaly 벡터 차원 증가

현재 설정:
- `vector_dim=100`

**개선 방안**:

```python
detector = LogAnomalyDetector(vector_dim=200)  # 100 → 200
```

### 2.3 LogRobust 모델 개선

현재 설정:
- `embedding_dim=128`
- `hidden_dim=64`
- `num_layers=2`

**개선 방안**:

```python
detector = LogRobustDetector(
    embedding_dim=256,      # 128 → 256
    hidden_dim=128,         # 64 → 128
    num_layers=3            # 2 → 3
)
```

---

## 3. 학습 파라미터 최적화

### 3.1 Learning Rate 조정

현재: `learning_rate=0.001`

**개선 방안**:
- Learning Rate Scheduler 사용
- 더 작은 Learning Rate (0.0005, 0.0001)
- Adaptive Learning Rate (AdamW)

### 3.2 Early Stopping

과적합 방지를 위해 Early Stopping 추가:

```python
# Early Stopping 구현
best_val_loss = float('inf')
patience = 5
no_improve = 0

for epoch in range(epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        no_improve = 0
        save_checkpoint()
    else:
        no_improve += 1
        if no_improve >= patience:
            print("Early stopping!")
            break
```

### 3.3 클래스 가중치 조정

데이터 불균형 해결:

```python
# 정상:에러 비율이 9:1인 경우
class_weights = torch.tensor([1.0, 9.0])  # 정상:에러
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

---

## 4. 시퀀스 길이 및 윈도우 크기 조정

### 4.1 시퀀스 길이 증가

현재: `sequence_length=10`

**개선 방안**:
- `sequence_length=15` 또는 `20`
- 더 긴 패턴 학습 가능
- 메모리 사용량 증가

### 4.2 시퀀스 길이별 성능 테스트

```python
for seq_len in [5, 10, 15, 20]:
    detector = DeepLogDetector(sequence_length=seq_len)
    # 학습 및 평가
    # 최적 시퀀스 길이 선택
```

---

## 5. 앙상블 방법

### 5.1 투표 기반 앙상블

3개 모델의 결과를 결합:

```python
def ensemble_vote(results):
    """2개 이상 모델이 탐지하면 이상으로 판단"""
    all_anomalies = {}
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        if not anomalies_df.empty and 'sequence_index' in anomalies_df.columns:
            all_anomalies[model_type] = set(anomalies_df['sequence_index'].values)
    
    # 2개 이상 모델이 탐지한 시퀀스
    from collections import Counter
    all_indices = []
    for indices in all_anomalies.values():
        all_indices.extend(indices)
    
    index_counts = Counter(all_indices)
    consensus_indices = [idx for idx, count in index_counts.items() if count >= 2]
    
    return consensus_indices
```

### 5.2 가중 평균 앙상블

모델별 성능에 따라 가중치 부여:

```python
def ensemble_weighted(results, weights):
    """
    모델별 가중 평균
    
    Args:
        results: 모델별 탐지 결과
        weights: 모델별 가중치 (예: {'deeplog': 0.4, 'loganomaly': 0.4, 'logrobust': 0.2})
    """
    weighted_scores = {}
    
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        weight = weights.get(model_type, 0.33)
        
        if not anomalies_df.empty and 'sequence_index' in anomalies_df.columns:
            for _, row in anomalies_df.iterrows():
                seq_idx = row['sequence_index']
                score = row.get('anomaly_score', 0)
                
                if seq_idx not in weighted_scores:
                    weighted_scores[seq_idx] = 0
                weighted_scores[seq_idx] += score * weight
    
    # 임계값 적용
    threshold = 0.5
    final_anomalies = [idx for idx, score in weighted_scores.items() if score > threshold]
    
    return final_anomalies
```

---

## 6. 목표 성능 현실적 조정

### 6.1 현재 데이터 특성에 맞는 목표 설정

로그 데이터의 특성상 완벽한 성능은 어려울 수 있음:

```bash
# 현실적인 목표
python log_specific_model_comparison.py \
    --load-split split_data \
    --optimize-threshold \
    --sample-size 1000000 \
    --epochs 30 \
    --batch-size 64 \
    --target-accuracy 0.60 \
    --target-precision 0.40 \
    --target-recall 0.50 \
    --target-f1 0.45 \
    --target-specificity 0.70
```

### 6.2 비즈니스 요구사항에 맞는 목표

- **보안 중요**: 재현율 우선 (이상을 놓치면 안 됨)
- **알림 최소화**: 정밀도 우선 (오탐 최소화)

---

## 7. 모델 아키텍처 변경

### 7.1 Transformer 기반 모델

현재 LSTM 대신 Transformer 사용:

```python
# Transformer 기반 모델 (구현 필요)
class LogTransformerDetector:
    def __init__(self):
        self.model = TransformerModel(
            d_model=256,
            nhead=8,
            num_layers=6
        )
```

### 7.2 AutoEncoder 기반

정상 패턴 재구성 오차 기반:

```python
# AutoEncoder로 정상 패턴 학습
# 재구성 오차가 크면 이상
class LogAutoEncoder:
    def __init__(self):
        self.encoder = Encoder(...)
        self.decoder = Decoder(...)
    
    def detect_anomaly(self, sequence):
        reconstructed = self.decoder(self.encoder(sequence))
        error = mse_loss(sequence, reconstructed)
        return error > threshold
```

---

## 8. 데이터 전처리 개선

### 8.1 로그 템플릿 정규화

더 정확한 템플릿 추출:

```python
def extract_template_improved(message):
    """개선된 템플릿 추출"""
    # 숫자, IP, 시간 등 정규화
    message = re.sub(r'\d+', '<NUM>', message)
    message = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', '<IP>', message)
    message = re.sub(r'\d{4}-\d{2}-\d{2}', '<DATE>', message)
    return message
```

### 8.2 특성 엔지니어링

로그 레벨, 시간 간격 등 추가 특성:

```python
def extract_features(logs_df):
    """추가 특성 추출"""
    features = []
    for i in range(len(logs_df) - 1):
        time_diff = (logs_df.iloc[i+1]['timestamp'] - logs_df.iloc[i]['timestamp']).total_seconds()
        level = logs_df.iloc[i]['level']
        
        features.append({
            'time_diff': time_diff,
            'level': level,
            'message_length': len(logs_df.iloc[i]['message'])
        })
    return features
```

---

## 9. 단계별 개선 전략

### 단계 1: 빠른 진단 (1시간)

```bash
# 데이터 품질 확인
python check_data_quality.py

# 모델별 기본 성능 확인
python log_specific_model_comparison.py \
    --load-split split_data \
    --sample-size 100000 \
    --epochs 5
```

### 단계 2: 하이퍼파라미터 튜닝 (2-3시간)

```bash
# 시퀀스 길이별 테스트
for seq_len in 10 15 20; do
    python test_sequence_length.py --sequence-length $seq_len
done

# Learning Rate 조정
for lr in 0.001 0.0005 0.0001; do
    python test_learning_rate.py --learning-rate $lr
done
```

### 단계 3: 모델 구조 개선 (4-6시간)

```python
# 더 큰 모델로 학습
detector = DeepLogDetector(
    embedding_dim=256,
    hidden_dim=128,
    num_layers=3,
    sequence_length=15
)
```

### 단계 4: 앙상블 적용 (1-2시간)

```python
# 3개 모델 결과 결합
ensemble_results = ensemble_vote(all_results)
```

---

## 10. 최종 권장사항

### 우선순위 1: 데이터 품질 개선
- 정상/에러 로그 분류 정확도 확인
- 데이터 불균형 해결
- 더 다양한 데이터 수집

### 우선순위 2: 하이퍼파라미터 튜닝
- 시퀀스 길이 조정 (10 → 15 또는 20)
- Learning Rate 조정
- 모델 크기 증가

### 우선순위 3: 앙상블 방법
- 3개 모델 결과 결합
- 가중 평균 사용

### 우선순위 4: 목표 성능 조정
- 현실적인 목표 설정
- 비즈니스 요구사항에 맞게 조정

---

## 11. 성능이 여전히 부족한 경우

### 대안 1: 규칙 기반 필터링 추가

```python
# 모델 결과 + 규칙 기반 필터링
def filter_by_rules(anomalies_df, logs_df):
    """규칙 기반으로 오탐 제거"""
    filtered = []
    for _, row in anomalies_df.iterrows():
        seq_idx = row['sequence_index']
        sequence_logs = logs_df.iloc[seq_idx:seq_idx+10]
        
        # 규칙: ERROR 레벨이 3개 이상이면 확실한 이상
        error_count = (sequence_logs['level'] == 'ERROR').sum()
        if error_count >= 3:
            filtered.append(row)
    
    return pd.DataFrame(filtered)
```

### 대안 2: 전문가 검토

- 탐지된 이상을 전문가가 검토
- False Positive 패턴 분석
- 규칙 추가

### 대안 3: 다른 접근 방법

- 통계적 방법 (Isolation Forest, LOF)
- 규칙 기반 방법
- 외부 도구 활용 (ELK Stack, Splunk)

---

## 12. 체크리스트

성능 개선을 위한 체크리스트:

- [ ] 데이터 불균형 확인 및 해결
- [ ] 로그 분류 정확도 확인
- [ ] 템플릿 다양성 확인
- [ ] 시퀀스 길이 조정 (10 → 15, 20)
- [ ] Learning Rate 조정
- [ ] 모델 크기 증가
- [ ] Early Stopping 적용
- [ ] 클래스 가중치 조정
- [ ] 앙상블 방법 적용
- [ ] 목표 성능 현실적 조정
- [ ] 규칙 기반 필터링 추가

---

## 결론

100만 개 데이터와 30 epochs로도 성능이 나오지 않으면:

1. **데이터 품질 문제**일 가능성이 높음
2. **모델 구조 개선** 필요
3. **앙상블 방법**으로 성능 향상
4. **목표 성능을 현실적으로 조정**

가장 효과적인 방법은 **데이터 품질 개선**과 **앙상블 방법**입니다.

