# LogAnomaly 이상 탐지 실행 가이드

## 🚀 빠른 시작

### 기본 실행 (메모리 최적화 적용)

```bash
cd /Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog

python log_specific_anomaly_detectors.py
```

---

## 📊 메모리 최적화 설정

### 자동 샘플링 적용

```python
# 학습 데이터: 최대 500,000개 (정상 로그)
# 테스트 데이터: 최대 100,000개 (정상 80K + 에러 20K)

예상 메모리 사용량:
  - 학습 시퀀스: ~500K × 15 × 100 × 4 bytes = 약 2.8 GB
  - TensorFlow 오버헤드: ~1-2 GB
  - 총 예상: 4-5 GB ✅
```

### 기존 vs 최적화

| 항목 | 기존 (26M 로그) | 최적화 (500K) | 비율 |
|------|----------------|--------------|------|
| 학습 데이터 | 26,506,380개 | 500,000개 | 1/53 |
| 예상 메모리 | ~312 GB | ~5 GB | 1/62 |
| 학습 시간 | 예상 불가능 | 5-10분 | - |
| 실행 가능성 | ❌ OOM | ✅ 가능 | - |

---

## 🔧 수동 샘플링 조정 (필요 시)

소스 코드에서 수동으로 조정 가능:

```python
# log_specific_anomaly_detectors.py, line ~1186

# 더 작게 조정 (메모리가 여전히 부족할 경우)
max_train_samples = 300000  # 기본: 500000

# 더 크게 조정 (메모리가 충분한 경우)
max_train_samples = 1000000  # 기본: 500000
```

---

## 📁 실행 결과

### 출력 파일 위치

```
pattern/prelog/results/loganomaly_risk_analysis/
├── anomalies_with_risk.csv      # 전체 이상 탐지 결과 (위험도 포함)
├── risk_critical.csv             # CRITICAL 위험 이상 (80+ 점)
├── risk_high.csv                 # HIGH 위험 이상 (60-79 점)
├── risk_medium.csv               # MEDIUM 위험 이상 (40-59 점)
├── risk_low.csv                  # LOW 위험 이상 (20-39 점)
├── risk_info.csv                 # INFO 위험 이상 (0-19 점)
└── risk_summary.txt              # 위험도 분석 요약 리포트
```

### 학습 로그 위치

```
pattern/logs/training/
└── loganomaly_training_YYYYMMDD_HHMMSS.log
```

---

## 📈 실행 단계

### 1단계: 로그 파싱
```
로그 디렉토리: logs/backup
파싱 대상: 모든 로그 파일
출력: 전체 로그 DataFrame
```

### 2단계: 데이터 준비 (메모리 최적화)
```
✅ 정상 로그 샘플링: 최대 500K
✅ 학습/테스트 분리: 80% / 20%
✅ 테스트 데이터 샘플링: 정상 80K + 에러 20K
```

### 3단계: 모델 학습 (LogAnomaly)
```
✅ 템플릿 추출
✅ TF-IDF 벡터화
✅ 정상 패턴 학습 (통계 기반)
```

### 4단계: 이상 탐지 및 심각도 평가
```
✅ Z-score 기반 이상 탐지
✅ 로그 레벨 기반 심각도 평가
✅ 예외 타입 분석
```

### 5단계: 위험도 분석
```
✅ 이상 점수 + 심각도 → 위험도 (0-100)
✅ 위험도 레벨 분류 (CRITICAL/HIGH/MEDIUM/LOW/INFO)
```

### 6단계: 결과 저장
```
✅ CSV 파일 (위험도별)
✅ 요약 리포트 (TXT)
```

---

## 🎯 예상 실행 시간

### MacBook (CPU) 기준

| 단계 | 예상 시간 | 메모리 |
|------|-----------|--------|
| 1. 로그 파싱 | 2-3분 | 2-3 GB |
| 2. 데이터 준비 | 10-20초 | 1 GB |
| 3. 모델 학습 | 3-5분 | 3-4 GB |
| 4. 이상 탐지 | 1-2분 | 2 GB |
| 5. 위험도 분석 | 10-20초 | 0.5 GB |
| **총 예상** | **7-11분** | **피크 5 GB** |

---

## 🚨 메모리 부족 시 대응

### 증상
```
zsh: killed     python log_specific_anomaly_detectors.py
```

### 해결 방법

#### 1. 학습 데이터 더 줄이기
```python
max_train_samples = 300000  # 500000 → 300000
```

#### 2. 테스트 데이터 줄이기
```python
max_test_normal = 50000     # 80000 → 50000
max_test_error = 10000      # 20000 → 10000
```

#### 3. 로그 파싱 시 제한
```python
logs_df = parser.parse_directory(
    log_directory,
    max_files=10,           # 파일 수 제한 추가
    sample_lines=1000000    # 총 라인 수 제한
)
```

---

## 📊 결과 해석

### 위험도 점수 (0-100)

```
위험도 = (이상 점수 × 60%) + (심각도 점수 × 40%)

이상 점수:
  - LogAnomaly의 Z-score 기반 이상도
  - 정상 패턴과의 거리

심각도 점수:
  - 로그 레벨: ERROR (80), WARN (60), INFO (20)
  - 예외 타입: Critical (100), High (80), Medium (60)
```

### 위험도 레벨

| 레벨 | 점수 범위 | 의미 | 대응 |
|------|-----------|------|------|
| **CRITICAL** | 80-100 | 즉시 조치 필요 | 즉시 알림 |
| **HIGH** | 60-79 | 높은 우선순위 | 당일 처리 |
| **MEDIUM** | 40-59 | 중간 우선순위 | 주간 리뷰 |
| **LOW** | 20-39 | 낮은 우선순위 | 월간 리뷰 |
| **INFO** | 0-19 | 정보성 | 기록만 |

---

## 🔍 실행 예시 출력

```
======================================================================
LogAnomaly 기반 이상 탐지 및 위험도 분석 시스템
======================================================================

✅ 사용 모델: LOGANOMALY (성능 측정 선정 모델)

1단계: 로그 파일 파싱
✅ 33,132,975개 로그 라인 파싱 완료

⚠️ 메모리 최적화: 정상 로그 26,506,380개 → 500,000개로 샘플링
   샘플링 완료: 500,000개

⚠️ 테스트 정상 로그 샘플링: 100,000개 → 80,000개
⚠️ 테스트 에러 로그 샘플링: 6,626,595개 → 20,000개

📊 데이터 준비 완료:
   학습 데이터: 400,000개 (정상 로그)
   테스트 데이터: 100,000개 (정상: 80,000개, 에러: 20,000개)

💾 예상 메모리 사용량: 2.24 GB

2단계: LOGANOMALY 시스템 초기화

3단계: 모델 학습
⚡ 빠른 학습 설정: Epochs=10, Batch Size=32
2025-12-09 09:00:00 - INFO - 템플릿 벡터 구축 중...
2025-12-09 09:01:30 - INFO - ✅ 템플릿 벡터 구축 완료: 45,231개 템플릿
2025-12-09 09:01:30 - INFO - 정상 시퀀스 생성 중...
2025-12-09 09:03:45 - INFO - ✅ 정상 시퀀스 생성 완료: 399,986개 시퀀스
✅ 모델 학습 완료

4단계: 이상 탐지 및 심각도 평가
   테스트 데이터 분석 중: 100,000개 로그...
✅ 1,234개 이상 시퀀스 탐지

5단계: 위험도 분석
======================================================================
위험도 분석 리포트
======================================================================

전체 통계:
  - 총 이상 탐지: 1,234개
  - 평균 위험도 점수: 42.5/100
  - 최고 위험도 점수: 95.2/100

위험도 분포:
  - CRITICAL  :    12개 ( 1.0%)
  - HIGH      :    56개 ( 4.5%)
  - MEDIUM    :   234개 (19.0%)
  - LOW       :   456개 (37.0%)
  - INFO      :   476개 (38.6%)

💾 전체 이상 탐지 결과 저장: .../anomalies_with_risk.csv
💾 CRITICAL 위험 이상 저장: .../risk_critical.csv (12개)
💾 HIGH 위험 이상 저장: .../risk_high.csv (56개)
💾 MEDIUM 위험 이상 저장: .../risk_medium.csv (234개)
💾 LOW 위험 이상 저장: .../risk_low.csv (456개)
💾 INFO 위험 이상 저장: .../risk_info.csv (476개)
💾 위험도 요약 리포트 저장: .../risk_summary.txt

======================================================================
✅ 이상 탐지 및 위험도 분석 완료!
======================================================================

📁 결과 저장 위치: .../loganomaly_risk_analysis
   - 전체 이상 탐지: 1,234개
   - CRITICAL 위험: 12개
   - HIGH 위험: 56개
   - MEDIUM 위험: 234개
```

---

## 🛠️ 고급 사용법

### 1. 특정 로그 디렉토리만 분석

```python
# main() 함수에서 수정
log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup/specific_date"
```

### 2. 모델 파라미터 조정

```python
# main() 함수에서 수정
system.train(
    train_ratio=1.0,
    epochs=10,          # LogAnomaly에서는 무시됨
    batch_size=32       # LogAnomaly에서는 무시됨
)
```

### 3. 위험도 임계값 조정

```python
# analyze_risk_level() 함수에서 수정
def get_risk_level(risk_score):
    if risk_score >= 90:      # 기본: 80
        return 'CRITICAL'
    elif risk_score >= 70:    # 기본: 60
        return 'HIGH'
    # ...
```

---

## 📝 주요 변경 사항

### OOM 방지 최적화 (2025-12-09)

1. **학습 데이터 자동 샘플링**: 최대 500K
2. **테스트 데이터 자동 샘플링**: 최대 100K
3. **메모리 예상치 계산**: 실행 전 경고
4. **균등 샘플링**: 시간 순서 유지하며 샘플링

### 개선 효과

```
Before (OOM):
  - 학습 데이터: 26.5M
  - 예상 메모리: 312 GB
  - 상태: ❌ zsh: killed

After (최적화):
  - 학습 데이터: 500K
  - 예상 메모리: 5 GB
  - 상태: ✅ 실행 가능
```

---

## 🎓 참고 문서

- 모델 선정 과정: `LogAnomaly_모델_선정_과정_분석.md`
- 실험 결과 비교: `실험_결과_시각화_비교.md`
- 모델 선정 요약: `모델_선정_요약.md`

---

## 🚀 다음 단계

### 실시간 모니터링 구축
```
1. 로그 파일 와처 구현
2. 새 로그 발생 시 자동 분석
3. CRITICAL/HIGH 위험 시 알림
```

### 성능 개선
```
1. GPU 환경에서 실행 (RTX 5070 Ti)
2. 더 많은 학습 데이터 (1M+)
3. 앙상블 모델 (LogAnomaly + Isolation Forest)
```

### 대시보드 구축
```
1. Grafana/Kibana 연동
2. 실시간 위험도 차트
3. 이상 탐지 히스토리
```

---

## 💡 팁

1. **첫 실행은 빠른 검증**: 기본 설정으로 실행하여 동작 확인
2. **메모리 여유 확인**: Activity Monitor에서 메모리 사용량 모니터링
3. **CRITICAL/HIGH만 확인**: 대부분의 경우 이 두 레벨만 처리하면 충분
4. **정기 실행**: cron/launchd로 일일 분석 자동화
5. **결과 누적**: 시간에 따른 이상 탐지 트렌드 분석

---

## ❓ FAQ

### Q1. 메모리가 여전히 부족하면?
**A**: `max_train_samples`를 더 줄이세요 (300K → 200K).

### Q2. 학습 시간을 더 줄이려면?
**A**: LogAnomaly는 통계 기반이라 이미 충분히 빠릅니다. 파싱 시 `max_files` 제한이 효과적입니다.

### Q3. 실제 이상인데 탐지가 안 되면?
**A**: Z-score 임계값을 낮추세요 (`threshold=3.0` → `2.5`).

### Q4. 오탐지가 너무 많으면?
**A**: Z-score 임계값을 높이거나 CRITICAL/HIGH만 필터링하세요.

### Q5. 다른 모델을 사용하려면?
**A**: `model_type = 'deeplog'` 또는 `'logrobust'`로 변경 (권장하지 않음 - 성능 낮음).

