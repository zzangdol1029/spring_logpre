"""
ë¡œê·¸ íŠ¹í™” ì´ìƒ íƒì§€ ëª¨ë¸ êµ¬í˜„
- LogRobust: Attention + Bi-LSTM
- DeepLog: LSTM ê¸°ë°˜ ì‹¤í–‰ ê²½ë¡œ ì˜ˆì¸¡
- LogAnomaly: Template2Vec ê¸°ë°˜ ì˜ë¯¸ ë²¡í„°í™”
"""

import re
import os
import pickle
import pandas as pd
import numpy as np
from collections import Counter, defaultdict
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import warnings
import logging
import sys
import time
warnings.filterwarnings('ignore')

# Deep Learning ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒì )
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì • (MPS > CUDA > CPU ìˆœìœ¼ë¡œ ì„ íƒ)
    if torch.backends.mps.is_available():
        DEVICE = torch.device("mps")
        print("âœ… Apple Silicon GPU (MPS) ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
    elif torch.cuda.is_available():
        DEVICE = torch.device("cuda")
        print("âœ… NVIDIA GPU (CUDA) ì‚¬ìš© ê°€ëŠ¥ - GPU ê°€ì† í™œì„±í™”")
    else:
        DEVICE = torch.device("cpu")
        print("âš ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
except ImportError:
    TORCH_AVAILABLE = False
    DEVICE = None
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Deep Learning ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì„¤ì¹˜í•˜ì„¸ìš”: pip install torch")

# TransformersëŠ” LogRobustì—ì„œë§Œ í•„ìš”í•˜ë¯€ë¡œ ì§€ì—° import
# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ importí•˜ì§€ ì•ŠìŒ (TensorFlow ì˜ì¡´ì„± ë¬¸ì œ ë°©ì§€)
TRANSFORMERS_AVAILABLE = None  # Noneìœ¼ë¡œ ì´ˆê¸°í™”, ì‹¤ì œ ì‚¬ìš© ì‹œ í™•ì¸

from severity_assessment import SeverityAssessment


def setup_training_logger(model_name: str, log_dir: str = None) -> logging.Logger:
    """
    í•™ìŠµ ë¡œê·¸ë¥¼ ìœ„í•œ ë¡œê±° ì„¤ì •
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        log_dir: ë¡œê·¸ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì½˜ì†”ë§Œ)
        
    Returns:
        ì„¤ì •ëœ ë¡œê±°
    """
    logger = logging.getLogger(f'training_{model_name}')
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
    if logger.handlers:
        logger.handlers.clear()
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_format = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_format)
    logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ì„ íƒì )
    if log_dir:
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(
            log_dir, 
            f'{model_name}_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        )
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_format = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_format)
        logger.addHandler(file_handler)
        logger.info(f"ğŸ“ í•™ìŠµ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    return logger


class LogTemplateExtractor:
    """ë¡œê·¸ í…œí”Œë¦¿ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.templates = {}
        self.template_patterns = {}
        
    def extract_template(self, log_message: str) -> str:
        """
        ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í…œí”Œë¦¿ ì¶”ì¶œ
        
        ì˜ˆì‹œ:
        "Connection to database failed: timeout after 30 seconds"
        â†’ "Connection to database failed: timeout after * seconds"
        """
        # ìˆ«ì, IP ì£¼ì†Œ, íŒŒì¼ ê²½ë¡œ ë“±ì„ *ë¡œ ë³€í™˜
        template = log_message
        
        # ìˆ«ì ë³€í™˜
        template = re.sub(r'\d+', '*', template)
        
        # IP ì£¼ì†Œ ë³€í™˜
        template = re.sub(r'\d+\.\d+\.\d+\.\d+', '*', template)
        
        # íŒŒì¼ ê²½ë¡œ ë³€í™˜
        template = re.sub(r'[/\\][\w/\\\.]+', '/*', template)
        
        # UUID ë³€í™˜
        template = re.sub(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', '*', template, flags=re.IGNORECASE)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
        template = re.sub(r'\d{4}-\d{2}-\d{2}[\sT]\d{2}:\d{2}:\d{2}', '*', template)
        
        return template.strip()
    
    def build_template_vocabulary(self, log_messages: List[str]) -> Dict[str, int]:
        """ë¡œê·¸ í…œí”Œë¦¿ ì‚¬ì „ êµ¬ì¶•"""
        templates = [self.extract_template(msg) for msg in log_messages]
        template_counts = Counter(templates)
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¸ë±ìŠ¤ í• ë‹¹
        template_vocab = {template: idx for idx, (template, _) in enumerate(template_counts.most_common())}
        
        return template_vocab


class DeepLogDetector:
    """DeepLog: LSTM ê¸°ë°˜ ì‹¤í–‰ ê²½ë¡œ ì˜ˆì¸¡"""
    
    def __init__(self, embedding_dim=128, hidden_dim=64, num_layers=2, sequence_length=10):
        """
        Args:
            embedding_dim: ë¡œê·¸ ì„ë² ë”© ì°¨ì›
            hidden_dim: LSTM hidden ì°¨ì›
            num_layers: LSTM ë ˆì´ì–´ ìˆ˜
            sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´
        """
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.sequence_length = sequence_length
        
        self.template_extractor = LogTemplateExtractor()
        self.template_vocab = {}
        self.model = None
        self.scaler = StandardScaler()
        self.is_fitted = False
        
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install torch")
    
    def prepare_sequences(self, logs_df: pd.DataFrame, logger=None) -> Tuple[np.ndarray, np.ndarray]:
        """
        ë¡œê·¸ ì‹œí€€ìŠ¤ ì¤€ë¹„
        
        Args:
            logs_df: ë¡œê·¸ DataFrame (timestamp, message ì»¬ëŸ¼ í•„ìš”)
            logger: ë¡œê±° (ì§„í–‰ ìƒí™© ì¶œë ¥ìš©, ì„ íƒì )
        
        Returns:
            sequences: ì‹œí€€ìŠ¤ ë°°ì—´
            next_logs: ë‹¤ìŒ ë¡œê·¸ (ë¼ë²¨)
        """
        # ì‹œê°„ ìˆœì„œ ì •ë ¬ (inplace=Falseì´ë¯€ë¡œ ìƒˆë¡œìš´ DataFrame ìƒì„±, ì›ë³¸ì€ ìœ ì§€)
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì •ë ¬ëœ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ê³  ì›ë³¸ ì°¸ì¡°ëŠ” ìœ ì§€
        logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
        
        # í…œí”Œë¦¿ ì¶”ì¶œ ë° ì‚¬ì „ êµ¬ì¶•
        if not self.template_vocab:
            if logger:
                logger.info("  í…œí”Œë¦¿ ì‚¬ì „ êµ¬ì¶• ì¤‘...")
            log_messages = logs_df['message'].tolist()
            self.template_vocab = self.template_extractor.build_template_vocabulary(log_messages)
            if logger:
                logger.info(f"  âœ… í…œí”Œë¦¿ ì‚¬ì „ êµ¬ì¶• ì™„ë£Œ: {len(self.template_vocab):,}ê°œ í…œí”Œë¦¿")
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì : ë¯¸ë¦¬ NumPy ë°°ì—´ í• ë‹¹ (ë¦¬ìŠ¤íŠ¸ ì˜¤ë²„í—¤ë“œ ì œê±°)
        total_sequences = len(logs_df) - self.sequence_length
        if logger:
            logger.info(f"  ì‹œí€€ìŠ¤ ìƒì„± ì¤‘: ì˜ˆìƒ {total_sequences:,}ê°œ ì‹œí€€ìŠ¤...")
            # ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            estimated_mb = (total_sequences * self.sequence_length * 4) / (1024 * 1024)  # int32 = 4 bytes
            logger.info(f"  ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_mb:.1f} MB")
        
        # ë¯¸ë¦¬ NumPy ë°°ì—´ í• ë‹¹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        sequences = np.zeros((total_sequences, self.sequence_length), dtype=np.int32)
        next_logs = np.zeros(total_sequences, dtype=np.int32)
        
        start_time = time.time()
        for i in range(total_sequences):
            # í˜„ì¬ ì‹œí€€ìŠ¤
            sequence_logs = logs_df.iloc[i:i + self.sequence_length]
            sequence_templates = [
                self.template_extractor.extract_template(msg)
                for msg in sequence_logs['message']
            ]
            sequence_indices = [
                self.template_vocab.get(template, 0)
                for template in sequence_templates
            ]
            
            # ë‹¤ìŒ ë¡œê·¸
            next_log = logs_df.iloc[i + self.sequence_length]
            next_template = self.template_extractor.extract_template(next_log['message'])
            next_index = self.template_vocab.get(next_template, 0)
            
            # NumPy ë°°ì—´ì— ì§ì ‘ í• ë‹¹ (ë¦¬ìŠ¤íŠ¸ append ëŒ€ì‹ )
            sequences[i] = sequence_indices
            next_logs[i] = next_index
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (5%ë§ˆë‹¤ ë˜ëŠ” 100ë§Œê°œë§ˆë‹¤)
            if logger and ((i + 1) % max(1, min(total_sequences // 20, 1000000)) == 0 or i == total_sequences - 1):
                progress = ((i + 1) / total_sequences) * 100
                elapsed = time.time() - start_time
                if i > 0:
                    rate = (i + 1) / elapsed  # ì‹œí€€ìŠ¤/ì´ˆ
                    remaining = (total_sequences - (i + 1)) / rate if rate > 0 else 0
                    logger.info(f"  ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰: {i + 1:,}/{total_sequences:,} ({progress:.1f}%) - "
                              f"ê²½ê³¼: {elapsed:.1f}ì´ˆ - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining:.1f}ì´ˆ ({remaining/60:.1f}ë¶„)")
                else:
                    logger.info(f"  ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰: {i + 1:,}/{total_sequences:,} ({progress:.1f}%)")
        
        return sequences, next_logs
    
    def build_model(self, vocab_size: int):
        """LSTM ëª¨ë¸ êµ¬ì¶•"""
        class LSTMPredictor(nn.Module):
            def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embedding_dim)
                self.lstm = nn.LSTM(
                    embedding_dim,
                    hidden_dim,
                    num_layers,
                    batch_first=True
                )
                self.fc = nn.Linear(hidden_dim, vocab_size)
            
            def forward(self, x):
                embedded = self.embedding(x)
                lstm_out, _ = self.lstm(embedded)
                # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶œë ¥ ì‚¬ìš©
                last_output = lstm_out[:, -1, :]
                output = self.fc(last_output)
                return output
        
        self.model = LSTMPredictor(
            vocab_size=vocab_size,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim,
            num_layers=self.num_layers
        )
        
        # ëª¨ë¸ì„ MPS/CUDA/CPU ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if TORCH_AVAILABLE and DEVICE is not None:
            self.model = self.model.to(DEVICE)
        
        return self.model
    
    def train(self, logs_df: pd.DataFrame, epochs=50, batch_size=32, learning_rate=0.001, log_dir=None):
        """ëª¨ë¸ í•™ìŠµ"""
        # ë¡œê±° ì„¤ì •
        log_dir_path = log_dir or os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs', 'training')
        logger = setup_training_logger('deeplog', log_dir_path)
        
        logger.info("=" * 60)
        logger.info("DeepLog ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"í•™ìŠµ íŒŒë¼ë¯¸í„°:")
        logger.info(f"  - Epochs: {epochs}")
        logger.info(f"  - Batch Size: {batch_size}")
        logger.info(f"  - Learning Rate: {learning_rate}")
        logger.info(f"  - í•™ìŠµ ë°ì´í„°: {len(logs_df):,}ê°œ ë¡œê·¸")
        
        start_time = time.time()
        
        # ì‹œí€€ìŠ¤ ì¤€ë¹„
        logger.info("ì‹œí€€ìŠ¤ ì¤€ë¹„ ì¤‘...")
        sequences, next_logs = self.prepare_sequences(logs_df, logger=logger)
        
        if len(sequences) == 0:
            logger.warning("âš ï¸ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ì¤€ë¹„ ì™„ë£Œ: {len(sequences):,}ê°œ")
        
        vocab_size = len(self.template_vocab) + 1  # +1 for unknown
        logger.info(f"  - ì–´íœ˜ í¬ê¸°: {vocab_size:,}")
        
        # logs_df ë©”ëª¨ë¦¬ í•´ì œ (ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ í›„ ë” ì´ìƒ í•„ìš” ì—†ìŒ)
        del logs_df
        import gc
        gc.collect()
        logger.info(f"   ğŸ’¡ ì›ë³¸ ë¡œê·¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")
        
        # ëª¨ë¸ êµ¬ì¶•
        if self.model is None:
            logger.info("ëª¨ë¸ êµ¬ì¶• ì¤‘...")
            self.model = self.build_model(vocab_size)
            device_str = str(DEVICE) if TORCH_AVAILABLE and DEVICE is not None else "CPU"
            logger.info(f"âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device_str})")
        
        # PyTorch í…ì„œ ë³€í™˜ ë° ë””ë°”ì´ìŠ¤ ì´ë™
        device_str = str(DEVICE) if TORCH_AVAILABLE and DEVICE is not None else "CPU"
        logger.info(f"ë°ì´í„° í…ì„œ ë³€í™˜ ì¤‘... (ë””ë°”ì´ìŠ¤: {device_str})")
        num_sequences = len(sequences)  # ì‚­ì œ ì „ì— ê¸¸ì´ ì €ì¥
        sequences_tensor = torch.LongTensor(sequences)
        next_logs_tensor = torch.LongTensor(next_logs)
        
        # í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if TORCH_AVAILABLE and DEVICE is not None:
            sequences_tensor = sequences_tensor.to(DEVICE)
            next_logs_tensor = next_logs_tensor.to(DEVICE)
        num_batches = (num_sequences + batch_size - 1) // batch_size
        
        # NumPy ë°°ì—´ ë©”ëª¨ë¦¬ í•´ì œ (í…ì„œ ë³€í™˜ ì™„ë£Œ í›„)
        del sequences
        del next_logs
        import gc
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        
        logger.info(f"âœ… í…ì„œ ë³€í™˜ ì™„ë£Œ: {num_batches:,}ê°œ ë°°ì¹˜")
        logger.info(f"   ğŸ’¡ NumPy ë°°ì—´ ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ (í…ì„œë§Œ ë©”ëª¨ë¦¬ì— ìœ ì§€)")
        
        # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # í•™ìŠµ
        logger.info("\n" + "=" * 60)
        logger.info("í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        self.model.train()
        
        best_loss = float('inf')
        for epoch in range(epochs):
            epoch_start = time.time()
            total_loss = 0
            num_batches_processed = 0
            
            for i in range(0, num_sequences, batch_size):
                batch_sequences = sequences_tensor[i:i + batch_size]
                batch_next = next_logs_tensor[i:i + batch_size]
                
                optimizer.zero_grad()
                outputs = self.model(batch_sequences)
                loss = criterion(outputs, batch_next)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches_processed += 1
                
                # ë°°ì¹˜ ì§„í–‰ ìƒí™© (10%ë§ˆë‹¤)
                if num_batches_processed % max(1, num_batches // 10) == 0:
                    progress = (num_batches_processed / num_batches) * 100
                    logger.info(f"  Epoch {epoch + 1}/{epochs} - ë°°ì¹˜ {num_batches_processed}/{num_batches} ({progress:.1f}%) - í˜„ì¬ Loss: {loss.item():.4f}")
            
            avg_loss = total_loss / num_batches_processed
            epoch_time = time.time() - epoch_start
            
            # ë§¤ epochë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
            elapsed_time = time.time() - start_time
            remaining_epochs = epochs - (epoch + 1)
            estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0
            
            logger.info(f"Epoch {epoch + 1}/{epochs} ì™„ë£Œ:")
            logger.info(f"  - í‰ê·  Loss: {avg_loss:.4f}")
            logger.info(f"  - Epoch ì†Œìš” ì‹œê°„: {epoch_time:.2f}ì´ˆ")
            logger.info(f"  - ì´ ê²½ê³¼ ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
            if estimated_remaining > 0:
                logger.info(f"  - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining:.2f}ì´ˆ ({estimated_remaining/60:.1f}ë¶„)")
            
            # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
            if avg_loss < best_loss:
                best_loss = avg_loss
                logger.info(f"  â­ ìµœê³  Loss ê°±ì‹ : {best_loss:.4f}")
        
        total_time = time.time() - start_time
        self.is_fitted = True
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… DeepLog ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        logger.info(f"  - ìµœì¢… Loss: {avg_loss:.4f}")
        logger.info(f"  - ìµœê³  Loss: {best_loss:.4f}")
        logger.info("=" * 60)
        
        return True
    
    def predict_anomaly(self, logs_df: pd.DataFrame, threshold=0.5) -> pd.DataFrame:
        """
        ì´ìƒì¹˜ íƒì§€
        
        Args:
            logs_df: í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame
            threshold: ì´ìƒì¹˜ ì„ê³„ê°’
        
        Returns:
            ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ DataFrame
        """
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        sequences, actual_next = self.prepare_sequences(logs_df)
        
        if len(sequences) == 0:
            return pd.DataFrame()
        
        self.model.eval()
        anomalies = []
        
        with torch.no_grad():
            sequences_tensor = torch.LongTensor(sequences)
            # í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            if TORCH_AVAILABLE and DEVICE is not None:
                sequences_tensor = sequences_tensor.to(DEVICE)
            
            outputs = self.model(sequences_tensor)
            
            # í™•ë¥  ê³„ì‚°
            probs = torch.softmax(outputs, dim=1)
            # CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
            if TORCH_AVAILABLE and DEVICE is not None:
                predicted = torch.argmax(probs, dim=1).cpu().numpy()
                predicted_probs = probs[np.arange(len(actual_next)), actual_next].cpu().numpy()
            else:
                predicted = torch.argmax(probs, dim=1).numpy()
                predicted_probs = probs[np.arange(len(actual_next)), actual_next].numpy()
            
            # ì´ìƒì¹˜ íŒë‹¨ (ì˜ˆì¸¡ í™•ë¥ ì´ ë‚®ìœ¼ë©´ ì´ìƒ)
            anomaly_scores = 1 - predicted_probs
            
            for i in range(len(sequences)):
                if anomaly_scores[i] > threshold:
                    anomalies.append({
                        'sequence_index': i,
                        'predicted_template': predicted[i],
                        'actual_template': actual_next[i],
                        'prediction_prob': predicted_probs[i],
                        'anomaly_score': anomaly_scores[i],
                        'is_anomaly': True
                    })
        
        return pd.DataFrame(anomalies)


class LogAnomalyDetector:
    """LogAnomaly: Template2Vec ê¸°ë°˜ ì˜ë¯¸ ë²¡í„°í™”"""
    
    def __init__(self, vector_dim=100):
        """
        Args:
            vector_dim: í…œí”Œë¦¿ ë²¡í„° ì°¨ì›
        """
        self.vector_dim = vector_dim
        self.template_extractor = LogTemplateExtractor()
        self.template_vectors = {}
        self.template_vocab = {}
        self.scaler = StandardScaler()
        self.is_fitted = False
        
    def build_template_vectors(self, log_messages: List[str]):
        """í…œí”Œë¦¿ ë²¡í„° êµ¬ì¶• (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # í…œí”Œë¦¿ ì¶”ì¶œ
        templates = [self.template_extractor.extract_template(msg) for msg in log_messages]
        unique_templates = list(set(templates))
        
        # TF-IDF ë²¡í„°í™”
        vectorizer = TfidfVectorizer(max_features=self.vector_dim, ngram_range=(1, 2))
        template_vectors = vectorizer.fit_transform(unique_templates).toarray()
        
        # í…œí”Œë¦¿ë³„ ë²¡í„° ì €ì¥
        self.template_vectors = {
            template: vector
            for template, vector in zip(unique_templates, template_vectors)
        }
        
        # í…œí”Œë¦¿ ì‚¬ì „
        self.template_vocab = {template: idx for idx, template in enumerate(unique_templates)}
        
        return self.template_vectors
    
    def create_sequences(self, logs_df: pd.DataFrame, window_size=10) -> np.ndarray:
        """ë¡œê·¸ ì‹œí€€ìŠ¤ë¥¼ ë²¡í„° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜"""
        logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
        
        sequences = []
        for i in range(len(logs_df) - window_size + 1):
            sequence_logs = logs_df.iloc[i:i + window_size]
            sequence_templates = [
                self.template_extractor.extract_template(msg)
                for msg in sequence_logs['message']
            ]
            
            # í…œí”Œë¦¿ ë²¡í„°ë¡œ ë³€í™˜
            sequence_vectors = [
                self.template_vectors.get(template, np.zeros(self.vector_dim))
                for template in sequence_templates
            ]
            
            sequences.append(sequence_vectors)
        
        return np.array(sequences)
    
    def train(self, logs_df: pd.DataFrame, window_size=10, log_dir=None, epochs=None, batch_size=None):
        """
        ì •ìƒ íŒ¨í„´ í•™ìŠµ
        
        Args:
            logs_df: í•™ìŠµìš© ë¡œê·¸ DataFrame
            window_size: ì‹œí€€ìŠ¤ ìœˆë„ìš° í¬ê¸°
            log_dir: ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            epochs: ë¬´ì‹œë¨ (í†µê³„ ê¸°ë°˜ ëª¨ë¸ì´ë¯€ë¡œ ë¶ˆí•„ìš”)
            batch_size: ë¬´ì‹œë¨ (í†µê³„ ê¸°ë°˜ ëª¨ë¸ì´ë¯€ë¡œ ë¶ˆí•„ìš”)
        """
        # ë¡œê±° ì„¤ì •
        log_dir_path = log_dir or os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs', 'training')
        logger = setup_training_logger('loganomaly', log_dir_path)
        
        logger.info("=" * 60)
        logger.info("LogAnomaly ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"í•™ìŠµ íŒŒë¼ë¯¸í„°:")
        logger.info(f"  - Window Size: {window_size}")
        logger.info(f"  - í•™ìŠµ ë°ì´í„°: {len(logs_df):,}ê°œ ë¡œê·¸")
        if epochs is not None:
            logger.info(f"  - Epochs: {epochs} (ë¬´ì‹œë¨, í†µê³„ ê¸°ë°˜ ëª¨ë¸)")
        if batch_size is not None:
            logger.info(f"  - Batch Size: {batch_size} (ë¬´ì‹œë¨, í†µê³„ ê¸°ë°˜ ëª¨ë¸)")
        
        start_time = time.time()
        
        # í…œí”Œë¦¿ ë²¡í„° êµ¬ì¶•
        logger.info("í…œí”Œë¦¿ ë²¡í„° êµ¬ì¶• ì¤‘...")
        log_messages = logs_df['message'].tolist()
        logger.info(f"  - ë¡œê·¸ ë©”ì‹œì§€ ìˆ˜: {len(log_messages):,}ê°œ")
        
        self.build_template_vectors(log_messages)
        logger.info(f"âœ… í…œí”Œë¦¿ ë²¡í„° êµ¬ì¶• ì™„ë£Œ: {len(self.template_vectors):,}ê°œ í…œí”Œë¦¿")
        
        # ì •ìƒ ì‹œí€€ìŠ¤ ìƒì„±
        logger.info("ì •ìƒ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        normal_sequences = self.create_sequences(logs_df, window_size)
        
        if len(normal_sequences) == 0:
            logger.warning("âš ï¸ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(normal_sequences):,}ê°œ")
        
        # ì •ìƒ íŒ¨í„´ í†µê³„ ì €ì¥
        logger.info("ì •ìƒ íŒ¨í„´ í†µê³„ ê³„ì‚° ì¤‘...")
        self.normal_mean = np.mean(normal_sequences, axis=(0, 1))
        self.normal_std = np.std(normal_sequences, axis=(0, 1)) + 1e-8
        logger.info(f"  - í‰ê·  ë²¡í„° ì°¨ì›: {len(self.normal_mean)}")
        logger.info(f"  - í‘œì¤€í¸ì°¨ ë²¡í„° ì°¨ì›: {len(self.normal_std)}")
        
        total_time = time.time() - start_time
        self.is_fitted = True
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… LogAnomaly ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        logger.info(f"  - í…œí”Œë¦¿ ìˆ˜: {len(self.template_vectors):,}ê°œ")
        logger.info(f"  - ì‹œí€€ìŠ¤ ìˆ˜: {len(normal_sequences):,}ê°œ")
        logger.info("=" * 60)
        
        return True
    
    def predict_anomaly(self, logs_df: pd.DataFrame, window_size=10, threshold=3.0) -> pd.DataFrame:
        """ì´ìƒì¹˜ íƒì§€ (Z-score ê¸°ë°˜)"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        test_sequences = self.create_sequences(logs_df, window_size)
        
        if len(test_sequences) == 0:
            return pd.DataFrame()
        
        anomalies = []
        
        for i, sequence in enumerate(test_sequences):
            # ì‹œí€€ìŠ¤ í‰ê· 
            seq_mean = np.mean(sequence, axis=0)
            
            # Z-score ê³„ì‚°
            z_scores = np.abs((seq_mean - self.normal_mean) / self.normal_std)
            max_z_score = np.max(z_scores)
            
            if max_z_score > threshold:
                anomalies.append({
                    'sequence_index': i,
                    'max_z_score': max_z_score,
                    'anomaly_score': max_z_score / threshold,
                    'is_anomaly': True
                })
        
        return pd.DataFrame(anomalies)


class LogRobustDetector:
    """LogRobust: Attention + Bi-LSTM (ê°„ì†Œí™” ë²„ì „)"""
    
    def __init__(self, embedding_dim=128, hidden_dim=64, num_layers=2):
        """
        Args:
            embedding_dim: ì„ë² ë”© ì°¨ì›
            hidden_dim: LSTM hidden ì°¨ì›
            num_layers: LSTM ë ˆì´ì–´ ìˆ˜
        """
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.template_extractor = LogTemplateExtractor()
        self.tokenizer = None
        self.model = None
        self.is_fitted = False
        
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install torch")
        
        # Transformers ì§€ì—° import (ì‹¤ì œ ì‚¬ìš© ì‹œì—ë§Œ)
        global TRANSFORMERS_AVAILABLE
        if TRANSFORMERS_AVAILABLE is None:
            try:
                from transformers import BertTokenizer
                TRANSFORMERS_AVAILABLE = True
                # BertTokenizer ë¡œë“œ ì‹œë„
                try:
                    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
                except Exception as e:
                    print(f"âš ï¸ BertTokenizer ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    self.tokenizer = None
            except Exception as e:
                TRANSFORMERS_AVAILABLE = False
                print(f"âš ï¸ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   LogRobustëŠ” ê°„ì†Œí™”ëœ ë²„ì „(í•´ì‹œ ê¸°ë°˜ ì¸ì½”ë”©)ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
                self.tokenizer = None
    
    def encode_log(self, log_message: str) -> np.ndarray:
        """ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        if self.tokenizer is not None:
            try:
                # BERT ê¸°ë°˜ ì¸ì½”ë”© (ê°„ì†Œí™”)
                tokens = self.tokenizer(log_message, return_tensors='pt', truncation=True, max_length=128)
                # ì‹¤ì œë¡œëŠ” BERT ëª¨ë¸ì„ í†µê³¼ì‹œì¼œì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ì†Œí™”
                return np.random.randn(self.embedding_dim)  # ì„ì‹œ
            except Exception as e:
                # BERT ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²• ì‚¬ìš©
                pass
        
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì¸ì½”ë”© (ê¸°ë³¸ ë°©ë²•)
        return np.array([hash(log_message) % 1000] * self.embedding_dim) / 1000.0
    
    def build_model(self, input_dim: int):
        """Bi-LSTM + Attention ëª¨ë¸ êµ¬ì¶•"""
        class BiLSTMAttention(nn.Module):
            def __init__(self, input_dim, hidden_dim, num_layers):
                super().__init__()
                self.lstm = nn.LSTM(
                    input_dim,
                    hidden_dim,
                    num_layers,
                    batch_first=True,
                    bidirectional=True
                )
                self.attention = nn.MultiheadAttention(
                    embed_dim=hidden_dim * 2,
                    num_heads=4,
                    batch_first=True
                )
                self.fc = nn.Linear(hidden_dim * 2, 1)
                self.sigmoid = nn.Sigmoid()
            
            def forward(self, x):
                lstm_out, _ = self.lstm(x)
                attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
                # í‰ê·  í’€ë§
                pooled = torch.mean(attn_out, dim=1)
                output = self.sigmoid(self.fc(pooled))
                return output
        
        self.model = BiLSTMAttention(input_dim, self.hidden_dim, self.num_layers)
        
        # ëª¨ë¸ì„ MPS/CUDA/CPU ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if TORCH_AVAILABLE and DEVICE is not None:
            self.model = self.model.to(DEVICE)
        
        return self.model
    
    def train(self, logs_df: pd.DataFrame, sequence_length=10, epochs=50, batch_size=32, log_dir=None):
        """ëª¨ë¸ í•™ìŠµ"""
        # ë¡œê±° ì„¤ì •
        log_dir_path = log_dir or os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs', 'training')
        logger = setup_training_logger('logrobust', log_dir_path)
        
        logger.info("=" * 60)
        logger.info("LogRobust ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        logger.info(f"í•™ìŠµ íŒŒë¼ë¯¸í„°:")
        logger.info(f"  - Epochs: {epochs}")
        logger.info(f"  - Batch Size: {batch_size}")
        logger.info(f"  - Sequence Length: {sequence_length}")
        logger.info(f"  - Embedding Dim: {self.embedding_dim}")
        logger.info(f"  - í•™ìŠµ ë°ì´í„°: {len(logs_df):,}ê°œ ë¡œê·¸")
        
        start_time = time.time()
        
        logger.info("ë°ì´í„° ì •ë ¬ ì¤‘...")
        logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        logger.info("ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        sequences = []
        labels = []  # ì •ìƒ=0, ì´ìƒ=1 (ì—¬ê¸°ì„œëŠ” ì •ìƒ ë°ì´í„°ë§Œ í•™ìŠµ)
        
        total_sequences = len(logs_df) - sequence_length + 1
        logger.info(f"  - ì˜ˆìƒ ì‹œí€€ìŠ¤ ìˆ˜: {total_sequences:,}ê°œ")
        
        for i in range(total_sequences):
            sequence_logs = logs_df.iloc[i:i + sequence_length]
            sequence_vectors = [
                self.encode_log(msg) for msg in sequence_logs['message']
            ]
            sequences.append(sequence_vectors)
            labels.append(0)  # ì •ìƒ ë°ì´í„°
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10%ë§ˆë‹¤)
            if (i + 1) % max(1, total_sequences // 10) == 0:
                progress = ((i + 1) / total_sequences) * 100
                logger.info(f"  ì‹œí€€ìŠ¤ ìƒì„± ì§„í–‰: {i + 1:,}/{total_sequences:,} ({progress:.1f}%)")
        
        if len(sequences) == 0:
            logger.warning("âš ï¸ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(sequences):,}ê°œ")
        
        sequences = np.array(sequences)
        labels = np.array(labels)
        logger.info(f"  - ì‹œí€€ìŠ¤ í˜•íƒœ: {sequences.shape}")
        
        # ëª¨ë¸ êµ¬ì¶•
        if self.model is None:
            logger.info("ëª¨ë¸ êµ¬ì¶• ì¤‘...")
            self.model = self.build_model(self.embedding_dim)
            device_str = str(DEVICE) if TORCH_AVAILABLE and DEVICE is not None else "CPU"
            logger.info(f"âœ… ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device_str})")
        
        # í•™ìŠµ
        device_str = str(DEVICE) if TORCH_AVAILABLE and DEVICE is not None else "CPU"
        logger.info(f"ë°ì´í„° í…ì„œ ë³€í™˜ ì¤‘... (ë””ë°”ì´ìŠ¤: {device_str})")
        sequences_tensor = torch.FloatTensor(sequences)
        labels_tensor = torch.FloatTensor(labels).unsqueeze(1)
        
        # í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if TORCH_AVAILABLE and DEVICE is not None:
            sequences_tensor = sequences_tensor.to(DEVICE)
            labels_tensor = labels_tensor.to(DEVICE)
        
        logger.info("âœ… í…ì„œ ë³€í™˜ ì™„ë£Œ")
        
        optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.BCELoss()
        
        logger.info("\n" + "=" * 60)
        logger.info("í•™ìŠµ ì‹œì‘")
        logger.info("=" * 60)
        
        self.model.train()
        best_loss = float('inf')
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            optimizer.zero_grad()
            outputs = self.model(sequences_tensor)
            loss = criterion(outputs, labels_tensor)
            loss.backward()
            optimizer.step()
            
            epoch_time = time.time() - epoch_start
            elapsed_time = time.time() - start_time
            remaining_epochs = epochs - (epoch + 1)
            estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs if epoch > 0 else 0
            
            # ë§¤ epochë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
            logger.info(f"Epoch {epoch + 1}/{epochs} ì™„ë£Œ:")
            logger.info(f"  - Loss: {loss.item():.4f}")
            logger.info(f"  - Epoch ì†Œìš” ì‹œê°„: {epoch_time:.2f}ì´ˆ")
            logger.info(f"  - ì´ ê²½ê³¼ ì‹œê°„: {elapsed_time:.2f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
            if estimated_remaining > 0:
                logger.info(f"  - ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining:.2f}ì´ˆ ({estimated_remaining/60:.1f}ë¶„)")
            
            # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
            if loss.item() < best_loss:
                best_loss = loss.item()
                logger.info(f"  â­ ìµœê³  Loss ê°±ì‹ : {best_loss:.4f}")
        
        total_time = time.time() - start_time
        self.is_fitted = True
        
        logger.info("\n" + "=" * 60)
        logger.info("âœ… LogRobust ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
        logger.info(f"  - ìµœì¢… Loss: {loss.item():.4f}")
        logger.info(f"  - ìµœê³  Loss: {best_loss:.4f}")
        logger.info("=" * 60)
        
        return True
    
    def predict_anomaly(self, logs_df: pd.DataFrame, sequence_length=10, threshold=0.5) -> pd.DataFrame:
        """ì´ìƒì¹˜ íƒì§€"""
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
        
        sequences = []
        for i in range(len(logs_df) - sequence_length + 1):
            sequence_logs = logs_df.iloc[i:i + sequence_length]
            sequence_vectors = [
                self.encode_log(msg) for msg in sequence_logs['message']
            ]
            sequences.append(sequence_vectors)
        
        if len(sequences) == 0:
            return pd.DataFrame()
        
        sequences = np.array(sequences)
        sequences_tensor = torch.FloatTensor(sequences)
        
        # í…ì„œë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        if TORCH_AVAILABLE and DEVICE is not None:
            sequences_tensor = sequences_tensor.to(DEVICE)
        
        self.model.eval()
        anomalies = []
        
        with torch.no_grad():
            outputs = self.model(sequences_tensor)
            # CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
            if TORCH_AVAILABLE and DEVICE is not None:
                anomaly_scores = outputs.squeeze().cpu().numpy()
            else:
                anomaly_scores = outputs.squeeze().numpy()
            
            for i, score in enumerate(anomaly_scores):
                if score > threshold:
                    anomalies.append({
                        'sequence_index': i,
                        'anomaly_score': float(score),
                        'is_anomaly': True
                    })
        
        return pd.DataFrame(anomalies)


class LogSpecificAnomalySystem:
    """ë¡œê·¸ íŠ¹í™” ì´ìƒ íƒì§€ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_type='deeplog'):
        """
        Args:
            model_type: 'deeplog', 'loganomaly', 'logrobust'
        """
        self.model_type = model_type
        self.detector = None
        self.severity_assessor = SeverityAssessment()
        self.logs_df = None
        
        if model_type == 'deeplog':
            self.detector = DeepLogDetector(sequence_length=15)  # 10 â†’ 15
        elif model_type == 'loganomaly':
            self.detector = LogAnomalyDetector()
        elif model_type == 'logrobust':
            self.detector = LogRobustDetector()
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
    
    def load_logs(self, logs_df: pd.DataFrame):
        """ë¡œê·¸ ë°ì´í„° ë¡œë“œ"""
        self.logs_df = logs_df.copy()
        print(f"âœ… {len(self.logs_df)}ê°œ ë¡œê·¸ ë¡œë“œ ì™„ë£Œ")
    
    def train(self, train_ratio=0.8, log_dir=None, epochs=5, batch_size=128):
        """ëª¨ë¸ í•™ìŠµ"""
        if self.logs_df is None or self.logs_df.empty:
            print("âš ï¸ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì •ìƒ ë¡œê·¸ë§Œ í•™ìŠµ
        normal_logs = self.logs_df[self.logs_df['is_error'] == False]
        
        if len(normal_logs) == 0:
            print("âš ï¸ ì •ìƒ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        split_idx = int(len(normal_logs) * train_ratio)
        train_logs = normal_logs.iloc[:split_idx]
        
        print(f"\ní•™ìŠµ ë°ì´í„°: {len(train_logs):,}ê°œ ë¡œê·¸ (ì •ìƒ)")
        print(f"âš¡ ë¹ ë¥¸ í•™ìŠµ ì„¤ì •: Epochs={epochs}, Batch Size={batch_size}")
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì‹œí€€ìŠ¤ ê¸¸ì´ ì „ë‹¬
        if self.model_type == 'loganomaly':
            return self.detector.train(train_logs, window_size=15, log_dir=log_dir, epochs=epochs, batch_size=batch_size)  # 10 â†’ 15
        elif self.model_type == 'logrobust':
            return self.detector.train(train_logs, sequence_length=15, epochs=epochs, batch_size=batch_size, log_dir=log_dir)  # 10 â†’ 15
        else:
            return self.detector.train(train_logs, log_dir=log_dir, epochs=epochs, batch_size=batch_size)
    
    def detect_anomalies(self, test_logs_df=None):
        """ì´ìƒì¹˜ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€"""
        if not self.detector.is_fitted:
            print("âš ï¸ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        if test_logs_df is None:
            test_logs_df = self.logs_df
        
        print("\n" + "=" * 60)
        print(f"{self.model_type.upper()} ì´ìƒ íƒì§€")
        print("=" * 60)
        
        # ì´ìƒì¹˜ íƒì§€ (ì‹œí€€ìŠ¤ ê¸¸ì´ 15ë¡œ ì„¤ì •)
        if self.model_type == 'loganomaly':
            anomalies = self.detector.predict_anomaly(test_logs_df, window_size=15)  # 10 â†’ 15
        elif self.model_type == 'logrobust':
            anomalies = self.detector.predict_anomaly(test_logs_df, sequence_length=15)  # 10 â†’ 15
        else:
            anomalies = self.detector.predict_anomaly(test_logs_df)
        
        if anomalies.empty:
            print("âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {'anomalies': pd.DataFrame(), 'summary': {}}
        
        print(f"âœ… {len(anomalies)}ê°œ ì´ìƒ ì‹œí€€ìŠ¤ íƒì§€")
        
        # íƒì§€ëœ ì‹œí€€ìŠ¤ì˜ ë¡œê·¸ ì¶”ì¶œ ë° ì‹¬ê°ë„ í‰ê°€
        anomaly_logs_list = []
        
        for idx, row in anomalies.iterrows():
            seq_idx = row['sequence_index']
            # ì‹œí€€ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ ì¶”ì¶œ (ê°„ë‹¨í™”)
            if seq_idx < len(test_logs_df):
                # ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë§ê²Œ ì¡°ì • (DeepLogëŠ” 15, LogAnomalyëŠ” 15, LogRobustëŠ” 15)
                seq_len = getattr(self.detector, 'sequence_length', getattr(self.detector, 'window_size', 15))
                sequence_logs = test_logs_df.iloc[seq_idx:seq_idx + seq_len]  # ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼
                
                # ì‹¬ê°ë„ í‰ê°€
                severity_info = self.severity_assessor.assess_time_window_severity(sequence_logs)
                
                anomaly_logs_list.append({
                    'sequence_index': seq_idx,
                    'anomaly_score': row.get('anomaly_score', 0),
                    'max_severity_score': severity_info['max_severity_score'],
                    'max_severity_level': severity_info['max_severity_level'],
                    'avg_severity_score': severity_info['avg_severity_score'],
                    'critical_count': severity_info['critical_count'],
                    'high_count': severity_info['high_count'],
                    'medium_count': severity_info['medium_count'],
                    'low_count': severity_info['low_count'],
                })
        
        results_df = pd.DataFrame(anomaly_logs_list)
        
        # ì‹¬ê°ë„ ê¸°ì¤€ ì •ë ¬
        if not results_df.empty and 'max_severity_score' in results_df.columns:
            results_df = results_df.sort_values('max_severity_score', ascending=False)
            results_df['priority'] = range(1, len(results_df) + 1)
        
        # ìš”ì•½ í†µê³„
        summary = {
            'total_anomalies': len(results_df),
            'by_severity': results_df['max_severity_level'].value_counts().to_dict() if 'max_severity_level' in results_df.columns else {},
            'avg_severity_score': results_df['max_severity_score'].mean() if 'max_severity_score' in results_df.columns else 0,
            'max_severity_score': results_df['max_severity_score'].max() if 'max_severity_score' in results_df.columns else 0,
        }
        
        return {
            'anomalies': results_df,
            'summary': summary
        }
    
    def generate_report(self, results):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print(f"{self.model_type.upper()} ì´ìƒ íƒì§€ + ì‹¬ê°ë„ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        
        if not results or results.get('total_anomalies', 0) == 0:
            print("âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        summary = results.get('summary', {})
        
        print(f"\nğŸ“Š íƒì§€ ê²°ê³¼:")
        print(f"   ì´ ì´ìƒ ì‹œí€€ìŠ¤: {summary.get('total_anomalies', 0)}ê°œ")
        
        if 'by_severity' in summary:
            print(f"\nğŸ” ì‹¬ê°ë„ ë¶„í¬:")
            for level, count in summary['by_severity'].items():
                print(f"   {level}: {count}ê°œ")
        
        print(f"\n   í‰ê·  ì‹¬ê°ë„ ì ìˆ˜: {summary.get('avg_severity_score', 0):.2f}")
        print(f"   ìµœê³  ì‹¬ê°ë„ ì ìˆ˜: {summary.get('max_severity_score', 0):.2f}")


def analyze_risk_level(anomalies_df: pd.DataFrame, test_logs_df: pd.DataFrame = None) -> pd.DataFrame:
    """
    ì´ìƒ íƒì§€ ê²°ê³¼ë¥¼ ìœ„í—˜ë„ë³„ë¡œ ë¶„ì„ (ê°œì„ ëœ ë¡œì§)
    
    Args:
        anomalies_df: ì´ìƒ íƒì§€ ê²°ê³¼ DataFrame
        test_logs_df: í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame (ë¡œê·¸ ë©”ì‹œì§€ í™•ì¸ìš©, ì„ íƒì )
    
    Returns:
        ìœ„í—˜ë„ ë¶„ì„ ê²°ê³¼ DataFrame
    """
    if anomalies_df.empty:
        return pd.DataFrame()
    
    anomalies_df = anomalies_df.copy()
    
    # ì‹¤ì œ ìœ„í—˜ í‚¤ì›Œë“œ (ì˜ˆì™¸, ì˜¤ë¥˜ ë“±)
    CRITICAL_KEYWORDS = [
        'exception', 'error', 'failed', 'timeout', 'nullpointer',
        'outofmemory', 'connection refused', 'unauthorized', 'forbidden',
        'sql injection', 'xss', 'csrf', 'stacktrace', 'traceback',
        'fatal', 'critical', 'panic', 'crash', 'hang', 'deadlock',
        'out of memory', 'memory leak', 'disk full', 'permission denied'
    ]
    
    # ì •ìƒ ì¿¼ë¦¬ íŒ¨í„´ (ìœ„í—˜ë„ ë‚®ì¶¤)
    NORMAL_QUERY_PATTERNS = [
        'binding parameter', '==> parameters', '==>  preparing',
        'committing jdbc', 'extracted value', '<==      total',
        'creating a new sqlsession', 'closing non transactional',
        'jdbc connection', 'hikariproxyconnection', 'will not be managed',
        'registered for synchronization', 'accept-language',
        'heartbeat status: 200', 'discoveryclient'
    ]
    
    # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (ê°œì„ ëœ ë¡œì§)
    def calculate_risk_score(row):
        anomaly_score = row['anomaly_score']
        severity_score = row['max_severity_score']
        
        # ë¡œê·¸ ë©”ì‹œì§€ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        messages = ""
        if test_logs_df is not None and 'sequence_index' in row:
            try:
                seq_idx = int(row['sequence_index'])
                seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´
                start_idx = max(0, seq_idx)
                end_idx = min(len(test_logs_df), seq_idx + seq_len)
                sequence_logs = test_logs_df.iloc[start_idx:end_idx]
                messages = ' '.join(sequence_logs['message'].astype(str).tolist()).lower()
            except:
                pass
        
        # sample_messages ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        if 'sample_messages' in row and pd.notna(row['sample_messages']):
            messages = str(row['sample_messages']).lower()
        
        # ì •ìƒ ì¿¼ë¦¬ íŒ¨í„´ í™•ì¸
        is_normal_query = False
        if messages:
            is_normal_query = any(pattern in messages for pattern in NORMAL_QUERY_PATTERNS)
        
        # ì‹¤ì œ ìœ„í—˜ í‚¤ì›Œë“œ í™•ì¸
        has_real_exception = False
        if messages:
            has_real_exception = any(keyword in messages for keyword in CRITICAL_KEYWORDS)
        
        # ìœ„í—˜ë„ ê³„ì‚° (ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •)
        if is_normal_query and not has_real_exception:
            # ì •ìƒ ì¿¼ë¦¬ ë¡œê·¸: ìœ„í—˜ë„ ëŒ€í­ ë‚®ì¶¤ (CRITICAL ë°©ì§€)
            # ì •ìƒ ì¿¼ë¦¬ëŠ” ìµœëŒ€ 79ì ìœ¼ë¡œ ì œí•œí•˜ì—¬ CRITICALì´ ë˜ì§€ ì•Šë„ë¡ í•¨
            risk_score = (
                anomaly_score * 15 +  # ì´ìƒ ì ìˆ˜ ê°€ì¤‘ì¹˜ ë” ë‚®ì¶¤
                (severity_score / 10) * 10  # ì‹¬ê°ë„ ì ìˆ˜ ê°€ì¤‘ì¹˜ ë” ë‚®ì¶¤
            )
            # ì •ìƒ ì¿¼ë¦¬ëŠ” ìµœëŒ€ 79ì ìœ¼ë¡œ ì œí•œ
            risk_score = min(79, risk_score)
        elif has_real_exception:
            # ì‹¤ì œ ì˜ˆì™¸/ì˜¤ë¥˜: ìœ„í—˜ë„ ë†’ì„
            risk_score = (
                anomaly_score * 60 +  # ì´ìƒ ì ìˆ˜ ê°€ì¤‘ì¹˜ ë†’ì„
                (severity_score / 10) * 70  # ì‹¬ê°ë„ ì ìˆ˜ ê°€ì¤‘ì¹˜ ë†’ì„
            )
        else:
            # ê¸°ë³¸ ê³„ì‚° (ê¸°ì¡´ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì•½ê°„ ì¡°ì •)
            risk_score = (
                anomaly_score * 40 +  # ì´ìƒ ì ìˆ˜ 40% ê°€ì¤‘ì¹˜
                (severity_score / 10) * 40  # ì‹¬ê°ë„ ì ìˆ˜ 40% ê°€ì¤‘ì¹˜
            )
        
        # ì ìˆ˜ ë²”ìœ„ ì œí•œ (0-150, í•˜ì§€ë§Œ 100 ì´ìƒì€ ë§¤ìš° ë“œë­„)
        return min(150, max(0, risk_score))
    
    # ìœ„í—˜ë„ ì ìˆ˜ ê³„ì‚°
    anomalies_df['risk_score'] = anomalies_df.apply(calculate_risk_score, axis=1)
    
    # ìœ„í—˜ë„ ë ˆë²¨ ë¶„ë¥˜
    def classify_risk_level(risk_score):
        if risk_score >= 80:
            return 'CRITICAL'
        elif risk_score >= 60:
            return 'HIGH'
        elif risk_score >= 40:
            return 'MEDIUM'
        elif risk_score >= 20:
            return 'LOW'
        else:
            return 'INFO'
    
    anomalies_df['risk_level'] = anomalies_df['risk_score'].apply(classify_risk_level)
    
    return anomalies_df


def generate_risk_report(anomalies_df: pd.DataFrame, test_logs_df: pd.DataFrame) -> Dict:
    """
    ìœ„í—˜ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        anomalies_df: ì´ìƒ íƒì§€ ê²°ê³¼ DataFrame
        test_logs_df: í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame
    
    Returns:
        ìœ„í—˜ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    if anomalies_df.empty:
        return {
            'total_anomalies': 0,
            'risk_distribution': {},
            'critical_anomalies': pd.DataFrame(),
            'high_anomalies': pd.DataFrame(),
            'medium_anomalies': pd.DataFrame(),
            'low_anomalies': pd.DataFrame(),
            'info_anomalies': pd.DataFrame()
        }
    
    # ìœ„í—˜ë„ë³„ ë¶„ë¥˜
    critical = anomalies_df[anomalies_df['risk_level'] == 'CRITICAL'].copy()
    high = anomalies_df[anomalies_df['risk_level'] == 'HIGH'].copy()
    medium = anomalies_df[anomalies_df['risk_level'] == 'MEDIUM'].copy()
    low = anomalies_df[anomalies_df['risk_level'] == 'LOW'].copy()
    info = anomalies_df[anomalies_df['risk_level'] == 'INFO'].copy()
    
    # ìœ„í—˜ë„ ë¶„í¬
    risk_distribution = anomalies_df['risk_level'].value_counts().to_dict()
    
    # ê° ìœ„í—˜ë„ë³„ ìƒì„¸ ì •ë³´ ì¶”ê°€ (ë¡œê·¸ ë‚´ìš© í¬í•¨)
    def add_log_details(risk_df, test_logs_df):
        if risk_df.empty:
            return risk_df
        
        log_details = []
        for idx, row in risk_df.iterrows():
            seq_idx = row['sequence_index']
            seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´
            start_idx = max(0, seq_idx)
            end_idx = min(len(test_logs_df), seq_idx + seq_len)
            
            sequence_logs = test_logs_df.iloc[start_idx:end_idx]
            
            # ë¡œê·¸ ë©”ì‹œì§€ ìš”ì•½
            log_messages = sequence_logs['message'].tolist()
            log_levels = sequence_logs['level'].tolist()
            
            log_details.append({
                'log_count': len(sequence_logs),
                'log_levels': ', '.join(set(log_levels)),
                'sample_messages': ' | '.join(log_messages[:3])  # ì²˜ìŒ 3ê°œë§Œ
            })
        
        details_df = pd.DataFrame(log_details)
        for col in details_df.columns:
            risk_df[col] = details_df[col].values
        
        return risk_df
    
    critical = add_log_details(critical, test_logs_df)
    high = add_log_details(high, test_logs_df)
    medium = add_log_details(medium, test_logs_df)
    low = add_log_details(low, test_logs_df)
    info = add_log_details(info, test_logs_df)
    
    return {
        'total_anomalies': len(anomalies_df),
        'risk_distribution': risk_distribution,
        'critical_anomalies': critical,
        'high_anomalies': high,
        'medium_anomalies': medium,
        'low_anomalies': low,
        'info_anomalies': info,
        'avg_risk_score': anomalies_df['risk_score'].mean(),
        'max_risk_score': anomalies_df['risk_score'].max()
    }


def print_risk_report(risk_report: Dict):
    """ìœ„í—˜ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("ìœ„í—˜ë„ ë¶„ì„ ë¦¬í¬íŠ¸")
    print("=" * 70)
    
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"   ì´ ì´ìƒ íƒì§€: {risk_report['total_anomalies']:,}ê°œ")
    print(f"   í‰ê·  ìœ„í—˜ë„ ì ìˆ˜: {risk_report.get('avg_risk_score', 0):.2f}/100")
    print(f"   ìµœê³  ìœ„í—˜ë„ ì ìˆ˜: {risk_report.get('max_risk_score', 0):.2f}/100")
    
    print(f"\nğŸ” ìœ„í—˜ë„ ë¶„í¬:")
    risk_dist = risk_report.get('risk_distribution', {})
    risk_order = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']
    for level in risk_order:
        count = risk_dist.get(level, 0)
        if count > 0:
            percentage = (count / risk_report['total_anomalies']) * 100
            print(f"   {level:10s}: {count:5d}ê°œ ({percentage:5.1f}%)")
    
    # ìœ„í—˜ë„ë³„ ìƒì„¸ ì •ë³´
    for risk_level in ['CRITICAL', 'HIGH', 'MEDIUM']:
        risk_df = risk_report.get(f'{risk_level.lower()}_anomalies', pd.DataFrame())
        if not risk_df.empty:
            print(f"\nâš ï¸ {risk_level} ìœ„í—˜ ì´ìƒ ({len(risk_df)}ê°œ):")
            print("   " + "-" * 66)
            
            # ìƒìœ„ 10ê°œë§Œ ì¶œë ¥
            top_risks = risk_df.head(10)
            for idx, row in top_risks.iterrows():
                print(f"   [{row.get('priority', idx+1)}] ìœ„í—˜ë„: {row.get('risk_score', 0):.1f}/100")
                print(f"       - ì´ìƒ ì ìˆ˜: {row.get('anomaly_score', 0):.4f}")
                print(f"       - ì‹¬ê°ë„: {row.get('max_severity_level', 'N/A')} (ì ìˆ˜: {row.get('max_severity_score', 0):.2f})")
                print(f"       - ì‹œí€€ìŠ¤ ì¸ë±ìŠ¤: {row.get('sequence_index', 'N/A')}")
                if 'log_count' in row:
                    print(f"       - ë¡œê·¸ ìˆ˜: {row.get('log_count', 0)}ê°œ")
                    print(f"       - ë¡œê·¸ ë ˆë²¨: {row.get('log_levels', 'N/A')}")
                print()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - LogAnomaly ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ìƒ íƒì§€ ë° ìœ„í—˜ë„ ë¶„ì„"""
    from log_anomaly_detector import SpringBootLogParser
    
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    print("=" * 70)
    print("LogAnomaly ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ìœ„í—˜ë„ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 70)
    
    # ëª¨ë¸ ì„ íƒ: LogAnomaly (ì„±ëŠ¥ ì¸¡ì •ìœ¼ë¡œ ì„ ì •ëœ ëª¨ë¸)
    model_type = 'loganomaly'
    
    print(f"\nâœ… ì‚¬ìš© ëª¨ë¸: {model_type.upper()} (ì„±ëŠ¥ ì¸¡ì • ì„ ì • ëª¨ë¸)")
    
    # 1. ë¡œê·¸ íŒŒì‹±
    print("\n1ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ íŒŒì‹±")
    parser = SpringBootLogParser()
    logs_df = parser.parse_directory(
        log_directory,
        max_files=None,
        sample_lines=None
    )
    
    if logs_df.empty:
        print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(logs_df):,}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
    
    # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print(f"\n2ë‹¨ê³„: {model_type.upper()} ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    system = LogSpecificAnomalySystem(model_type=model_type)
    
    # í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
    normal_logs = logs_df[logs_df['is_error'] == False]
    error_logs = logs_df[logs_df['is_error'] == True]
    
    # ë©”ëª¨ë¦¬ ìµœì í™”: í•™ìŠµ ë°ì´í„° ìƒ˜í”Œë§ (ìµœëŒ€ 500K)
    max_train_samples = 500000
    
    if len(normal_logs) > max_train_samples:
        print(f"\nâš ï¸ ë©”ëª¨ë¦¬ ìµœì í™”: ì •ìƒ ë¡œê·¸ {len(normal_logs):,}ê°œ â†’ {max_train_samples:,}ê°œë¡œ ìƒ˜í”Œë§")
        # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§ (ì‹œê°„ ìˆœì„œ ìœ ì§€)
        step = len(normal_logs) // max_train_samples
        normal_logs = normal_logs.iloc[::step][:max_train_samples].copy()
        print(f"   ìƒ˜í”Œë§ ì™„ë£Œ: {len(normal_logs):,}ê°œ")
    
    # í•™ìŠµìš©: ì •ìƒ ë¡œê·¸ì˜ 80%
    train_size = int(len(normal_logs) * 0.8)
    train_logs = normal_logs.iloc[:train_size].copy()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ìƒ˜í”Œë§ (ìµœëŒ€ 100K)
    max_test_normal = 80000
    max_test_error = 20000
    
    test_normal = normal_logs.iloc[train_size:].copy()
    if len(test_normal) > max_test_normal:
        print(f"\nâš ï¸ í…ŒìŠ¤íŠ¸ ì •ìƒ ë¡œê·¸ ìƒ˜í”Œë§: {len(test_normal):,}ê°œ â†’ {max_test_normal:,}ê°œ")
        step = len(test_normal) // max_test_normal
        test_normal = test_normal.iloc[::step][:max_test_normal].copy()
    
    if len(error_logs) > max_test_error:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ë¡œê·¸ ìƒ˜í”Œë§: {len(error_logs):,}ê°œ â†’ {max_test_error:,}ê°œ")
        step = len(error_logs) // max_test_error
        error_logs = error_logs.iloc[::step][:max_test_error].copy()
    
    test_logs = pd.concat([
        test_normal,
        error_logs
    ], ignore_index=True).sort_values('timestamp').reset_index(drop=True)
    
    print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"   í•™ìŠµ ë°ì´í„°: {len(train_logs):,}ê°œ (ì •ìƒ ë¡œê·¸)")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_logs):,}ê°œ (ì •ìƒ: {len(test_normal):,}ê°œ, ì—ëŸ¬: {len(error_logs):,}ê°œ)")
    
    # ë©”ëª¨ë¦¬ ì˜ˆìƒì¹˜ ê³„ì‚°
    estimated_memory_gb = (len(train_logs) * 15 * 100 * 4) / (1024 ** 3)
    print(f"\nğŸ’¾ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {estimated_memory_gb:.2f} GB")
    if estimated_memory_gb > 16:
        print(f"âš ï¸ ê²½ê³ : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì‘ì€ ìƒ˜í”Œ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
        return
    
    # 3. ëª¨ë¸ í•™ìŠµ
    print("\n3ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ")
    system.load_logs(train_logs)
    if not system.train(train_ratio=1.0, epochs=10, batch_size=32):
        print("âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨")
        return
    
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
    
    # 4. ì´ìƒ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€
    print("\n4ë‹¨ê³„: ì´ìƒ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ì¤‘: {len(test_logs):,}ê°œ ë¡œê·¸...")
    results = system.detect_anomalies(test_logs)
    
    if not results or results.get('anomalies', pd.DataFrame()).empty:
        print("âœ… ì´ìƒì´ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    anomalies_df = results['anomalies']
    print(f"âœ… {len(anomalies_df):,}ê°œ ì´ìƒ ì‹œí€€ìŠ¤ íƒì§€")
    
    # 5. ìœ„í—˜ë„ ë¶„ì„ (ê°œì„ ëœ ë¡œì§ ì ìš©)
    print("\n5ë‹¨ê³„: ìœ„í—˜ë„ ë¶„ì„ (ê°œì„ ëœ ë¡œì§: ì •ìƒ ì¿¼ë¦¬ í•„í„°ë§, ì‹¤ì œ ì˜ˆì™¸ ê°ì§€)")
    anomalies_with_risk = analyze_risk_level(anomalies_df, test_logs)
    risk_report = generate_risk_report(anomalies_with_risk, test_logs)
    
    # 6. ìœ„í—˜ë„ ë¦¬í¬íŠ¸ ì¶œë ¥
    print_risk_report(risk_report)
    
    # 7. ê²°ê³¼ ì €ì¥ (ë™ì  í´ë” ìƒì„±)
    base_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results"
    base_folder_name = "loganomaly_risk_analysis"
    
    # í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë²ˆí˜¸ë¥¼ ì¦ê°€ì‹œì¼œ ìƒˆ í´ë” ìƒì„±
    output_dir = os.path.join(base_dir, base_folder_name)
    folder_num = 0
    while os.path.exists(output_dir):
        folder_num += 1
        output_dir = os.path.join(base_dir, f"{base_folder_name}_{folder_num}")
    
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {os.path.basename(output_dir)}")
    
    # ì „ì²´ ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥
    if not anomalies_with_risk.empty:
        output_path = os.path.join(output_dir, "anomalies_with_risk.csv")
        anomalies_with_risk.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì „ì²´ ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìœ„í—˜ë„ë³„ ê²°ê³¼ ì €ì¥
    for risk_level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']:
        risk_df = risk_report.get(f'{risk_level.lower()}_anomalies', pd.DataFrame())
        if not risk_df.empty:
            risk_path = os.path.join(output_dir, f"risk_{risk_level.lower()}.csv")
            risk_df.to_csv(risk_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ {risk_level} ìœ„í—˜ ì´ìƒ ì €ì¥: {risk_path} ({len(risk_df)}ê°œ)")
    
    # ìœ„í—˜ë„ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    summary_path = os.path.join(output_dir, "risk_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("LogAnomaly ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ìœ„í—˜ë„ ë¶„ì„ ë¦¬í¬íŠ¸\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"ì „ì²´ í†µê³„:\n")
        f.write(f"  - ì´ ì´ìƒ íƒì§€: {risk_report['total_anomalies']:,}ê°œ\n")
        f.write(f"  - í‰ê·  ìœ„í—˜ë„ ì ìˆ˜: {risk_report.get('avg_risk_score', 0):.2f}/100\n")
        f.write(f"  - ìµœê³  ìœ„í—˜ë„ ì ìˆ˜: {risk_report.get('max_risk_score', 0):.2f}/100\n\n")
        f.write(f"ìœ„í—˜ë„ ë¶„í¬:\n")
        risk_dist = risk_report.get('risk_distribution', {})
        for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']:
            count = risk_dist.get(level, 0)
            if count > 0:
                percentage = (count / risk_report['total_anomalies']) * 100
                f.write(f"  - {level:10s}: {count:5d}ê°œ ({percentage:5.1f}%)\n")
    
    print(f"ğŸ’¾ ìœ„í—˜ë„ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_path}")
    
    print("\n" + "=" * 70)
    print("âœ… ì´ìƒ íƒì§€ ë° ìœ„í—˜ë„ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"   - ì „ì²´ ì´ìƒ íƒì§€: {len(anomalies_with_risk):,}ê°œ")
    print(f"   - CRITICAL ìœ„í—˜: {len(risk_report.get('critical_anomalies', pd.DataFrame()))}ê°œ")
    print(f"   - HIGH ìœ„í—˜: {len(risk_report.get('high_anomalies', pd.DataFrame()))}ê°œ")
    print(f"   - MEDIUM ìœ„í—˜: {len(risk_report.get('medium_anomalies', pd.DataFrame()))}ê°œ")


if __name__ == "__main__":
    main()

