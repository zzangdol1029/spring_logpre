"""
Temporal (Time-series) Anomaly Detection + ì‹¬ê°ë„ í‰ê°€ í†µí•© ì‹œìŠ¤í…œ
ì‹œê³„ì—´ ê¸°ë°˜ ì´ìƒ íƒì§€ í›„ ì‹¬ê°ë„ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import re
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.copod import COPOD
from severity_assessment import SeverityAssessment
import warnings
warnings.filterwarnings('ignore')


class TemporalLogAnomalyDetector:
    """ì‹œê³„ì—´ ê¸°ë°˜ ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ í´ë˜ìŠ¤"""
    
    def __init__(self, window_size=10, step_size=1, model_type='autoencoder'):
        """
        Args:
            window_size: ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸° (ë¡œê·¸ ë¼ì¸ ìˆ˜ ë˜ëŠ” ì‹œê°„ ë‹¨ìœ„)
            step_size: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤í… í¬ê¸°
            model_type: ì‚¬ìš©í•  ëª¨ë¸ ('autoencoder', 'isolation_forest', 'lof', 'copod')
        """
        self.window_size = window_size
        self.step_size = step_size
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.is_fitted = False
        
    def create_sequences(self, features_df, time_col='timestamp'):
        """
        ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
        
        Args:
            features_df: íŠ¹ì§• DataFrame (ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•¨)
            time_col: ì‹œê°„ ì»¬ëŸ¼ëª…
        
        Returns:
            sequences: ì‹œí€€ìŠ¤ ë°°ì—´
            sequence_indices: ê° ì‹œí€€ìŠ¤ì˜ ì›ë³¸ ì¸ë±ìŠ¤
        """
        if features_df.empty:
            return np.array([]), []
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        if time_col in features_df.columns:
            features_df = features_df.sort_values(time_col).reset_index(drop=True)
        
        # ìˆ˜ì¹˜í˜• íŠ¹ì§•ë§Œ ì„ íƒ
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return np.array([]), []
        
        features = features_df[numeric_cols].values
        
        sequences = []
        sequence_indices = []
        
        for i in range(0, len(features) - self.window_size + 1, self.step_size):
            sequence = features[i:i + self.window_size]
            sequences.append(sequence)
            sequence_indices.append((i, i + self.window_size))
        
        return np.array(sequences), sequence_indices
    
    def train(self, sequences):
        """
        ì‹œê³„ì—´ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ
        
        Args:
            sequences: ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°°ì—´
        """
        if len(sequences) == 0:
            print("âš ï¸ ì‹œí€€ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“Š ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        print(f"   ì‹œí€€ìŠ¤ ìˆ˜: {len(sequences)}")
        print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´: {self.window_size}")
        print(f"   íŠ¹ì§• ì°¨ì›: {sequences.shape[2] if len(sequences.shape) > 2 else sequences.shape[1]}")
        
        # ì‹œí€€ìŠ¤ë¥¼ 2Dë¡œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥ìš©)
        n_samples, seq_len, n_features = sequences.shape
        X = sequences.reshape(n_samples, seq_len * n_features)
        
        # ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X)
        
        # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
        try:
            if self.model_type == 'autoencoder':
                # AutoEncoderëŠ” ì‹œí€€ìŠ¤ í˜•íƒœë¡œ í•™ìŠµ ê°€ëŠ¥
                # 3D â†’ 2D ë³€í™˜ í›„ í•™ìŠµ
                self.model = AutoEncoder(
                    contamination=0.1,
                    hidden_neurons=[128, 64, 32, 64, 128],
                    epochs=50,
                    batch_size=32,
                    verbose=0,
                    random_state=42
                )
            elif self.model_type == 'isolation_forest':
                self.model = IForest(contamination=0.1, random_state=42)
            elif self.model_type == 'lof':
                self.model = LOF(contamination=0.1, n_neighbors=20)
            elif self.model_type == 'copod':
                self.model = COPOD(contamination=0.1)
            else:
                raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
            
            self.model.fit(X_scaled)
            self.is_fitted = True
            print(f"   âœ… {self.model_type} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"   âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            return False
    
    def predict(self, sequences):
        """
        ì‹œê³„ì—´ ì´ìƒ íƒì§€
        
        Args:
            sequences: ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°°ì—´
        
        Returns:
            predictions: ì´ìƒì¹˜ ì˜ˆì¸¡ (1=ì´ìƒ, 0=ì •ìƒ)
            scores: ì´ìƒ ì ìˆ˜
        """
        if not self.is_fitted:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        if len(sequences) == 0:
            return np.array([]), np.array([])
        
        # ì‹œí€€ìŠ¤ë¥¼ 2Dë¡œ ë³€í™˜
        n_samples, seq_len, n_features = sequences.shape
        X = sequences.reshape(n_samples, seq_len * n_features)
        
        # ì •ê·œí™”
        X_scaled = self.scaler.transform(X)
        
        # ì˜ˆì¸¡
        predictions = self.model.predict(X_scaled)
        scores = self.model.decision_function(X_scaled)
        
        return predictions, scores


class TemporalAnomalyWithSeverity:
    """ì‹œê³„ì—´ ì´ìƒ íƒì§€ + ì‹¬ê°ë„ í‰ê°€ í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self, window_size=10, model_type='autoencoder'):
        """
        Args:
            window_size: ì‹œê³„ì—´ ìœˆë„ìš° í¬ê¸°
            model_type: ì´ìƒ íƒì§€ ëª¨ë¸ íƒ€ì…
        """
        self.window_size = window_size
        self.detector = TemporalLogAnomalyDetector(
            window_size=window_size,
            model_type=model_type
        )
        self.severity_assessor = SeverityAssessment()
        self.logs_df = None
        self.features_df = None
        
    def prepare_features(self, logs_df):
        """
        ë¡œê·¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            logs_df: íŒŒì‹±ëœ ë¡œê·¸ DataFrame
        """
        from log_anomaly_detector import LogAnomalyDetector
        
        print("=" * 60)
        print("íŠ¹ì§• ì¶”ì¶œ")
        print("=" * 60)
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ íŠ¹ì§• ì¶”ì¶œ
        detector = LogAnomalyDetector()
        self.features_df = detector.extract_features(logs_df)
        
        if self.features_df.empty:
            print("âš ï¸ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨")
            return False
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        if 'time_window' in self.features_df.columns:
            self.features_df = self.features_df.sort_values('time_window').reset_index(drop=True)
        
        print(f"âœ… {len(self.features_df)}ê°œ ì‹œê°„ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        self.logs_df = logs_df
        return True
    
    def train(self, train_ratio=0.8):
        """
        ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ
        
        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
        """
        if self.features_df is None or self.features_df.empty:
            print("âš ï¸ íŠ¹ì§• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print("\n" + "=" * 60)
        print("ì‹œê³„ì—´ ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµ")
        print("=" * 60)
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
        split_idx = int(len(self.features_df) * train_ratio)
        train_features = self.features_df.iloc[:split_idx]
        
        print(f"í•™ìŠµ ë°ì´í„°: {len(train_features)}ê°œ ìœˆë„ìš°")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.features_df) - split_idx}ê°œ ìœˆë„ìš°")
        
        # ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±
        train_sequences, _ = self.detector.create_sequences(
            train_features,
            time_col='time_window' if 'time_window' in train_features.columns else None
        )
        
        if len(train_sequences) == 0:
            print("âš ï¸ í•™ìŠµ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return False
        
        # ëª¨ë¸ í•™ìŠµ
        return self.detector.train(train_sequences)
    
    def detect_anomalies(self):
        """
        ì‹œê³„ì—´ ê¸°ë°˜ ì´ìƒ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€
        
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not self.detector.is_fitted:
            print("âš ï¸ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        print("\n" + "=" * 60)
        print("ì‹œê³„ì—´ ê¸°ë°˜ ì´ìƒ íƒì§€")
        print("=" * 60)
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì‹œí€€ìŠ¤ ìƒì„±
        all_sequences, sequence_indices = self.detector.create_sequences(
            self.features_df,
            time_col='time_window' if 'time_window' in self.features_df.columns else None
        )
        
        if len(all_sequences) == 0:
            print("âš ï¸ ì‹œí€€ìŠ¤ ìƒì„± ì‹¤íŒ¨")
            return {}
        
        # ì´ìƒ íƒì§€
        predictions, scores = self.detector.predict(all_sequences)
        
        # ì´ìƒì¹˜ë¡œ íƒì§€ëœ ì‹œí€€ìŠ¤ í•„í„°ë§
        anomaly_indices = np.where(predictions == 1)[0]
        
        print(f"âœ… {len(anomaly_indices)}ê°œ ì´ìƒ ì‹œí€€ìŠ¤ íƒì§€")
        
        # ê° ì´ìƒ ì‹œí€€ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ ì¶”ì¶œ
        anomaly_results = []
        
        for idx in anomaly_indices:
            start_idx, end_idx = sequence_indices[idx]
            
            # í•´ë‹¹ ì‹œí€€ìŠ¤ì˜ ì‹œê°„ ìœˆë„ìš°ë“¤
            sequence_windows = self.features_df.iloc[start_idx:end_idx]
            
            if 'time_window' in sequence_windows.columns:
                time_windows = sequence_windows['time_window'].unique()
                
                # í•´ë‹¹ ì‹œê°„ ìœˆë„ìš°ì˜ ë¡œê·¸ë“¤ ì¶”ì¶œ
                sequence_logs = self.logs_df[
                    self.logs_df['timestamp'].dt.floor('10T').isin(time_windows)
                ].copy()
                
                if not sequence_logs.empty:
                    # ì‹¬ê°ë„ í‰ê°€
                    severity_info = self.severity_assessor.assess_time_window_severity(
                        sequence_logs
                    )
                    
                    anomaly_results.append({
                        'sequence_index': idx,
                        'time_windows': list(time_windows),
                        'start_time': time_windows[0] if len(time_windows) > 0 else None,
                        'end_time': time_windows[-1] if len(time_windows) > 0 else None,
                        'anomaly_score': scores[idx],
                        'log_count': len(sequence_logs),
                        'max_severity_score': severity_info['max_severity_score'],
                        'max_severity_level': severity_info['max_severity_level'],
                        'avg_severity_score': severity_info['avg_severity_score'],
                        'critical_count': severity_info['critical_count'],
                        'high_count': severity_info['high_count'],
                        'medium_count': severity_info['medium_count'],
                        'low_count': severity_info['low_count'],
                        'logs': sequence_logs  # ì›ë³¸ ë¡œê·¸ í¬í•¨
                    })
        
        # ê²°ê³¼ ì •ë¦¬
        results_df = pd.DataFrame([
            {
                k: v for k, v in result.items() 
                if k != 'logs'  # ë¡œê·¸ëŠ” ë³„ë„ ì €ì¥
            }
            for result in anomaly_results
        ])
        
        # ì‹¬ê°ë„ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        if not results_df.empty and 'max_severity_score' in results_df.columns:
            results_df = results_df.sort_values(
                'max_severity_score',
                ascending=False
            )
            results_df['priority'] = range(1, len(results_df) + 1)
        
        return {
            'anomaly_sequences': results_df,
            'anomaly_logs': pd.concat([r['logs'] for r in anomaly_results], ignore_index=True) if anomaly_results else pd.DataFrame(),
            'total_anomalies': len(anomaly_results),
            'summary': self._generate_summary(results_df)
        }
    
    def _generate_summary(self, results_df):
        """ìš”ì•½ í†µê³„ ìƒì„±"""
        if results_df.empty:
            return {}
        
        return {
            'total_anomaly_sequences': len(results_df),
            'by_severity': results_df['max_severity_level'].value_counts().to_dict() if 'max_severity_level' in results_df.columns else {},
            'avg_severity_score': results_df['max_severity_score'].mean() if 'max_severity_score' in results_df.columns else 0,
            'max_severity_score': results_df['max_severity_score'].max() if 'max_severity_score' in results_df.columns else 0,
            'avg_anomaly_score': results_df['anomaly_score'].mean() if 'anomaly_score' in results_df.columns else 0,
        }
    
    def generate_report(self, results):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ì‹œê³„ì—´ ì´ìƒ íƒì§€ + ì‹¬ê°ë„ í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        
        if not results or results.get('total_anomalies', 0) == 0:
            print("âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        summary = results.get('summary', {})
        
        print(f"\nğŸ“Š íƒì§€ ê²°ê³¼:")
        print(f"   ì´ ì´ìƒ ì‹œí€€ìŠ¤: {summary.get('total_anomaly_sequences', 0)}ê°œ")
        print(f"   í‰ê·  ì´ìƒ ì ìˆ˜: {summary.get('avg_anomaly_score', 0):.4f}")
        
        if 'by_severity' in summary:
            print(f"\nğŸ” ì‹¬ê°ë„ ë¶„í¬:")
            for level, count in summary['by_severity'].items():
                print(f"   {level}: {count}ê°œ")
        
        print(f"\n   í‰ê·  ì‹¬ê°ë„ ì ìˆ˜: {summary.get('avg_severity_score', 0):.2f}")
        print(f"   ìµœê³  ì‹¬ê°ë„ ì ìˆ˜: {summary.get('max_severity_score', 0):.2f}")
        
        # ìƒìœ„ 5ê°œ ì´ìƒ ì‹œí€€ìŠ¤
        anomaly_df = results.get('anomaly_sequences', pd.DataFrame())
        if not anomaly_df.empty:
            print(f"\nğŸš¨ ìƒìœ„ 5ê°œ ì´ìƒ ì‹œí€€ìŠ¤:")
            top_5 = anomaly_df.head(5)
            for idx, row in top_5.iterrows():
                print(f"\n   [{row.get('priority', 'N/A')}] ìš°ì„ ìˆœìœ„")
                print(f"   ì‹œê°„: {row.get('start_time')} ~ {row.get('end_time')}")
                print(f"   ì´ìƒ ì ìˆ˜: {row.get('anomaly_score', 0):.4f}")
                print(f"   ì‹¬ê°ë„: {row.get('max_severity_level', 'N/A')} ({row.get('max_severity_score', 0):.2f})")
                print(f"   ë¡œê·¸ ìˆ˜: {row.get('log_count', 0)}ê°œ")
                print(f"   ì‹¬ê°ë„ ë¶„í¬: CRITICAL={row.get('critical_count', 0)}, HIGH={row.get('high_count', 0)}, MEDIUM={row.get('medium_count', 0)}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from log_anomaly_detector import SpringBootLogParser
    
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    print("=" * 60)
    print("ì‹œê³„ì—´ ê¸°ë°˜ ì´ìƒ íƒì§€ + ì‹¬ê°ë„ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # 1. ë¡œê·¸ íŒŒì‹±
    print("\n1ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ íŒŒì‹±")
    parser = SpringBootLogParser()
    logs_df = parser.parse_directory(
        log_directory,
        max_files=None,
        sample_lines=None
    )
    
    if logs_df.empty:
        print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
    
    # 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n2ë‹¨ê³„: ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    system = TemporalAnomalyWithSeverity(
        window_size=10,  # 10ê°œ ì‹œê°„ ìœˆë„ìš° ì‹œí€€ìŠ¤
        model_type='autoencoder'  # ë˜ëŠ” 'isolation_forest', 'lof', 'copod'
    )
    
    # 3. íŠ¹ì§• ì¶”ì¶œ
    print("\n3ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œ")
    if not system.prepare_features(logs_df):
        return
    
    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n4ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ")
    if not system.train(train_ratio=0.8):
        return
    
    # 5. ì´ìƒ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€
    print("\n5ë‹¨ê³„: ì´ìƒ íƒì§€ ë° ì‹¬ê°ë„ í‰ê°€")
    results = system.detect_anomalies()
    
    # 6. ë¦¬í¬íŠ¸ ìƒì„±
    print("\n6ë‹¨ê³„: ê²°ê³¼ ë¦¬í¬íŠ¸")
    system.generate_report(results)
    
    # 7. ê²°ê³¼ ì €ì¥
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results/temporal"
    os.makedirs(output_dir, exist_ok=True)
    
    if results and not results.get('anomaly_sequences', pd.DataFrame()).empty:
        # ì´ìƒ ì‹œí€€ìŠ¤ ê²°ê³¼ ì €ì¥
        anomaly_path = os.path.join(output_dir, "temporal_anomalies.csv")
        results['anomaly_sequences'].to_csv(anomaly_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì´ìƒ ì‹œí€€ìŠ¤ ê²°ê³¼ ì €ì¥: {anomaly_path}")
        
        # ì´ìƒ ë¡œê·¸ ì €ì¥
        if not results.get('anomaly_logs', pd.DataFrame()).empty:
            logs_path = os.path.join(output_dir, "temporal_anomaly_logs.csv")
            results['anomaly_logs'].to_csv(logs_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ì´ìƒ ë¡œê·¸ ì €ì¥: {logs_path}")
    
    print("\n" + "=" * 60)
    print("âœ… ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()














