"""
ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ
- ì •ìƒ ë¡œê·¸ë§Œ í•™ìŠµ
- ì—ëŸ¬ ë¡œê·¸ë¥¼ ì´ìƒì¹˜ë¡œ íƒì§€
- ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
"""

import re
import os
import glob
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
from collections import Counter
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve
)
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
from pyod.models.ocsvm import OCSVM
from pyod.models.copod import COPOD
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from severity_assessment import SeverityAssessment, add_severity_to_anomaly_results


class LogFeatureExtractor:
    """ë¡œê·¸ íŠ¹ì§• ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False
        
    def extract_features(self, log_df):
        """
        ë¡œê·¸ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
        
        Args:
            log_df: íŒŒì‹±ëœ ë¡œê·¸ DataFrame
        
        Returns:
            numpy array: íŠ¹ì§• ë²¡í„°
        """
        features = []
        
        for idx, row in log_df.iterrows():
            feature = []
            
            # 1. ë¡œê·¸ ë ˆë²¨ ì›-í•« ì¸ì½”ë”©
            level_map = {'DEBUG': 0, 'INFO': 1, 'WARN': 2, 'ERROR': 3, 'FATAL': 4}
            level = row.get('level', 'INFO')
            level_encoded = [0] * 5
            if level in level_map:
                level_encoded[level_map[level]] = 1
            feature.extend(level_encoded)
            
            # 2. ì—ëŸ¬ ì—¬ë¶€ (ì´ê±´ ë¼ë²¨ì´ë¯€ë¡œ ì œì™¸í•  ìˆ˜ë„ ìˆìŒ)
            # feature.append(1 if row.get('is_error', False) else 0)
            
            # 3. ì˜ˆì™¸ ì—¬ë¶€
            feature.append(1 if row.get('has_exception', False) else 0)
            
            # 4. ë©”ì‹œì§€ ê¸¸ì´
            feature.append(row.get('message_length', 0))
            
            # 5. í”„ë¡œì„¸ìŠ¤ ID (ì •ê·œí™”)
            pid = str(row.get('pid', '0'))
            try:
                feature.append(int(pid) % 1000 / 1000.0)
            except:
                feature.append(0.0)
            
            # 6. í´ë˜ìŠ¤ ê²½ë¡œ í•´ì‹œ (ì •ê·œí™”)
            class_path = str(row.get('class_path', ''))
            feature.append((hash(class_path) % 1000) / 1000.0)
            
            # 7. ìŠ¤ë ˆë“œëª… í•´ì‹œ (ì •ê·œí™”)
            thread = str(row.get('thread', ''))
            feature.append((hash(thread) % 1000) / 1000.0)
            
            # 8. ë©”ì‹œì§€ì— íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
            message = str(row.get('message', '')).lower()
            keywords = ['exception', 'error', 'failed', 'timeout', 'connection', 
                       'null', 'stack', 'trace', 'warning', 'critical']
            for keyword in keywords:
                feature.append(1 if keyword in message else 0)
            
            # 9. ë©”ì‹œì§€ ë‹¨ì–´ ìˆ˜
            word_count = len(message.split())
            feature.append(word_count)
            
            # 10. íŠ¹ìˆ˜ ë¬¸ì ë¹„ìœ¨
            special_chars = sum(1 for c in message if not c.isalnum() and c != ' ')
            feature.append(special_chars / max(len(message), 1))
            
            features.append(feature)
        
        return np.array(features)
    
    def fit_transform(self, log_df):
        """íŠ¹ì§• ì¶”ì¶œ ë° ì •ê·œí™”"""
        features = self.extract_features(log_df)
        features_scaled = self.scaler.fit_transform(features)
        self.is_fitted = True
        return features_scaled
    
    def transform(self, log_df):
        """ìƒˆë¡œìš´ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ"""
        if not self.is_fitted:
            raise ValueError("Scalerê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit_transform()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        features = self.extract_features(log_df)
        return self.scaler.transform(features)


class LogAnomalyModelComparator:
    """ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, models_config=None):
        """
        Args:
            models_config: ëª¨ë¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
                ì˜ˆ: {'isolation_forest': IForest(...), 'autoencoder': AutoEncoder(...)}
        """
        if models_config is None:
            # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
            self.models_config = {
                'Isolation Forest': {
                    'model': IForest(contamination=0.1, random_state=42, n_estimators=100),
                    'description': 'Isolation Forest - íŠ¸ë¦¬ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€'
                },
                'AutoEncoder': {
                    'model': None,  # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ë™ì  ìƒì„±
                    'description': 'AutoEncoder - ì‹ ê²½ë§ ê¸°ë°˜ ì¬êµ¬ì„± ì˜¤ì°¨ íƒì§€'
                },
                'LOF': {
                    'model': LOF(contamination=0.1, n_neighbors=20),
                    'description': 'Local Outlier Factor - ì§€ì—­ì  ì´ìƒì¹˜ íƒì§€'
                },
                'OCSVM': {
                    'model': OCSVM(contamination=0.1, kernel='rbf'),
                    'description': 'One-Class SVM - ì„œí¬íŠ¸ ë²¡í„° ê¸°ë°˜ íƒì§€'
                },
                'COPOD': {
                    'model': COPOD(contamination=0.1),
                    'description': 'COPOD - Copula ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€'
                }
            }
        else:
            self.models_config = models_config
        
        self.feature_extractor = LogFeatureExtractor()
        self.trained_models = {}
        self.results = {}
        
    def prepare_data(self, normal_logs_df, error_logs_df, train_ratio=0.8, valid_ratio=0.2):
        """
        ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• 
        - ì „ì²´ì˜ 80% â†’ train
        - trainì˜ 20% â†’ valid (ì „ì²´ì˜ 16%)
        - ë‚˜ë¨¸ì§€ 20% â†’ test (ì „ì²´ì˜ 20%)
        
        Args:
            normal_logs_df: ì •ìƒ ë¡œê·¸ DataFrame
            error_logs_df: ì—ëŸ¬ ë¡œê·¸ DataFrame
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.8 = 80%)
            valid_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (trainì˜ ë¹„ìœ¨, ê¸°ë³¸ 0.2 = 20%)
        
        Returns:
            dict: í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°
        """
        print("=" * 60)
        print("ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• ")
        print("=" * 60)
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        normal_logs_df = normal_logs_df.sort_values('timestamp').reset_index(drop=True)
        error_logs_df = error_logs_df.sort_values('timestamp').reset_index(drop=True)
        
        # ì „ì²´ ë¡œê·¸ (ì •ìƒ + ì—ëŸ¬)ë¥¼ ì‹œê°„ ìˆœì„œë¡œ ê²°í•©
        all_logs = pd.concat([normal_logs_df, error_logs_df], ignore_index=True)
        all_logs = all_logs.sort_values('timestamp').reset_index(drop=True)
        
        # ì „ì²´ ë°ì´í„° ë¶„í• : 80% train, 20% test
        total_split_idx = int(len(all_logs) * train_ratio)
        train_all = all_logs.iloc[:total_split_idx]
        test_all = all_logs.iloc[total_split_idx:]
        
        # Train ë°ì´í„°ì—ì„œ ì •ìƒ/ì—ëŸ¬ ë¶„ë¦¬
        train_normal_df = train_all[train_all['is_error'] == False].copy()
        train_error_df = train_all[train_all['is_error'] == True].copy()
        
        # Trainì˜ 20%ë¥¼ Validë¡œ ë¶„í•  (ì •ìƒ ë¡œê·¸ ê¸°ì¤€)
        train_normal_split_idx = int(len(train_normal_df) * (1 - valid_ratio))
        train_normal_final = train_normal_df.iloc[:train_normal_split_idx]
        valid_normal_df = train_normal_df.iloc[train_normal_split_idx:]
        
        # ì—ëŸ¬ ë¡œê·¸ë„ trainì˜ 20%ë¥¼ validë¡œ
        if len(train_error_df) > 0:
            train_error_split_idx = int(len(train_error_df) * (1 - valid_ratio))
            train_error_final = train_error_df.iloc[:train_error_split_idx]
            valid_error_df = train_error_df.iloc[train_error_split_idx:]
        else:
            train_error_final = train_error_df
            valid_error_df = pd.DataFrame()
        
        # Test ë°ì´í„°ì—ì„œ ì •ìƒ/ì—ëŸ¬ ë¶„ë¦¬
        test_normal_df = test_all[test_all['is_error'] == False].copy()
        test_error_df = test_all[test_all['is_error'] == True].copy()
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
        print(f"   Train (í•™ìŠµìš©):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(train_normal_final)}ê°œ ({len(train_normal_final)/len(all_logs)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(train_error_final)}ê°œ")
        print(f"      - ì „ì²´: {len(train_normal_final) + len(train_error_final)}ê°œ ({len(train_all)/len(all_logs)*100:.1f}%)")
        print(f"\n   Valid (ê²€ì¦ìš©, trainì˜ {valid_ratio*100:.0f}%):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(valid_normal_df)}ê°œ ({len(valid_normal_df)/len(all_logs)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(valid_error_df)}ê°œ")
        print(f"      - ì „ì²´: {len(valid_normal_df) + len(valid_error_df)}ê°œ ({(len(valid_normal_df) + len(valid_error_df))/len(all_logs)*100:.1f}%)")
        print(f"\n   Test (í…ŒìŠ¤íŠ¸ìš©):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(test_normal_df)}ê°œ ({len(test_normal_df)/len(all_logs)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(test_error_df)}ê°œ ({len(test_error_df)/len(all_logs)*100:.1f}%)")
        print(f"      - ì „ì²´: {len(test_normal_df) + len(test_error_df)}ê°œ ({len(test_all)/len(all_logs)*100:.1f}%)")
        
        # íŠ¹ì§• ì¶”ì¶œ
        print(f"\níŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        X_train = self.feature_extractor.fit_transform(train_normal_final)
        X_valid_normal = self.feature_extractor.transform(valid_normal_df) if not valid_normal_df.empty else np.array([]).reshape(0, X_train.shape[1])
        X_test_normal = self.feature_extractor.transform(test_normal_df)
        X_test_error = self.feature_extractor.transform(test_error_df)
        
        # Valid ë°ì´í„° ê²°í•©
        if len(X_valid_normal) > 0:
            X_valid_error = self.feature_extractor.transform(valid_error_df) if not valid_error_df.empty else np.array([]).reshape(0, X_train.shape[1])
            if len(X_valid_error) > 0:
                X_valid = np.vstack([X_valid_normal, X_valid_error])
                y_valid = np.hstack([
                    np.zeros(len(X_valid_normal)),  # ì •ìƒ = 0
                    np.ones(len(X_valid_error))      # ì´ìƒ = 1
                ])
            else:
                X_valid = X_valid_normal
                y_valid = np.zeros(len(X_valid_normal))
        else:
            X_valid = np.array([]).reshape(0, X_train.shape[1])
            y_valid = np.array([])
        
        # Test ë°ì´í„° ê²°í•©
        X_test = np.vstack([X_test_normal, X_test_error])
        y_test = np.hstack([
            np.zeros(len(X_test_normal)),  # ì •ìƒ = 0
            np.ones(len(X_test_error))      # ì´ìƒ = 1
        ])
        
        print(f"   - í•™ìŠµ íŠ¹ì§• ì°¨ì›: {X_train.shape}")
        if len(X_valid) > 0:
            print(f"   - ê²€ì¦ íŠ¹ì§• ì°¨ì›: {X_valid.shape}")
        print(f"   - í…ŒìŠ¤íŠ¸ íŠ¹ì§• ì°¨ì›: {X_test.shape}")
        if len(y_valid) > 0:
            print(f"   - ê²€ì¦ ë¼ë²¨: ì •ìƒ {np.sum(y_valid==0)}ê°œ, ì´ìƒ {np.sum(y_valid==1)}ê°œ")
        print(f"   - í…ŒìŠ¤íŠ¸ ë¼ë²¨: ì •ìƒ {np.sum(y_test==0)}ê°œ, ì´ìƒ {np.sum(y_test==1)}ê°œ")
        
        return {
            'X_train': X_train,
            'X_valid': X_valid,
            'X_test': X_test,
            'y_valid': y_valid,
            'y_test': y_test,
            'train_normal_df': train_normal_final,
            'valid_normal_df': valid_normal_df,
            'valid_error_df': valid_error_df,
            'test_normal_df': test_normal_df,
            'test_error_df': test_error_df
        }
    
    def train_models(self, X_train, selected_models=None):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            selected_models: í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ëª¨ë¸)
        """
        print("\n" + "=" * 60)
        print("ëª¨ë¸ í•™ìŠµ")
        print("=" * 60)
        
        if selected_models is None:
            selected_models = list(self.models_config.keys())
        
        n_samples, n_features = X_train.shape
        
        for model_name in selected_models:
            if model_name not in self.models_config:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: {model_name}")
                continue
            
            print(f"\n[{model_name}] í•™ìŠµ ì¤‘...")
            print(f"   ì„¤ëª…: {self.models_config[model_name]['description']}")
            
            try:
                # AutoEncoderëŠ” ë™ì  ìƒì„±
                if model_name == 'AutoEncoder':
                    # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¡°ì •
                    if n_samples < 10:
                        print(f"   âš ï¸ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({n_samples}ê°œ). ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    if n_features < 50:
                        hidden_neurons = [max(8, n_features//2), max(4, n_features//4), max(8, n_features//2)]
                    elif n_features < 200:
                        hidden_neurons = [64, 32, 16, 32, 64]
                    else:
                        hidden_neurons = [128, 64, 32, 64, 128]
                    
                    if n_samples < 50:
                        epochs = 20
                        batch_size = min(8, n_samples)
                    elif n_samples < 100:
                        epochs = 30
                        batch_size = 16
                    else:
                        epochs = 50
                        batch_size = 32
                    
                    try:
                        model = AutoEncoder(
                            contamination=0.1,
                            hidden_neurons=hidden_neurons,
                            epochs=epochs,
                            batch_size=batch_size,
                            dropout_rate=0.2,
                            verbose=0,
                            random_state=42
                        )
                    except TypeError:
                        model = AutoEncoder(
                            contamination=0.1,
                            hidden_neuron_list=hidden_neurons,
                            epoch_num=epochs,
                            batch_size=batch_size,
                            dropout_rate=0.2,
                            verbose=0,
                            random_state=42
                        )
                else:
                    model = self.models_config[model_name]['model']
                
                # ëª¨ë¸ í•™ìŠµ
                model.fit(X_train)
                self.trained_models[model_name] = model
                print(f"   âœ… í•™ìŠµ ì™„ë£Œ")
                
            except Exception as e:
                print(f"   âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
                continue
    
    def evaluate_models(self, X_test, y_test, test_logs_df=None):
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨ (0=ì •ìƒ, 1=ì´ìƒ)
            test_logs_df: í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame (ì‹¬ê°ë„ í‰ê°€ìš©)
        """
        print("\n" + "=" * 60)
        print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        
        results = {}
        severity_assessor = SeverityAssessment()
        
        for model_name, model in self.trained_models.items():
            print(f"\n[{model_name}] í‰ê°€ ì¤‘...")
            
            try:
                # ì˜ˆì¸¡
                y_pred = model.predict(X_test)  # 1=ì´ìƒ, 0=ì •ìƒ
                y_scores = model.decision_function(X_test)  # ì´ìƒ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì´ìƒ)
                
                # ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (ì¼ë¶€ ëª¨ë¸ì€ ìŒìˆ˜ ì ìˆ˜ ì‚¬ìš©)
                # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì´ìƒì´ë¯€ë¡œ, -scoreë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì •ê·œí™”
                if y_scores.min() < 0:
                    # ìŒìˆ˜ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ ë³€í™˜ (ë‚®ì€ ì ìˆ˜ = ë†’ì€ ì´ìƒ í™•ë¥ )
                    y_scores_normalized = -y_scores
                    y_scores_normalized = (y_scores_normalized - y_scores_normalized.min()) / (
                        y_scores_normalized.max() - y_scores_normalized.min() + 1e-8
                    )
                else:
                    y_scores_normalized = 1 - (y_scores - y_scores.min()) / (
                        y_scores.max() - y_scores.min() + 1e-8
                    )
                
                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                accuracy = accuracy_score(y_test, y_pred)
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                
                # ROC-AUC (ì¼ë¶€ ëª¨ë¸ì€ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                try:
                    roc_auc = roc_auc_score(y_test, y_scores_normalized)
                except:
                    roc_auc = None
                
                # í˜¼ë™ í–‰ë ¬
                cm = confusion_matrix(y_test, y_pred)
                tn, fp, fn, tp = cm.ravel()
                
                # ì¶”ê°€ ì§€í‘œ
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
                false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
                
                # ì‹¬ê°ë„ í‰ê°€ (ì´ìƒì¹˜ë¡œ íƒì§€ëœ ë¡œê·¸ë“¤)
                severity_info = None
                if test_logs_df is not None and len(test_logs_df) == len(y_pred):
                    # ì´ìƒì¹˜ë¡œ íƒì§€ëœ ë¡œê·¸ë“¤ (y_pred == 1)
                    detected_anomalies = test_logs_df[y_pred == 1].copy()
                    
                    if not detected_anomalies.empty:
                        # ì‹¬ê°ë„ í‰ê°€
                        detected_anomalies = severity_assessor.assess_anomaly_severity(detected_anomalies)
                        
                        # ì‹¬ê°ë„ í†µê³„
                        severity_summary = severity_assessor.generate_severity_summary(detected_anomalies)
                        severity_info = {
                            'detected_anomalies': detected_anomalies,
                            'summary': severity_summary
                        }
                
                results[model_name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'roc_auc': roc_auc,
                    'specificity': specificity,
                    'false_positive_rate': false_positive_rate,
                    'false_negative_rate': false_negative_rate,
                    'confusion_matrix': cm,
                    'y_pred': y_pred,
                    'y_scores': y_scores_normalized,
                    'tp': tp,
                    'tn': tn,
                    'fp': fp,
                    'fn': fn,
                    'severity_info': severity_info
                }
                
                print(f"   ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
                print(f"   ì •ë°€ë„: {precision:.4f} ({precision*100:.2f}%)")
                print(f"   ì¬í˜„ìœ¨: {recall:.4f} ({recall*100:.2f}%)")
                print(f"   F1 ì ìˆ˜: {f1:.4f}")
                if roc_auc:
                    print(f"   ROC-AUC: {roc_auc:.4f}")
                print(f"   íŠ¹ì´ë„: {specificity:.4f} ({specificity*100:.2f}%)")
                print(f"   í˜¼ë™ í–‰ë ¬:")
                print(f"      [ì •ìƒâ†’ì •ìƒ: {tn:4d}  ì •ìƒâ†’ì´ìƒ: {fp:4d}]")
                print(f"      [ì´ìƒâ†’ì •ìƒ: {fn:4d}  ì´ìƒâ†’ì´ìƒ: {tp:4d}]")
                
                # ì‹¬ê°ë„ ì •ë³´ ì¶œë ¥
                if severity_info and severity_info['summary']:
                    summary = severity_info['summary']
                    print(f"\n   ğŸ” ì‹¬ê°ë„ ë¶„ì„:")
                    print(f"      íƒì§€ëœ ì´ìƒì¹˜: {summary.get('total_anomalies', 0)}ê°œ")
                    if 'by_severity' in summary:
                        print(f"      ì‹¬ê°ë„ ë¶„í¬:")
                        for level, count in summary['by_severity'].items():
                            print(f"        {level}: {count}ê°œ")
                    if 'avg_severity_score' in summary:
                        print(f"      í‰ê·  ì‹¬ê°ë„ ì ìˆ˜: {summary['avg_severity_score']:.2f}")
                    if 'max_severity_score' in summary:
                        print(f"      ìµœê³  ì‹¬ê°ë„ ì ìˆ˜: {summary['max_severity_score']:.2f}")
                
            except Exception as e:
                print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        self.results = results
        return results
    
    def generate_comparison_report(self, output_dir=None):
        """ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "=" * 60)
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # ì„±ëŠ¥ ì§€í‘œ ë¹„êµ í…Œì´ë¸”
        comparison_data = []
        for model_name, metrics in self.results.items():
            comparison_data.append({
                'ëª¨ë¸': model_name,
                'ì •í™•ë„': f"{metrics['accuracy']:.4f}",
                'ì •ë°€ë„': f"{metrics['precision']:.4f}",
                'ì¬í˜„ìœ¨': f"{metrics['recall']:.4f}",
                'F1 ì ìˆ˜': f"{metrics['f1_score']:.4f}",
                'ROC-AUC': f"{metrics['roc_auc']:.4f}" if metrics['roc_auc'] else "N/A",
                'íŠ¹ì´ë„': f"{metrics['specificity']:.4f}",
                'TP': metrics['tp'],
                'TN': metrics['tn'],
                'FP': metrics['fp'],
                'FN': metrics['fn']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        print("\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë¹„êµ:")
        print(comparison_df.to_string(index=False))
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        print("\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
        best_f1 = max(self.results.items(), key=lambda x: x[1]['f1_score'])
        best_accuracy = max(self.results.items(), key=lambda x: x[1]['accuracy'])
        best_recall = max(self.results.items(), key=lambda x: x[1]['recall'])
        
        print(f"   ìµœê³  F1 ì ìˆ˜: {best_f1[0]} ({best_f1[1]['f1_score']:.4f})")
        print(f"   ìµœê³  ì •í™•ë„: {best_accuracy[0]} ({best_accuracy[1]['accuracy']:.4f})")
        print(f"   ìµœê³  ì¬í˜„ìœ¨: {best_recall[0]} ({best_recall[1]['recall']:.4f})")
        
        # ê²°ê³¼ ì €ì¥
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
            # ë¹„êµ í…Œì´ë¸” ì €ì¥
            comparison_path = os.path.join(output_dir, "model_comparison.csv")
            comparison_df.to_csv(comparison_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            for model_name, metrics in self.results.items():
                detail_path = os.path.join(output_dir, f"results_{model_name.replace(' ', '_')}.csv")
                detail_df = pd.DataFrame({
                    'y_true': [0] * len(metrics['y_pred']),  # ì‹¤ì œ ë¼ë²¨ì€ ë³„ë„ë¡œ ì €ì¥ í•„ìš”
                    'y_pred': metrics['y_pred'],
                    'y_score': metrics['y_scores']
                })
                detail_df.to_csv(detail_path, index=False, encoding='utf-8-sig')
                
                # ì‹¬ê°ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ë³„ë„ë¡œ ì €ì¥
                if metrics.get('severity_info') and metrics['severity_info'].get('detected_anomalies') is not None:
                    severity_path = os.path.join(output_dir, f"severity_{model_name.replace(' ', '_')}.csv")
                    severity_df = metrics['severity_info']['detected_anomalies']
                    # ìš°ì„ ìˆœìœ„ ì •ë ¬
                    severity_df = SeverityAssessment().prioritize_anomalies(severity_df)
                    severity_df.to_csv(severity_path, index=False, encoding='utf-8-sig')
                    print(f"ğŸ’¾ ì‹¬ê°ë„ ë¶„ì„ ê²°ê³¼ ì €ì¥: {severity_path}")
        
        return comparison_df
    
    def plot_roc_curves(self, y_test, output_path=None):
        """ROC ê³¡ì„  ì‹œê°í™”"""
        if not self.results:
            return
        
        plt.figure(figsize=(10, 8))
        
        for model_name, metrics in self.results.items():
            if metrics['roc_auc'] is not None:
                try:
                    fpr, tpr, _ = roc_curve(y_test, metrics['y_scores'])
                    plt.plot(fpr, tpr, label=f"{model_name} (AUC={metrics['roc_auc']:.3f})", linewidth=2)
                except:
                    continue
        
        plt.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ROC Curves Comparison', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ROC ê³¡ì„  ì €ì¥: {output_path}")
        
        plt.show()
    
    def plot_precision_recall_curves(self, y_test, output_path=None):
        """Precision-Recall ê³¡ì„  ì‹œê°í™”"""
        if not self.results:
            return
        
        plt.figure(figsize=(10, 8))
        
        for model_name, metrics in self.results.items():
            try:
                precision, recall, _ = precision_recall_curve(y_test, metrics['y_scores'])
                plt.plot(recall, precision, label=f"{model_name}", linewidth=2)
            except:
                continue
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall', fontsize=12)
        plt.ylabel('Precision', fontsize=12)
        plt.title('Precision-Recall Curves Comparison', fontsize=14, fontweight='bold')
        plt.legend(loc="lower left", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        if output_path:
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ Precision-Recall ê³¡ì„  ì €ì¥: {output_path}")
        
        plt.show()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from log_anomaly_detector import SpringBootLogParser
    
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    print("=" * 60)
    print("ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 60)
    
    # 1. ë¡œê·¸ íŒŒì‹± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    print("\n1ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ íŒŒì‹±")
    parser = SpringBootLogParser()
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
    # í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥:
    # - max_files: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
    # - sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ ë¼ì¸ ìˆ˜
    # - chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ 10,000)
    # - max_total_lines: ì „ì²´ ìµœëŒ€ ë¼ì¸ ìˆ˜
    logs_df = parser.parse_directory(
        log_directory,
        max_files=None,        # ì „ì²´ íŒŒì¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìˆ«ìë¡œ ì œí•œ)
        sample_lines=None,     # ì „ì²´ ë¼ì¸ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìˆ«ìë¡œ ì œí•œ)
        chunk_size=5000,       # ì²­í¬ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½, ê¸°ë³¸ê°’ë³´ë‹¤ ì‘ê²Œ)
        max_total_lines=None,  # ì „ì²´ ìµœëŒ€ ë¼ì¸ ìˆ˜ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì„¤ì •)
        save_chunks_to_disk=True  # íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    )
    
    if logs_df.empty:
        print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì´ {len(logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
    
    # 2. ì •ìƒ/ì—ëŸ¬ ë¡œê·¸ ë¶„ë¦¬
    print("\n2ë‹¨ê³„: ì •ìƒ/ì—ëŸ¬ ë¡œê·¸ ë¶„ë¦¬")
    # ì •ìƒ ë¡œê·¸: INFO, DEBUG, TRACEë§Œ í¬í•¨ (WARN ì œì™¸)
    # ì—ëŸ¬ ë¡œê·¸: ERROR, FATAL, WARN í¬í•¨
    normal_logs_df = logs_df[
        (logs_df['is_error'] == False) & 
        (logs_df['level'].isin(['INFO', 'DEBUG', 'TRACE']))
    ].copy()
    error_logs_df = logs_df[
        (logs_df['is_error'] == True) | 
        (logs_df['level'].isin(['WARN', 'ERROR', 'FATAL']))
    ].copy()
    
    print(f"   - ì •ìƒ ë¡œê·¸ (INFO/DEBUG/TRACEë§Œ): {len(normal_logs_df)}ê°œ")
    print(f"   - ì—ëŸ¬ ë¡œê·¸ (ERROR/FATAL/WARN í¬í•¨): {len(error_logs_df)}ê°œ")
    
    # ë ˆë²¨ë³„ í†µê³„
    level_counts = logs_df['level'].value_counts()
    print(f"\n   ë ˆë²¨ë³„ ë¶„í¬:")
    for level, count in level_counts.items():
        print(f"      {level}: {count}ê°œ ({count/len(logs_df)*100:.1f}%)")
    
    if len(normal_logs_df) == 0:
        print("âš ï¸ ì •ìƒ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if len(error_logs_df) == 0:
        print("âš ï¸ ì—ëŸ¬ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3. ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print("\n3ë‹¨ê³„: ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    # 2ê°œ ëª¨ë¸ ì„ íƒ (Isolation Forest, AutoEncoder)
    comparator = LogAnomalyModelComparator()
    selected_models = ['Isolation Forest', 'AutoEncoder']
    
    # 4. ë°ì´í„° ì¤€ë¹„
    print("\n4ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„")
    data = comparator.prepare_data(
        normal_logs_df=normal_logs_df,
        error_logs_df=error_logs_df,
        train_ratio=0.8,    # ì „ì²´ì˜ 80% â†’ train
        valid_ratio=0.2     # trainì˜ 20% â†’ valid (ì „ì²´ì˜ 16%)
    )
    
    # 5. ëª¨ë¸ í•™ìŠµ
    print("\n5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ")
    comparator.train_models(data['X_train'], selected_models=selected_models)
    
    if not comparator.trained_models:
        print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 6. ëª¨ë¸ í‰ê°€
    print("\n6ë‹¨ê³„: ëª¨ë¸ í‰ê°€")
    
    # ê²€ì¦ ë°ì´í„° í‰ê°€ (ì„ íƒì )
    if len(data['X_valid']) > 0 and len(data['y_valid']) > 0:
        print("\n6-1. ê²€ì¦ ë°ì´í„° í‰ê°€")
        valid_logs_list = [data['valid_normal_df']]
        if 'valid_error_df' in data and not data['valid_error_df'].empty:
            valid_logs_list.append(data['valid_error_df'])
        valid_logs_df = pd.concat(valid_logs_list, ignore_index=True) if valid_logs_list else pd.DataFrame()
        
        if not valid_logs_df.empty:
            valid_results = comparator.evaluate_models(data['X_valid'], data['y_valid'], test_logs_df=valid_logs_df)
            print("   âœ… ê²€ì¦ ë°ì´í„° í‰ê°€ ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€
    print("\n6-2. í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€")
    # í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame ì¤€ë¹„ (ì‹¬ê°ë„ í‰ê°€ìš©)
    test_logs_df = pd.concat([data['test_normal_df'], data['test_error_df']], ignore_index=True)
    results = comparator.evaluate_models(data['X_test'], data['y_test'], test_logs_df=test_logs_df)
    
    # 7. ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
    print("\n7ë‹¨ê³„: ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±")
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results/performance"
    os.makedirs(output_dir, exist_ok=True)
    
    comparison_df = comparator.generate_comparison_report(output_dir=output_dir)
    
    # 8. ì‹œê°í™”
    print("\n8ë‹¨ê³„: ì„±ëŠ¥ ê³¡ì„  ì‹œê°í™”")
    try:
        comparator.plot_roc_curves(
            data['y_test'],
            output_path=os.path.join(output_dir, "roc_curves.png")
        )
        comparator.plot_precision_recall_curves(
            data['y_test'],
            output_path=os.path.join(output_dir, "pr_curves.png")
        )
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    # 9. ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥
    print("\n9ë‹¨ê³„: ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥")
    report_path = os.path.join(output_dir, "performance_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸\n")
        f.write("=" * 60 + "\n\n")
        
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("ë°ì´í„° ì •ë³´:\n")
        f.write(f"  - ì „ì²´ ë¡œê·¸: {len(logs_df)}ê°œ\n")
        f.write(f"  - ì •ìƒ ë¡œê·¸: {len(normal_logs_df)}ê°œ\n")
        f.write(f"  - ì—ëŸ¬ ë¡œê·¸: {len(error_logs_df)}ê°œ\n")
        f.write(f"  - í•™ìŠµ ì •ìƒ ë¡œê·¸: {len(data['train_normal_df'])}ê°œ ({len(data['train_normal_df'])/len(logs_df)*100:.1f}%)\n")
        if 'valid_normal_df' in data:
            f.write(f"  - ê²€ì¦ ì •ìƒ ë¡œê·¸: {len(data['valid_normal_df'])}ê°œ ({len(data['valid_normal_df'])/len(logs_df)*100:.1f}%)\n")
        f.write(f"  - í…ŒìŠ¤íŠ¸ ì •ìƒ ë¡œê·¸: {len(data['test_normal_df'])}ê°œ ({len(data['test_normal_df'])/len(logs_df)*100:.1f}%)\n")
        f.write(f"  - í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ë¡œê·¸: {len(data['test_error_df'])}ê°œ ({len(data['test_error_df'])/len(logs_df)*100:.1f}%)\n\n")
        
        f.write("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n")
        f.write(comparison_df.to_string(index=False))
        f.write("\n\n")
        
        for model_name, metrics in results.items():
            f.write(f"[{model_name}] ìƒì„¸ ê²°ê³¼:\n")
            f.write(f"  ì •í™•ë„: {metrics['accuracy']:.4f}\n")
            f.write(f"  ì •ë°€ë„: {metrics['precision']:.4f}\n")
            f.write(f"  ì¬í˜„ìœ¨: {metrics['recall']:.4f}\n")
            f.write(f"  F1 ì ìˆ˜: {metrics['f1_score']:.4f}\n")
            if metrics['roc_auc']:
                f.write(f"  ROC-AUC: {metrics['roc_auc']:.4f}\n")
            f.write(f"  íŠ¹ì´ë„: {metrics['specificity']:.4f}\n")
            f.write(f"  í˜¼ë™ í–‰ë ¬:\n")
            f.write(f"    [ì •ìƒâ†’ì •ìƒ: {metrics['tn']:4d}  ì •ìƒâ†’ì´ìƒ: {metrics['fp']:4d}]\n")
            f.write(f"    [ì´ìƒâ†’ì •ìƒ: {metrics['fn']:4d}  ì´ìƒâ†’ì´ìƒ: {metrics['tp']:4d}]\n")
            
            # ì‹¬ê°ë„ ì •ë³´ ì¶”ê°€
            if metrics.get('severity_info') and metrics['severity_info'].get('summary'):
                summary = metrics['severity_info']['summary']
                f.write(f"\n  ì‹¬ê°ë„ ë¶„ì„:\n")
                f.write(f"    íƒì§€ëœ ì´ìƒì¹˜: {summary.get('total_anomalies', 0)}ê°œ\n")
                if 'by_severity' in summary:
                    f.write(f"    ì‹¬ê°ë„ ë¶„í¬:\n")
                    for level, count in summary['by_severity'].items():
                        f.write(f"      {level}: {count}ê°œ\n")
                if 'avg_severity_score' in summary:
                    f.write(f"    í‰ê·  ì‹¬ê°ë„ ì ìˆ˜: {summary['avg_severity_score']:.2f}\n")
                if 'max_severity_score' in summary:
                    f.write(f"    ìµœê³  ì‹¬ê°ë„ ì ìˆ˜: {summary['max_severity_score']:.2f}\n")
                if 'top_exceptions' in summary:
                    f.write(f"    ì£¼ìš” ì˜ˆì™¸ ìœ í˜•:\n")
                    for exc_type, count in list(summary['top_exceptions'].items())[:5]:
                        f.write(f"      {exc_type}: {count}íšŒ\n")
            f.write("\n")
    
    print(f"ğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    print("\n" + "=" * 60)
    print("ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 60)


if __name__ == "__main__":
    main()

