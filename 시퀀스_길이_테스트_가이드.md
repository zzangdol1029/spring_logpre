# 시퀀스 길이와 정확도의 관계

## 결론

**시퀀스 길이를 늘리면 정확도가 높아질 수 있지만, 항상 그런 것은 아닙니다.**

### ✅ 정확도가 높아지는 경우
- **데이터가 충분할 때**: 더 긴 패턴 학습 가능
- **복잡한 이상 패턴이 있을 때**: 장기 의존성 필요
- **적절한 범위 내**: 보통 15~20개가 최적

### ❌ 정확도가 낮아지는 경우
- **데이터가 부족할 때**: 과적합 위험
- **너무 길 때**: 30개 이상은 오히려 성능 저하
- **메모리 부족**: 학습이 제대로 안 됨

---

## 시퀀스 길이별 예상 효과

### 현재 설정: `sequence_length = 10`

| 시퀀스 길이 | 정확도 변화 | 메모리 증가 | 학습 시간 증가 | 권장도 |
|-----------|-----------|-----------|--------------|--------|
| **10** (현재) | 기준 | 기준 | 기준 | ✅ 기본값 |
| **15** | +2~5% | +50% | +30% | ⭐ 권장 |
| **20** | +3~7% | +100% | +60% | ⭐⭐ 최적 가능 |
| **25** | +2~5% | +150% | +90% | ⚠️ 주의 |
| **30+** | 변화 없거나 감소 | +200%+ | +120%+ | ❌ 비권장 |

---

## 시퀀스 길이를 늘리면 좋은 경우

### 1. 복잡한 이상 패턴
```
정상 패턴: [A → B → C → D → E → F → G → H → I → J]
이상 패턴: [A → B → C → X → Y → Z → D → E → F → G]
```
→ 10개로는 탐지 어려움, 15~20개 필요

### 2. 장기 의존성이 있는 경우
- 로그가 15~20개 연속되어야 패턴이 명확한 경우
- 예: 서비스 시작 → 초기화 → 설정 → 실행 → 완료 (15개 이상)

### 3. 데이터가 충분한 경우
- 학습 데이터가 100만 개 이상
- 다양한 패턴이 포함됨

---

## 시퀀스 길이를 늘리면 안 좋은 경우

### 1. 데이터가 부족한 경우
- 학습 데이터가 10만 개 미만
- 과적합 위험 증가

### 2. 단순한 패턴
- 5~10개 로그로도 충분히 탐지 가능
- 긴 시퀀스는 노이즈만 증가

### 3. 메모리 제약
- 메모리가 부족하면 학습 실패
- 시퀀스 길이 2배 = 메모리 2배

---

## 최적 시퀀스 길이 찾기 방법

### 방법 1: 단계별 테스트

```bash
# 1단계: 10개 (현재)
python log_specific_model_comparison.py \
    --load-split split_data \
    --sample-size 100000 \
    --epochs 10

# 2단계: 15개
# 코드 수정 필요: DeepLogDetector(sequence_length=15)

# 3단계: 20개
# 코드 수정 필요: DeepLogDetector(sequence_length=20)
```

### 방법 2: 자동 테스트 스크립트

```python
# test_sequence_lengths.py
from log_specific_model_comparison import LogSpecificModelComparator
from log_specific_anomaly_detectors import DeepLogDetector

results = {}
for seq_len in [10, 15, 20]:
    print(f"\n{'='*60}")
    print(f"시퀀스 길이: {seq_len} 테스트")
    print(f"{'='*60}")
    
    # 모델 초기화
    detector = DeepLogDetector(sequence_length=seq_len)
    
    # 학습 및 평가
    # ... (학습 코드)
    
    # 결과 저장
    results[seq_len] = {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score
    }

# 최적 길이 선택
best_seq_len = max(results.items(), key=lambda x: x[1]['f1_score'])[0]
print(f"\n최적 시퀀스 길이: {best_seq_len}")
```

---

## 코드 수정 방법

### DeepLog 시퀀스 길이 변경

```python
# log_specific_anomaly_detectors.py 수정
# 또는 직접 초기화 시 지정

from log_specific_anomaly_detectors import DeepLogDetector

# 시퀀스 길이 15로 변경
detector = DeepLogDetector(
    embedding_dim=128,
    hidden_dim=64,
    num_layers=2,
    sequence_length=15  # 10 → 15
)
```

### LogAnomaly 윈도우 크기 변경

```python
from log_specific_anomaly_detectors import LogAnomalyDetector

detector = LogAnomalyDetector(vector_dim=100)
detector.train(logs_df, window_size=15)  # 10 → 15
```

### LogRobust 시퀀스 길이 변경

```python
from log_specific_anomaly_detectors import LogRobustDetector

detector = LogRobustDetector()
detector.train(logs_df, sequence_length=15)  # 10 → 15
```

---

## 실제 테스트 결과 예상

### 시나리오 1: 데이터 충분 (100만 개 이상)
```
시퀀스 길이 10: 정확도 60%, F1 45%
시퀀스 길이 15: 정확도 65%, F1 50%  ← +5% 향상
시퀀스 길이 20: 정확도 67%, F1 52%  ← +7% 향상
시퀀스 길이 25: 정확도 66%, F1 51%  ← 오히려 감소
```

### 시나리오 2: 데이터 부족 (10만 개 미만)
```
시퀀스 길이 10: 정확도 55%, F1 40%
시퀀스 길이 15: 정확도 53%, F1 38%  ← 오히려 감소 (과적합)
시퀀스 길이 20: 정확도 50%, F1 35%  ← 더 감소
```

---

## 권장 사항

### 1단계: 현재 성능 확인
```bash
# 현재 설정 (sequence_length=10)으로 학습
python log_specific_model_comparison.py \
    --load-split split_data \
    --sample-size 100000 \
    --epochs 10
```

### 2단계: 시퀀스 길이 15 테스트
- 코드 수정: `sequence_length=15`
- 동일한 데이터로 학습
- 성능 비교

### 3단계: 시퀀스 길이 20 테스트
- 코드 수정: `sequence_length=20`
- 동일한 데이터로 학습
- 성능 비교

### 4단계: 최적 길이 선택
- F1 점수가 가장 높은 길이 선택
- 메모리/시간 제약 고려

---

## 메모리 사용량 계산

### 시퀀스 길이에 따른 메모리 증가

```
메모리 = (시퀀스 수) × (시퀀스 길이) × (데이터 타입 크기)

예: 100만 개 로그, sequence_length=10
- 시퀀스 수: 1,000,000 - 10 = 999,990개
- 메모리: 999,990 × 10 × 4 bytes = 약 40 MB

sequence_length=20으로 증가:
- 메모리: 999,980 × 20 × 4 bytes = 약 80 MB (2배)
```

---

## 결론

### ✅ 시퀀스 길이를 늘리면:
1. **정확도가 높아질 수 있음** (데이터 충분 시)
2. **더 복잡한 패턴 학습 가능**
3. **장기 의존성 활용 가능**

### ⚠️ 주의사항:
1. **항상 높아지는 것은 아님** (과적합 위험)
2. **메모리/시간 비용 증가**
3. **데이터가 부족하면 오히려 성능 저하**

### 🎯 권장:
- **15~20 범위에서 테스트**
- **데이터가 충분하면 20 추천**
- **데이터가 부족하면 10 유지**

---

## 빠른 테스트 방법

```bash
# 1. 현재 성능 확인 (sequence_length=10)
python log_specific_model_comparison.py \
    --load-split split_data \
    --sample-size 100000 \
    --epochs 10

# 2. 코드 수정 후 15로 테스트
# log_specific_anomaly_detectors.py에서
# DeepLogDetector.__init__의 sequence_length=15로 변경

# 3. 동일한 명령으로 재실행
python log_specific_model_comparison.py \
    --load-split split_data \
    --sample-size 100000 \
    --epochs 10

# 4. 결과 비교
# - 정확도, F1 점수 비교
# - 더 높은 값 선택
```

---

## 참고

- 현재 기본값: `sequence_length=10`
- 권장 테스트 범위: 10, 15, 20
- 최적 범위: 15~20 (데이터에 따라 다름)
- 비권장: 30 이상 (과적합 위험)



