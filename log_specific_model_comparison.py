"""
ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ
ì—¬ëŸ¬ ë¡œê·¸ íŠ¹í™” ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•˜ì—¬ ìµœì  ëª¨ë¸ì„ ì„ ì •í•©ë‹ˆë‹¤.
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve,
    precision_recall_curve
)
from log_specific_anomaly_detectors import (
    LogSpecificAnomalySystem,
    DeepLogDetector,
    LogAnomalyDetector,
    LogRobustDetector
)
from severity_assessment import SeverityAssessment
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import matplotlib
    matplotlib.use('Agg')  # ë°±ì—”ë“œ ì„¤ì • (GUI ì—†ì´ ì‚¬ìš©)
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
    sns.set_style("whitegrid")
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['axes.unicode_minus'] = False
except ImportError:
    PLOTTING_AVAILABLE = False
    print("âš ï¸ matplotlib/seabornì´ ì—†ì–´ ê·¸ë˜í”„ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. pip install matplotlib seaborn")


class LogSpecificModelComparator:
    """ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.trained_systems = {}
        self.results = {}
        self.severity_assessor = SeverityAssessment()
        
    def prepare_data_from_files(self, data_dir: str, load_only_train=False):
        """
        ë¶„í• ëœ ë°ì´í„° íŒŒì¼ì—ì„œ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            data_dir: ë¶„í• ëœ ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            load_only_train: Trueì´ë©´ trainë§Œ ë¡œë“œ (í•™ìŠµ ë‹¨ê³„ì—ì„œ ë©”ëª¨ë¦¬ ì ˆì•½)
            
        Returns:
            ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 60)
        print("ë¶„í• ëœ ë°ì´í„° íŒŒì¼ì—ì„œ ë¡œë“œ")
        if load_only_train:
            print("   ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ: Trainë§Œ ë¨¼ì € ë¡œë“œ")
        print("=" * 60)
        
        train_file = os.path.join(data_dir, 'train.parquet')
        valid_file = os.path.join(data_dir, 'valid.parquet')
        test_file = os.path.join(data_dir, 'test.parquet')
        
        # Train ë¡œë“œ (í•™ìŠµì— í•„ìš”)
        if os.path.exists(train_file):
            train_all = pd.read_parquet(train_file, engine='pyarrow')
            print(f"   âœ… Train ë¡œë“œ: {len(train_all):,}ê°œ")
        else:
            raise FileNotFoundError(f"Train íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_file}")
        
        # ì •ìƒ/ì—ëŸ¬ ë¡œê·¸ ë¶„ë¦¬
        # ì •ìƒ ë¡œê·¸: INFO, DEBUG, TRACEë§Œ í¬í•¨ (WARN ì œì™¸)
        # ì—ëŸ¬ ë¡œê·¸: ERROR, FATAL, WARN í¬í•¨
        
        train_normal = train_all[
            (train_all['is_error'] == False) & 
            (train_all['level'].isin(['INFO', 'DEBUG', 'TRACE']))
        ].copy()
        train_error = train_all[
            (train_all['is_error'] == True) | 
            (train_all['level'].isin(['WARN', 'ERROR', 'FATAL']))
        ].copy()
        
        # train_all ë©”ëª¨ë¦¬ í•´ì œ
        del train_all
        
        # Validì™€ TestëŠ” í•„ìš”í•  ë•Œë§Œ ë¡œë“œ
        valid_normal = pd.DataFrame()
        valid_error = pd.DataFrame()
        valid_logs = pd.DataFrame()
        y_valid = np.array([])
        
        test_normal = pd.DataFrame()
        test_error = pd.DataFrame()
        test_logs = pd.DataFrame()
        y_test = np.array([])
        
        if not load_only_train:
            # Valid ë¡œë“œ
            if os.path.exists(valid_file):
                valid_all = pd.read_parquet(valid_file, engine='pyarrow')
                print(f"   âœ… Valid ë¡œë“œ: {len(valid_all):,}ê°œ")
                
                valid_normal = valid_all[
                    (valid_all['is_error'] == False) & 
                    (valid_all['level'].isin(['INFO', 'DEBUG', 'TRACE']))
                ].copy()
                valid_error = valid_all[
                    (valid_all['is_error'] == True) | 
                    (valid_all['level'].isin(['WARN', 'ERROR', 'FATAL']))
                ].copy()
                
                # Valid ë°ì´í„° ê²°í•©
                valid_logs = pd.concat([valid_normal, valid_error], ignore_index=True)
                valid_logs = valid_logs.sort_values('timestamp').reset_index(drop=True)
                y_valid = (valid_logs['is_error'] == True).astype(int).values
                
                # valid_all ë©”ëª¨ë¦¬ í•´ì œ
                del valid_all
            else:
                print(f"   âš ï¸ Valid íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {valid_file}")
            
            # Test ë¡œë“œ
            if os.path.exists(test_file):
                test_all = pd.read_parquet(test_file, engine='pyarrow')
                print(f"   âœ… Test ë¡œë“œ: {len(test_all):,}ê°œ")
                
                test_normal = test_all[
                    (test_all['is_error'] == False) & 
                    (test_all['level'].isin(['INFO', 'DEBUG', 'TRACE']))
                ].copy()
                test_error = test_all[
                    (test_all['is_error'] == True) | 
                    (test_all['level'].isin(['WARN', 'ERROR', 'FATAL']))
                ].copy()
                
                # Test ë°ì´í„° ê²°í•©
                test_logs = pd.concat([test_normal, test_error], ignore_index=True)
                test_logs = test_logs.sort_values('timestamp').reset_index(drop=True)
                y_test = (test_logs['is_error'] == True).astype(int).values
                
                # test_all ë©”ëª¨ë¦¬ í•´ì œ
                del test_all
            else:
                raise FileNotFoundError(f"Test íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}")
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
        print(f"   Train (í•™ìŠµìš©):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(train_normal):,}ê°œ")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(train_error):,}ê°œ")
        if not load_only_train:
            if not valid_logs.empty:
                print(f"\n   Valid (ê²€ì¦ìš©):")
                print(f"      - ì •ìƒ ë¡œê·¸: {len(valid_normal):,}ê°œ")
                print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(valid_error):,}ê°œ")
                print(f"      - ì „ì²´: {len(valid_logs):,}ê°œ")
            print(f"\n   Test (í…ŒìŠ¤íŠ¸ìš©):")
            print(f"      - ì •ìƒ ë¡œê·¸: {len(test_normal):,}ê°œ")
            print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(test_error):,}ê°œ")
            print(f"      - ì „ì²´: {len(test_logs):,}ê°œ")
        
        return {
            'train_normal': train_normal,
            'train_error': train_error,
            'valid_normal': valid_normal,
            'valid_error': valid_error,
            'valid_logs': valid_logs,
            'test_normal': test_normal,
            'test_error': test_error,
            'test_logs': test_logs,
            'y_valid': y_valid,
            'y_test': y_test
        }
    
    def load_test_data(self, data_dir: str):
        """
        í‰ê°€ ë‹¨ê³„ì—ì„œ Test ë°ì´í„°ë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            data_dir: ë¶„í• ëœ ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            
        Returns:
            test_logs, y_test
        """
        test_file = os.path.join(data_dir, 'test.parquet')
        valid_file = os.path.join(data_dir, 'valid.parquet')
        
        print("\nğŸ“‚ í‰ê°€ìš© ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # Test ë¡œë“œ
        if os.path.exists(test_file):
            test_all = pd.read_parquet(test_file, engine='pyarrow')
            print(f"   âœ… Test ë¡œë“œ: {len(test_all):,}ê°œ")
            
            test_normal = test_all[
                (test_all['is_error'] == False) & 
                (test_all['level'].isin(['INFO', 'DEBUG', 'TRACE']))
            ].copy()
            test_error = test_all[
                (test_all['is_error'] == True) | 
                (test_all['level'].isin(['WARN', 'ERROR', 'FATAL']))
            ].copy()
            
            test_logs = pd.concat([test_normal, test_error], ignore_index=True)
            test_logs = test_logs.sort_values('timestamp').reset_index(drop=True)
            y_test = (test_logs['is_error'] == True).astype(int).values
            
            del test_all
        else:
            raise FileNotFoundError(f"Test íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file}")
        
        # Valid ë¡œë“œ (ì„ íƒì )
        valid_logs = pd.DataFrame()
        y_valid = np.array([])
        
        if os.path.exists(valid_file):
            valid_all = pd.read_parquet(valid_file, engine='pyarrow')
            print(f"   âœ… Valid ë¡œë“œ: {len(valid_all):,}ê°œ")
            
            valid_normal = valid_all[
                (valid_all['is_error'] == False) & 
                (valid_all['level'].isin(['INFO', 'DEBUG', 'TRACE']))
            ].copy()
            valid_error = valid_all[
                (valid_all['is_error'] == True) | 
                (valid_all['level'].isin(['WARN', 'ERROR', 'FATAL']))
            ].copy()
            
            valid_logs = pd.concat([valid_normal, valid_error], ignore_index=True)
            valid_logs = valid_logs.sort_values('timestamp').reset_index(drop=True)
            y_valid = (valid_logs['is_error'] == True).astype(int).values
            
            del valid_all
        
        return {
            'test_logs': test_logs,
            'y_test': y_test,
            'valid_logs': valid_logs,
            'y_valid': y_valid
        }
    
    def prepare_data(self, logs_df: pd.DataFrame, train_ratio=0.8, valid_ratio=0.2):
        """
        ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• 
        - ì „ì²´ì˜ 80% â†’ train
        - trainì˜ 20% â†’ valid (ì „ì²´ì˜ 16%)
        - ë‚˜ë¨¸ì§€ 20% â†’ test (ì „ì²´ì˜ 20%)
        
        Args:
            logs_df: ì „ì²´ ë¡œê·¸ DataFrame
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.8 = 80%)
            valid_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (trainì˜ ë¹„ìœ¨, ê¸°ë³¸ 0.2 = 20%)
        
        Returns:
            ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        print("=" * 60)
        print("ë°ì´í„° ì¤€ë¹„ ë° ë¶„í• ")
        print("=" * 60)
        
        # ì‹œê°„ ìˆœì„œ ì •ë ¬
        logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
        
        # ì •ìƒ/ì—ëŸ¬ ë¡œê·¸ ë¶„ë¦¬
        # ì •ìƒ ë¡œê·¸: INFO, DEBUGë§Œ í¬í•¨ (WARN ì œì™¸)
        # ì—ëŸ¬ ë¡œê·¸: ERROR, FATAL, WARN í¬í•¨
        normal_logs = logs_df[
            (logs_df['is_error'] == False) & 
            (logs_df['level'].isin(['INFO', 'DEBUG', 'TRACE']))
        ].copy()
        error_logs = logs_df[
            (logs_df['is_error'] == True) | 
            (logs_df['level'].isin(['WARN', 'ERROR', 'FATAL']))
        ].copy()
        
        print(f"   - ì „ì²´ ë¡œê·¸: {len(logs_df)}ê°œ")
        print(f"   - ì •ìƒ ë¡œê·¸ (INFO/DEBUG/TRACEë§Œ): {len(normal_logs)}ê°œ")
        print(f"   - ì—ëŸ¬ ë¡œê·¸ (ERROR/FATAL/WARN í¬í•¨): {len(error_logs)}ê°œ")
        
        # ë ˆë²¨ë³„ í†µê³„
        level_counts = logs_df['level'].value_counts()
        print(f"\n   ë ˆë²¨ë³„ ë¶„í¬:")
        for level, count in level_counts.items():
            print(f"      {level}: {count}ê°œ ({count/len(logs_df)*100:.1f}%)")
        
        if len(normal_logs) == 0:
            raise ValueError("ì •ìƒ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if len(error_logs) == 0:
            raise ValueError("ì—ëŸ¬ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „ì²´ ë°ì´í„° ë¶„í• : 80% train, 20% test
        total_split_idx = int(len(logs_df) * train_ratio)
        train_all = logs_df.iloc[:total_split_idx]
        test_all = logs_df.iloc[total_split_idx:]
        
        # Train ë°ì´í„°ì—ì„œ ì •ìƒ/ì—ëŸ¬ ë¶„ë¦¬
        train_normal = train_all[train_all['is_error'] == False].copy()
        train_error = train_all[train_all['is_error'] == True].copy()
        
        # Trainì˜ 20%ë¥¼ Validë¡œ ë¶„í•  (ì •ìƒ ë¡œê·¸ ê¸°ì¤€)
        train_normal_split_idx = int(len(train_normal) * (1 - valid_ratio))
        train_normal_final = train_normal.iloc[:train_normal_split_idx]
        valid_normal = train_normal.iloc[train_normal_split_idx:]
        
        # Valid ë°ì´í„° (ì •ìƒ + ì—ëŸ¬)
        # ì—ëŸ¬ ë¡œê·¸ë„ trainì˜ 20%ë¥¼ validë¡œ
        if len(train_error) > 0:
            train_error_split_idx = int(len(train_error) * (1 - valid_ratio))
            train_error_final = train_error.iloc[:train_error_split_idx]
            valid_error = train_error.iloc[train_error_split_idx:]
        else:
            train_error_final = train_error
            valid_error = pd.DataFrame()
        
        # Test ë°ì´í„° (ì •ìƒ + ì—ëŸ¬)
        test_normal = test_all[test_all['is_error'] == False].copy()
        test_error = test_all[test_all['is_error'] == True].copy()
        
        # Valid ë°ì´í„° ê²°í•©
        valid_logs = pd.concat([valid_normal, valid_error], ignore_index=True)
        valid_logs = valid_logs.sort_values('timestamp').reset_index(drop=True)
        
        # Test ë°ì´í„° ê²°í•©
        test_logs = pd.concat([test_normal, test_error], ignore_index=True)
        test_logs = test_logs.sort_values('timestamp').reset_index(drop=True)
        
        # ë¼ë²¨ ìƒì„±
        y_valid = (valid_logs['is_error'] == True).astype(int).values if not valid_logs.empty else np.array([])
        y_test = (test_logs['is_error'] == True).astype(int).values
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í•  ê²°ê³¼:")
        print(f"   Train (í•™ìŠµìš©):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(train_normal_final)}ê°œ ({len(train_normal_final)/len(logs_df)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(train_error_final)}ê°œ")
        print(f"      - ì „ì²´: {len(train_normal_final) + len(train_error_final)}ê°œ ({len(train_all)/len(logs_df)*100:.1f}%)")
        print(f"\n   Valid (ê²€ì¦ìš©, trainì˜ {valid_ratio*100:.0f}%):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(valid_normal)}ê°œ ({len(valid_normal)/len(logs_df)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(valid_error)}ê°œ")
        print(f"      - ì „ì²´: {len(valid_logs)}ê°œ ({len(valid_logs)/len(logs_df)*100:.1f}%)")
        print(f"\n   Test (í…ŒìŠ¤íŠ¸ìš©):")
        print(f"      - ì •ìƒ ë¡œê·¸: {len(test_normal)}ê°œ ({len(test_normal)/len(logs_df)*100:.1f}%)")
        print(f"      - ì—ëŸ¬ ë¡œê·¸: {len(test_error)}ê°œ ({len(test_error)/len(logs_df)*100:.1f}%)")
        print(f"      - ì „ì²´: {len(test_logs)}ê°œ ({len(test_logs)/len(logs_df)*100:.1f}%)")
        
        return {
            'train_normal': train_normal_final,
            'train_error': train_error_final,
            'valid_normal': valid_normal,
            'valid_error': valid_error,
            'valid_logs': valid_logs,
            'test_normal': test_normal,
            'test_error': test_error,
            'test_logs': test_logs,
            'y_valid': y_valid,
            'y_test': y_test
        }
    
    def train_models(self, train_normal_logs: pd.DataFrame, valid_normal_logs: pd.DataFrame = None, 
                     model_types=None, log_dir=None, epochs=5, batch_size=128):
        """
        ì—¬ëŸ¬ ë¡œê·¸ íŠ¹í™” ëª¨ë¸ í•™ìŠµ
        
        Args:
            train_normal_logs: í•™ìŠµìš© ì •ìƒ ë¡œê·¸
            valid_normal_logs: ê²€ì¦ìš© ì •ìƒ ë¡œê·¸ (ì„ íƒì , ì¡°ê¸° ì¢…ë£Œ ë“±ì— ì‚¬ìš©)
            model_types: í•™ìŠµí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë‘)
            log_dir: í•™ìŠµ ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬
            epochs: í•™ìŠµ epoch ìˆ˜ (ê¸°ë³¸: 5, ë¹ ë¥¸ í•™ìŠµìš©)
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 128, ë¹ ë¥¸ í•™ìŠµìš©)
        """
        if model_types is None:
            model_types = ['deeplog', 'loganomaly']
            # LogRobustëŠ” ì œì™¸ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ê³  OOM ë°œìƒ ê°€ëŠ¥)
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        if log_dir is None:
            log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs', 'training')
        
        print("\n" + "=" * 60)
        print("ë¡œê·¸ íŠ¹í™” ëª¨ë¸ í•™ìŠµ")
        print("=" * 60)
        print(f"í•™ìŠµ ë¡œê·¸ ì €ì¥ ìœ„ì¹˜: {log_dir}")
        print(f"í•™ìŠµí•  ëª¨ë¸: {', '.join(model_types)}")
        print(f"í•™ìŠµ ë°ì´í„°: {len(train_normal_logs):,}ê°œ ë¡œê·¸")
        print(f"âš¡ ë¹ ë¥¸ í•™ìŠµ ì„¤ì •: Epochs={epochs}, Batch Size={batch_size}")
        
        for idx, model_type in enumerate(model_types, 1):
            print(f"\n[{idx}/{len(model_types)}] {model_type.upper()} í•™ìŠµ ì‹œì‘...")
            print("=" * 60)
            
            try:
                system = LogSpecificAnomalySystem(model_type=model_type)
                system.load_logs(train_normal_logs)
                
                # epochsì™€ batch_size ì „ë‹¬ (LogAnomalyëŠ” ë¬´ì‹œë¨)
                if system.train(train_ratio=1.0, log_dir=log_dir, epochs=epochs, batch_size=batch_size):  # ì „ì²´ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ì‚¬ìš©
                    self.trained_systems[model_type] = system
                    print(f"\nâœ… {model_type.upper()} í•™ìŠµ ì™„ë£Œ")
                    
                    # í•™ìŠµ ì™„ë£Œ í›„ ë¡œê·¸ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ (ëª¨ë¸ì€ ì´ë¯¸ í•™ìŠµë¨)
                    system.logs_df = None
                    import gc
                    gc.collect()
                    
                    # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í™•ì¸ (ì„ íƒì )
                    if valid_normal_logs is not None and not valid_normal_logs.empty:
                        print(f"   ğŸ“Š ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í™•ì¸ ì¤‘...")
                        # ê²€ì¦ì€ ì„ íƒì ìœ¼ë¡œ ìˆ˜í–‰
                else:
                    print(f"\nâŒ {model_type.upper()} í•™ìŠµ ì‹¤íŒ¨")
            
            except Exception as e:
                print(f"\nâŒ {model_type.upper()} í•™ìŠµ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print("\n" + "=" * 60)
        print(f"í•™ìŠµ ì™„ë£Œ: {len(self.trained_systems)}/{len(model_types)}ê°œ ëª¨ë¸ ì„±ê³µ")
        print("=" * 60)
    
    def find_optimal_threshold(self, y_test: np.ndarray, anomaly_scores: np.ndarray, 
                               metric='f1', min_precision=0.3, min_recall=0.5):
        """
        ìµœì  ì„ê³„ê°’ ì°¾ê¸°
        
        Args:
            y_test: ì‹¤ì œ ë¼ë²¨
            anomaly_scores: ì´ìƒ ì ìˆ˜
            metric: ìµœì í™”í•  ì§€í‘œ ('f1', 'precision', 'recall', 'balanced')
            min_precision: ìµœì†Œ ì •ë°€ë„ ìš”êµ¬ì‚¬í•­
            min_recall: ìµœì†Œ ì¬í˜„ìœ¨ ìš”êµ¬ì‚¬í•­
            
        Returns:
            ìµœì  ì„ê³„ê°’, ìµœì  ì„±ëŠ¥ ì§€í‘œ
        """
        if len(anomaly_scores) == 0 or len(np.unique(anomaly_scores)) < 2:
            return 0.5, {}
        
        # ì„ê³„ê°’ í›„ë³´ ìƒì„±
        thresholds = np.linspace(anomaly_scores.min(), anomaly_scores.max(), 100)
        best_threshold = 0.5
        best_score = 0
        best_metrics = {}
        
        for threshold in thresholds:
            y_pred = (anomaly_scores >= threshold).astype(int)
            
            if len(np.unique(y_pred)) < 2:  # ëª¨ë‘ 0ì´ê±°ë‚˜ ëª¨ë‘ 1ì¸ ê²½ìš°
                continue
            
            try:
                precision = precision_score(y_test, y_pred, zero_division=0)
                recall = recall_score(y_test, y_pred, zero_division=0)
                f1 = f1_score(y_test, y_pred, zero_division=0)
                accuracy = accuracy_score(y_test, y_pred)
                
                # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
                if precision < min_precision or recall < min_recall:
                    continue
                
                # ë©”íŠ¸ë¦­ì— ë”°ë¼ ì ìˆ˜ ê³„ì‚°
                if metric == 'f1':
                    score = f1
                elif metric == 'precision':
                    score = precision
                elif metric == 'recall':
                    score = recall
                elif metric == 'balanced':
                    # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê· í˜•
                    score = (precision + recall) / 2
                else:
                    score = f1
                
                if score > best_score:
                    best_score = score
                    best_threshold = threshold
                    best_metrics = {
                        'threshold': threshold,
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1_score': f1
                    }
            except:
                continue
        
        return best_threshold, best_metrics
    
    def evaluate_models(self, test_logs: pd.DataFrame, y_test: np.ndarray, 
                       optimize_threshold=True, target_metrics=None):
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            test_logs: í…ŒìŠ¤íŠ¸ ë¡œê·¸ DataFrame
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨ (0=ì •ìƒ, 1=ì´ìƒ)
            optimize_threshold: Trueì´ë©´ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            target_metrics: ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬ (ì˜ˆ: {'precision': 0.5, 'recall': 0.6})
        """
        import time
        
        if target_metrics is None:
            target_metrics = {
                'accuracy': 0.70,
                'precision': 0.50,
                'recall': 0.60,
                'f1_score': 0.55,
                'specificity': 0.80
            }
        
        print("\n" + "=" * 60)
        print("ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        print(f"í‰ê°€ ë°ì´í„°: {len(test_logs):,}ê°œ ë¡œê·¸")
        if optimize_threshold:
            print(f"ğŸ’¡ ìµœì  ì„ê³„ê°’ ìë™ ì¡°ì • ëª¨ë“œ í™œì„±í™”")
            print(f"   ëª©í‘œ ì„±ëŠ¥:")
            print(f"      - ì •í™•ë„: {target_metrics['accuracy']*100:.0f}% ì´ìƒ")
            print(f"      - ì •ë°€ë„: {target_metrics['precision']*100:.0f}% ì´ìƒ")
            print(f"      - ì¬í˜„ìœ¨: {target_metrics['recall']*100:.0f}% ì´ìƒ")
            print(f"      - F1 ì ìˆ˜: {target_metrics['f1_score']*100:.0f}% ì´ìƒ")
        
        results = {}
        
        for idx, (model_type, system) in enumerate(self.trained_systems.items(), 1):
            print(f"\n[{idx}/{len(self.trained_systems)}] {model_type.upper()} í‰ê°€ ì¤‘...")
            eval_start_time = time.time()
            
            try:
                # ì´ìƒ íƒì§€ (ì‹œê°„ ì¸¡ì •)
                print(f"   â³ ì´ìƒ íƒì§€ ìˆ˜í–‰ ì¤‘... (ì˜ˆìƒ ì‹œê°„: {len(test_logs) // 5000}ì´ˆ)")
                detection_results = system.detect_anomalies(test_logs)
                
                anomalies_df = detection_results.get('anomalies', pd.DataFrame()) if detection_results else pd.DataFrame()
                
                # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± (ì‹œí€€ìŠ¤ ê¸°ë°˜)
                # ê° ë¡œê·¸ê°€ ì´ìƒ ì‹œí€€ìŠ¤ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
                y_pred = np.zeros(len(test_logs))
                anomaly_scores = np.zeros(len(test_logs))
                
                if not anomalies_df.empty:
                    # ì´ìƒì´ íƒì§€ëœ ê²½ìš°
                    for idx, row in anomalies_df.iterrows():
                        seq_idx = row.get('sequence_index', 0)
                        score = row.get('anomaly_score', 0)
                        
                        # ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì´ìƒìœ¼ë¡œ í‘œì‹œ
                        seq_length = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´ 15ë¡œ ë³€ê²½
                        start_idx = max(0, seq_idx)
                        end_idx = min(len(test_logs), seq_idx + seq_length)
                        
                        y_pred[start_idx:end_idx] = 1
                        anomaly_scores[start_idx:end_idx] = np.maximum(
                            anomaly_scores[start_idx:end_idx],
                            score
                        )
                else:
                    # ì´ìƒì´ íƒì§€ë˜ì§€ ì•Šì€ ê²½ìš° (LogRobust ë“±)
                    # ì§ì ‘ ëª¨ë¸ì—ì„œ anomaly_scoresë¥¼ ê°€ì ¸ì™€ì„œ í‰ê°€
                    print(f"   âš ï¸ ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ì ìˆ˜ë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
                    
                    # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì§ì ‘ ì ìˆ˜ ê³„ì‚°
                    if model_type == 'logrobust':
                        # LogRobustëŠ” ì§ì ‘ ëª¨ë¸ì„ í†µí•´ ì ìˆ˜ ê³„ì‚°
                        try:
                            from log_specific_anomaly_detectors import LogRobustDetector
                            if hasattr(system.detector, 'model') and system.detector.model is not None:
                                logs_df_sorted = test_logs.sort_values('timestamp').reset_index(drop=True)
                                sequences = []
                                seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´ 15
                                for i in range(len(logs_df_sorted) - seq_len + 1):
                                    sequence_logs = logs_df_sorted.iloc[i:i + seq_len]
                                    sequence_vectors = [
                                        system.detector.encode_log(msg) for msg in sequence_logs['message']
                                    ]
                                    sequences.append(sequence_vectors)
                                
                                if len(sequences) > 0:
                                    sequences = np.array(sequences)
                                    sequences_tensor = torch.FloatTensor(sequences)
                                    
                                    system.detector.model.eval()
                                    with torch.no_grad():
                                        outputs = system.detector.model(sequences_tensor)
                                        scores = outputs.squeeze().numpy()
                                        
                                        # ì ìˆ˜ë¥¼ anomaly_scoresì— ì €ì¥
                                        seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´ 15
                                        for i, score in enumerate(scores):
                                            start_idx = max(0, i)
                                            end_idx = min(len(test_logs), i + seq_len)
                                            anomaly_scores[start_idx:end_idx] = np.maximum(
                                                anomaly_scores[start_idx:end_idx],
                                                float(score)
                                            )
                        except Exception as e:
                            print(f"   âš ï¸ LogRobust ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                            # ê¸°ë³¸ê°’ ì‚¬ìš© (ëª¨ë‘ 0)
                    elif model_type == 'deeplog':
                        # DeepLogëŠ” ì§ì ‘ ëª¨ë¸ì„ í†µí•´ ì ìˆ˜ ê³„ì‚°
                        try:
                            sequences, actual_next = system.detector.prepare_sequences(test_logs)
                            if len(sequences) > 0:
                                sequences_tensor = torch.LongTensor(sequences)
                                system.detector.model.eval()
                                with torch.no_grad():
                                    outputs = system.detector.model(sequences_tensor)
                                    probs = torch.softmax(outputs, dim=1)
                                    predicted_probs = probs[np.arange(len(actual_next)), actual_next].numpy()
                                    anomaly_scores_seq = 1 - predicted_probs
                                    
                                    seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´ 15
                                    for i, score in enumerate(anomaly_scores_seq):
                                        start_idx = max(0, i)
                                        end_idx = min(len(test_logs), i + seq_len)
                                        anomaly_scores[start_idx:end_idx] = np.maximum(
                                            anomaly_scores[start_idx:end_idx],
                                            score
                                        )
                        except Exception as e:
                            print(f"   âš ï¸ DeepLog ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    elif model_type == 'loganomaly':
                        # LogAnomalyëŠ” ì§ì ‘ ì ìˆ˜ ê³„ì‚°
                        try:
                            test_sequences = system.detector.create_sequences(test_logs, window_size=15)  # 10 â†’ 15
                            if len(test_sequences) > 0:
                                for i, sequence in enumerate(test_sequences):
                                    seq_mean = np.mean(sequence, axis=0)
                                    z_scores = np.abs((seq_mean - system.detector.normal_mean) / system.detector.normal_std)
                                    max_z_score = np.max(z_scores)
                                    anomaly_score = max_z_score / 3.0  # threshold 3.0 ê¸°ì¤€
                                    
                                    seq_len = 15  # ì‹œí€€ìŠ¤ ê¸¸ì´ 15
                                    start_idx = max(0, i)
                                    end_idx = min(len(test_logs), i + seq_len)
                                    anomaly_scores[start_idx:end_idx] = np.maximum(
                                        anomaly_scores[start_idx:end_idx],
                                        anomaly_score
                                    )
                        except Exception as e:
                            print(f"   âš ï¸ LogAnomaly ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                
                # ìµœì  ì„ê³„ê°’ ì°¾ê¸° (ì˜µì…˜)
                optimal_threshold = None
                optimal_metrics = None
                if optimize_threshold and len(anomaly_scores) > 0 and len(np.unique(anomaly_scores)) > 1:
                    print(f"   ğŸ” ìµœì  ì„ê³„ê°’ íƒìƒ‰ ì¤‘...")
                    optimal_threshold, optimal_metrics = self.find_optimal_threshold(
                        y_test, anomaly_scores,
                        metric='balanced',  # ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ê· í˜•
                        min_precision=target_metrics.get('precision', 0.3),
                        min_recall=target_metrics.get('recall', 0.5)
                    )
                    
                    if optimal_metrics:
                        print(f"   âœ… ìµœì  ì„ê³„ê°’ ë°œê²¬: {optimal_threshold:.4f}")
                        print(f"      ì˜ˆìƒ ì„±ëŠ¥:")
                        print(f"         - ì •í™•ë„: {optimal_metrics['accuracy']:.4f} ({optimal_metrics['accuracy']*100:.2f}%)")
                        print(f"         - ì •ë°€ë„: {optimal_metrics['precision']:.4f} ({optimal_metrics['precision']*100:.2f}%)")
                        print(f"         - ì¬í˜„ìœ¨: {optimal_metrics['recall']:.4f} ({optimal_metrics['recall']*100:.2f}%)")
                        print(f"         - F1 ì ìˆ˜: {optimal_metrics['f1_score']:.4f} ({optimal_metrics['f1_score']*100:.2f}%)")
                        
                        # ìµœì  ì„ê³„ê°’ìœ¼ë¡œ ì¬ê³„ì‚°
                        y_pred_optimal = (anomaly_scores >= optimal_threshold).astype(int)
                        accuracy = accuracy_score(y_test, y_pred_optimal)
                        precision = precision_score(y_test, y_pred_optimal, zero_division=0)
                        recall = recall_score(y_test, y_pred_optimal, zero_division=0)
                        f1 = f1_score(y_test, y_pred_optimal, zero_division=0)
                        y_pred = y_pred_optimal  # ìµœì í™”ëœ ì˜ˆì¸¡ ì‚¬ìš©
                    else:
                        print(f"   âš ï¸ ëª©í‘œ ì„±ëŠ¥ì„ ë§Œì¡±í•˜ëŠ” ì„ê³„ê°’ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„ê³„ê°’ ì‚¬ìš©.")
                        accuracy = accuracy_score(y_test, y_pred)
                        precision = precision_score(y_test, y_pred, zero_division=0)
                        recall = recall_score(y_test, y_pred, zero_division=0)
                        f1 = f1_score(y_test, y_pred, zero_division=0)
                else:
                    # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                    accuracy = accuracy_score(y_test, y_pred)
                    precision = precision_score(y_test, y_pred, zero_division=0)
                    recall = recall_score(y_test, y_pred, zero_division=0)
                    f1 = f1_score(y_test, y_pred, zero_division=0)
                
                # ROC-AUC
                try:
                    roc_auc = roc_auc_score(y_test, anomaly_scores)
                except:
                    roc_auc = None
                
                # í˜¼ë™ í–‰ë ¬
                cm = confusion_matrix(y_test, y_pred)
                tn, fp, fn, tp = cm.ravel()
                
                # íŠ¹ì´ë„
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                
                # ì‹¬ê°ë„ ì •ë³´
                severity_info = detection_results.get('summary', {})
                
                # ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
                meets_target = (
                    accuracy >= target_metrics.get('accuracy', 0) and
                    precision >= target_metrics.get('precision', 0) and
                    recall >= target_metrics.get('recall', 0) and
                    f1 >= target_metrics.get('f1_score', 0) and
                    specificity >= target_metrics.get('specificity', 0)
                )
                
                results[model_type] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'roc_auc': roc_auc,
                    'specificity': specificity,
                    'tp': tp,
                    'tn': tn,
                    'fp': fp,
                    'fn': fn,
                    'confusion_matrix': cm,
                    'y_pred': y_pred,
                    'y_test': y_test,  # ROC ê³¡ì„ ì„ ìœ„í•´ ì €ì¥
                    'anomaly_scores': anomaly_scores,
                    'severity_info': severity_info,
                    'anomalies_df': anomalies_df,
                    'optimal_threshold': optimal_threshold,
                    'optimal_metrics': optimal_metrics,
                    'meets_target': meets_target
                }
                
                print(f"   ì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%) {'âœ…' if accuracy >= target_metrics.get('accuracy', 0) else 'âŒ'}")
                print(f"   ì •ë°€ë„: {precision:.4f} ({precision*100:.2f}%) {'âœ…' if precision >= target_metrics.get('precision', 0) else 'âŒ'}")
                print(f"   ì¬í˜„ìœ¨: {recall:.4f} ({recall*100:.2f}%) {'âœ…' if recall >= target_metrics.get('recall', 0) else 'âŒ'}")
                print(f"   F1 ì ìˆ˜: {f1:.4f} ({f1*100:.2f}%) {'âœ…' if f1 >= target_metrics.get('f1_score', 0) else 'âŒ'}")
                if roc_auc:
                    print(f"   ROC-AUC: {roc_auc:.4f}")
                print(f"   íŠ¹ì´ë„: {specificity:.4f} ({specificity*100:.2f}%) {'âœ…' if specificity >= target_metrics.get('specificity', 0) else 'âŒ'}")
                print(f"   í˜¼ë™ í–‰ë ¬:")
                print(f"      [ì •ìƒâ†’ì •ìƒ: {tn:4d}  ì •ìƒâ†’ì´ìƒ: {fp:4d}]")
                print(f"      [ì´ìƒâ†’ì •ìƒ: {fn:4d}  ì´ìƒâ†’ì´ìƒ: {tp:4d}]")
                
                if meets_target:
                    print(f"\n   ğŸ‰ ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±!")
                else:
                    print(f"\n   âš ï¸ ëª©í‘œ ì„±ëŠ¥ ë¯¸ë‹¬ì„± - ê°œì„  í•„ìš”")
                    if optimal_threshold and optimal_metrics:
                        print(f"   ğŸ’¡ ìµœì  ì„ê³„ê°’({optimal_threshold:.4f}) ì ìš© ì‹œ ì˜ˆìƒ ê°œì„ :")
                        print(f"      - ì •í™•ë„: {accuracy:.4f} â†’ {optimal_metrics['accuracy']:.4f}")
                        print(f"      - ì •ë°€ë„: {precision:.4f} â†’ {optimal_metrics['precision']:.4f}")
                        print(f"      - ì¬í˜„ìœ¨: {recall:.4f} â†’ {optimal_metrics['recall']:.4f}")
                        print(f"      - F1 ì ìˆ˜: {f1:.4f} â†’ {optimal_metrics['f1_score']:.4f}")
                
                # ì‹¬ê°ë„ ì •ë³´
                if severity_info:
                    print(f"\n   ğŸ” ì‹¬ê°ë„ ë¶„ì„:")
                    print(f"      íƒì§€ëœ ì´ìƒ ì‹œí€€ìŠ¤: {severity_info.get('total_anomalies', 0)}ê°œ")
                    if 'by_severity' in severity_info:
                        print(f"      ì‹¬ê°ë„ ë¶„í¬:")
                        for level, count in severity_info['by_severity'].items():
                            print(f"        {level}: {count}ê°œ")
                    if 'avg_severity_score' in severity_info:
                        print(f"      í‰ê·  ì‹¬ê°ë„: {severity_info['avg_severity_score']:.2f}")
                
                eval_time = time.time() - eval_start_time
                print(f"\n   âœ… {model_type.upper()} í‰ê°€ ì™„ë£Œ: {eval_time:.2f}ì´ˆ ({eval_time/60:.1f}ë¶„)")
            
            except Exception as e:
                print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        self.results = results
        return results
    
    def generate_comparison_report(self, output_dir=None):
        """ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.results:
            print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print("\n" + "=" * 60)
        print("ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # ì„±ëŠ¥ ì§€í‘œ ë¹„êµ í…Œì´ë¸”
        comparison_data = []
        for model_name, metrics in self.results.items():
            comparison_data.append({
                'ëª¨ë¸': model_name.upper(),
                'ì •í™•ë„': f"{metrics['accuracy']:.4f}",
                'ì •ë°€ë„': f"{metrics['precision']:.4f}",
                'ì¬í˜„ìœ¨': f"{metrics['recall']:.4f}",
                'F1 ì ìˆ˜': f"{metrics['f1_score']:.4f}",
                'ROC-AUC': f"{metrics['roc_auc']:.4f}" if metrics['roc_auc'] else "N/A",
                'íŠ¹ì´ë„': f"{metrics['specificity']:.4f}",
                'TP': metrics['tp'],
                'TN': metrics['tn'],
                'FP': metrics['fp'],
                'FN': metrics['fn']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        print("\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë¹„êµ:")
        print(comparison_df.to_string(index=False))
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        print("\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
        best_f1 = max(self.results.items(), key=lambda x: x[1]['f1_score'])
        best_accuracy = max(self.results.items(), key=lambda x: x[1]['accuracy'])
        best_recall = max(self.results.items(), key=lambda x: x[1]['recall'])
        best_precision = max(self.results.items(), key=lambda x: x[1]['precision'])
        
        print(f"   ìµœê³  F1 ì ìˆ˜: {best_f1[0].upper()} ({best_f1[1]['f1_score']:.4f})")
        print(f"   ìµœê³  ì •í™•ë„: {best_accuracy[0].upper()} ({best_accuracy[1]['accuracy']:.4f})")
        print(f"   ìµœê³  ì¬í˜„ìœ¨: {best_recall[0].upper()} ({best_recall[1]['recall']:.4f})")
        print(f"   ìµœê³  ì •ë°€ë„: {best_precision[0].upper()} ({best_precision[1]['precision']:.4f})")
        
        # ì¢…í•© í‰ê°€ (ê°€ì¤‘ í‰ê· )
        print("\nğŸ“ˆ ì¢…í•© í‰ê°€:")
        weighted_scores = {}
        for model_name, metrics in self.results.items():
            # F1 ì ìˆ˜ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ (ê°€ì¥ ì¤‘ìš”)
            weighted_score = (
                metrics['f1_score'] * 0.4 +
                metrics['accuracy'] * 0.3 +
                metrics['recall'] * 0.2 +
                metrics['precision'] * 0.1
            )
            weighted_scores[model_name] = weighted_score
        
        best_overall = max(weighted_scores.items(), key=lambda x: x[1])
        print(f"   ğŸ¥‡ ìµœì  ëª¨ë¸: {best_overall[0].upper()}")
        print(f"      ì¢…í•© ì ìˆ˜: {best_overall[1]:.4f}")
        print(f"      F1 ì ìˆ˜: {self.results[best_overall[0]]['f1_score']:.4f}")
        print(f"      ì •í™•ë„: {self.results[best_overall[0]]['accuracy']:.4f}")
        print(f"      ì¬í˜„ìœ¨: {self.results[best_overall[0]]['recall']:.4f}")
        
        # ê²°ê³¼ ì €ì¥
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
            # ë¹„êµ í…Œì´ë¸” ì €ì¥
            comparison_path = os.path.join(output_dir, "log_specific_model_comparison.csv")
            comparison_df.to_csv(comparison_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")
            
            # ê° ëª¨ë¸ì˜ ìƒì„¸ ê²°ê³¼ ì €ì¥
            for model_name, metrics in self.results.items():
                detail_path = os.path.join(output_dir, f"results_{model_name}.csv")
                detail_df = pd.DataFrame({
                    'y_true': [0] * len(metrics['y_pred']),  # ì‹¤ì œ ë¼ë²¨ì€ ë³„ë„ ì €ì¥ í•„ìš”
                    'y_pred': metrics['y_pred'],
                    'anomaly_score': metrics['anomaly_scores']
                })
                detail_df.to_csv(detail_path, index=False, encoding='utf-8-sig')
                
                # ì‹¬ê°ë„ ì •ë³´ ì €ì¥
                if not metrics.get('anomalies_df', pd.DataFrame()).empty:
                    severity_path = os.path.join(output_dir, f"severity_{model_name}.csv")
                    metrics['anomalies_df'].to_csv(severity_path, index=False, encoding='utf-8-sig')
                    print(f"ğŸ’¾ {model_name.upper()} ì‹¬ê°ë„ ê²°ê³¼ ì €ì¥: {severity_path}")
            
            # ê·¸ë˜í”„ ìƒì„±
            if PLOTTING_AVAILABLE:
                print("\nğŸ“Š ê·¸ë˜í”„ ìƒì„± ì¤‘...")
                self.plot_comparison_graphs(output_dir)
        
        return comparison_df, best_overall[0]
    
    def plot_comparison_graphs(self, output_dir):
        """ëª¨ë¸ ë¹„êµ ê·¸ë˜í”„ ìƒì„±"""
        if not PLOTTING_AVAILABLE:
            print("âš ï¸ matplotlib/seabornì´ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("   ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        
        # 1. ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
        
        models = list(self.results.keys())
        metrics_names = ['accuracy', 'precision', 'recall', 'f1_score']
        metric_labels = ['ì •í™•ë„ (Accuracy)', 'ì •ë°€ë„ (Precision)', 'ì¬í˜„ìœ¨ (Recall)', 'F1 ì ìˆ˜']
        
        for idx, (metric_name, metric_label) in enumerate(zip(metrics_names, metric_labels)):
            ax = axes[idx // 2, idx % 2]
            values = [self.results[model][metric_name] for model in models]
            colors = sns.color_palette("husl", len(models))
            
            bars = ax.bar(range(len(models)), values, color=colors, alpha=0.7, edgecolor='black')
            ax.set_xticks(range(len(models)))
            ax.set_xticklabels([m.upper() for m in models], rotation=0)
            ax.set_ylabel('ì ìˆ˜', fontsize=11)
            ax.set_title(metric_label, fontsize=12, fontweight='bold')
            ax.set_ylim(0, 1.1)
            ax.grid(axis='y', alpha=0.3)
            
            # ê°’ í‘œì‹œ
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:.3f}', ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        metrics_path = os.path.join(output_dir, "performance_comparison.png")
        plt.savefig(metrics_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {metrics_path}")
        
        # 2. í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ
        print("   ğŸ“Š í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        n_models = len(self.results)
        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))
        if n_models == 1:
            axes = [axes]
        
        fig.suptitle('í˜¼ë™ í–‰ë ¬ (Confusion Matrix)', fontsize=16, fontweight='bold')
        
        for idx, (model_name, metrics) in enumerate(self.results.items()):
            cm = metrics['confusion_matrix']
            ax = axes[idx]
            
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       xticklabels=['ì •ìƒ', 'ì´ìƒ'], yticklabels=['ì •ìƒ', 'ì´ìƒ'],
                       cbar_kws={'label': 'ê°œìˆ˜'})
            ax.set_title(f'{model_name.upper()}\n(TP:{metrics["tp"]}, TN:{metrics["tn"]}, FP:{metrics["fp"]}, FN:{metrics["fn"]})',
                        fontsize=11, fontweight='bold')
            ax.set_ylabel('ì‹¤ì œ', fontsize=11)
            ax.set_xlabel('ì˜ˆì¸¡', fontsize=11)
        
        plt.tight_layout()
        cm_path = os.path.join(output_dir, "confusion_matrices.png")
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ… í˜¼ë™ í–‰ë ¬ íˆíŠ¸ë§µ ì €ì¥: {cm_path}")
        
        # 3. ROC ê³¡ì„  (ê°€ëŠ¥í•œ ê²½ìš°)
        print("   ğŸ“ˆ ROC ê³¡ì„  ìƒì„± ì¤‘...")
        fig, ax = plt.subplots(figsize=(10, 8))
        
        for model_name, metrics in self.results.items():
            if metrics.get('roc_auc') is not None and metrics.get('y_test') is not None:
                try:
                    y_test = metrics.get('y_test')
                    y_scores = metrics.get('anomaly_scores')
                    
                    if y_scores is not None and len(y_scores) > 0:
                        fpr, tpr, _ = roc_curve(y_test, y_scores)
                        roc_auc = metrics['roc_auc']
                        ax.plot(fpr, tpr, lw=2, label=f'{model_name.upper()} (AUC = {roc_auc:.3f})')
                except Exception as e:
                    print(f"      âš ï¸ {model_name} ROC ê³¡ì„  ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
        
        ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random (AUC = 0.500)')
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
        ax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=12)
        ax.set_title('ROC ê³¡ì„  ë¹„êµ', fontsize=14, fontweight='bold')
        ax.legend(loc="lower right", fontsize=10)
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        roc_path = os.path.join(output_dir, "roc_curves.png")
        plt.savefig(roc_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ROC ê³¡ì„  ì €ì¥: {roc_path}")
        
        # 4. ì‹¬ê°ë„ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
        print("   ğŸ¥§ ì‹¬ê°ë„ ë¶„í¬ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        n_models = len(self.results)
        fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))
        if n_models == 1:
            axes = [axes]
        
        fig.suptitle('ì‹¬ê°ë„ ë¶„í¬', fontsize=16, fontweight='bold')
        
        for idx, (model_name, metrics) in enumerate(self.results.items()):
            ax = axes[idx]
            severity_info = metrics.get('severity_info', {})
            by_severity = severity_info.get('by_severity', {})
            
            if by_severity:
                labels = list(by_severity.keys())
                sizes = list(by_severity.values())
                colors_map = {'CRITICAL': '#d62728', 'HIGH': '#ff7f0e', 'MEDIUM': '#ffbb78', 'LOW': '#2ca02c'}
                colors = [colors_map.get(label, '#1f77b4') for label in labels]
                
                ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90,
                      colors=colors, textprops={'fontsize': 10})
                ax.set_title(f'{model_name.upper()}\n(ì´ {severity_info.get("total_anomalies", 0)}ê°œ)', 
                           fontsize=11, fontweight='bold')
            else:
                ax.text(0.5, 0.5, 'ë°ì´í„° ì—†ìŒ', ha='center', va='center', fontsize=12)
                ax.set_title(f'{model_name.upper()}', fontsize=11, fontweight='bold')
        
        plt.tight_layout()
        severity_path = os.path.join(output_dir, "severity_distribution.png")
        plt.savefig(severity_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ì‹¬ê°ë„ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥: {severity_path}")
        
        # 5. ì¢…í•© ë¹„êµ ë ˆì´ë” ì°¨íŠ¸
        print("   ğŸ“Š ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ ìƒì„± ì¤‘...")
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        categories = ['ì •í™•ë„', 'ì •ë°€ë„', 'ì¬í˜„ìœ¨', 'F1 ì ìˆ˜', 'íŠ¹ì´ë„']
        num_vars = len(categories)
        
        # ê°ë„ ê³„ì‚°
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]  # ì›í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°
        
        # ê° ëª¨ë¸ë³„ ë°ì´í„°
        for model_name, metrics in self.results.items():
            values = [
                metrics['accuracy'],
                metrics['precision'],
                metrics['recall'],
                metrics['f1_score'],
                metrics['specificity']
            ]
            values += values[:1]  # ì›í˜•ìœ¼ë¡œ ë§Œë“¤ê¸°
            
            ax.plot(angles, values, 'o-', linewidth=2, label=model_name.upper(), alpha=0.7)
            ax.fill(angles, values, alpha=0.15)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_title('ì¢…í•© ì„±ëŠ¥ ë¹„êµ (ë ˆì´ë” ì°¨íŠ¸)', fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
        
        plt.tight_layout()
        radar_path = os.path.join(output_dir, "performance_radar.png")
        plt.savefig(radar_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"   âœ… ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ ì €ì¥: {radar_path}")
        
        print("\nâœ… ëª¨ë“  ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!")
    
    def get_best_model(self):
        """ìµœì  ëª¨ë¸ ë°˜í™˜"""
        if not self.results:
            return None
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        weighted_scores = {}
        for model_name, metrics in self.results.items():
            weighted_score = (
                metrics['f1_score'] * 0.4 +
                metrics['accuracy'] * 0.3 +
                metrics['recall'] * 0.2 +
                metrics['precision'] * 0.1
            )
            weighted_scores[model_name] = weighted_score
        
        best_model_name = max(weighted_scores.items(), key=lambda x: x[1])[0]
        return best_model_name, self.trained_systems[best_model_name]


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    from log_anomaly_detector import SpringBootLogParser
    
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser_args = argparse.ArgumentParser(description='ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ')
    parser_args.add_argument('--parse-only', action='store_true', 
                            help='íŒŒì‹±ë§Œ ìˆ˜í–‰í•˜ê³  ì €ì¥ (ëª¨ë¸ í•™ìŠµ ìƒëµ)')
    parser_args.add_argument('--load-parsed', type=str, default=None,
                            help='ì €ì¥ëœ íŒŒì‹± ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ì¬ì‚¬ìš©)')
    parser_args.add_argument('--save-parsed', type=str, default=None,
                            help='íŒŒì‹± ë°ì´í„° ì €ì¥ ê²½ë¡œ')
    parser_args.add_argument('--keep-chunks', action='store_true',
                            help='ì²­í¬ íŒŒì¼ ìœ ì§€ (ì¬ì‚¬ìš© ê°€ëŠ¥, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )')
    parser_args.add_argument('--load-chunks', type=str, default=None,
                            help='ì²­í¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì²­í¬ íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )')
    parser_args.add_argument('--chunk-read-size', type=int, default=None,
                            help='Parquet íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ì„ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½, ì˜ˆ: 100000)')
    parser_args.add_argument('--streaming-split', action='store_true',
                            help='ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ë¶„í•  (ë©”ëª¨ë¦¬ íš¨ìœ¨ì , parsed_data.parquet ì‚¬ìš© ì‹œ ê¶Œì¥)')
    parser_args.add_argument('--split-output-dir', type=str, default=None,
                            help='ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: pattern/prelog/split_data)')
    parser_args.add_argument('--load-split', type=str, default=None,
                            help='ë¶„í• ëœ ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œ (ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ê²°ê³¼ ì¬ì‚¬ìš©)')
    parser_args.add_argument('--max-total-lines', type=int, default=100000,
                            help='íŒŒì‹±í•  ìµœëŒ€ ë¼ì¸ ìˆ˜ (ê¸°ë³¸: 100000, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)')
    parser_args.add_argument('--max-files', type=int, default=None,
                            help='ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)')
    parser_args.add_argument('--sample-size', type=int, default=300000,
                            help='í•™ìŠµìš© ë°ì´í„° ìƒ˜í”Œë§ í¬ê¸° (ê¸°ë³¸: 300000, OOM ë°©ì§€)')
    parser_args.add_argument('--epochs', type=int, default=5,
                            help='í•™ìŠµ epoch ìˆ˜ (ê¸°ë³¸: 5, ë¹ ë¥¸ í•™ìŠµìš©)')
    parser_args.add_argument('--batch-size', type=int, default=32,
                            help='ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 32, OOM ë°©ì§€)')
    parser_args.add_argument('--eval-sample-size', type=int, default=None,
                            help='í‰ê°€ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œë§ í¬ê¸° (ë¹ ë¥¸ í‰ê°€ìš©, ì˜ˆ: 20000)')
    parser_args.add_argument('--optimize-threshold', action='store_true',
                            help='ìµœì  ì„ê³„ê°’ ìë™ íƒìƒ‰ (ì„±ëŠ¥ ê°œì„ )')
    parser_args.add_argument('--target-accuracy', type=float, default=0.70,
                            help='ëª©í‘œ ì •í™•ë„ (ê¸°ë³¸: 0.70)')
    parser_args.add_argument('--target-precision', type=float, default=0.50,
                            help='ëª©í‘œ ì •ë°€ë„ (ê¸°ë³¸: 0.50)')
    parser_args.add_argument('--target-recall', type=float, default=0.60,
                            help='ëª©í‘œ ì¬í˜„ìœ¨ (ê¸°ë³¸: 0.60)')
    parser_args.add_argument('--target-f1', type=float, default=0.55,
                            help='ëª©í‘œ F1 ì ìˆ˜ (ê¸°ë³¸: 0.55)')
    parser_args.add_argument('--target-specificity', type=float, default=0.80,
                            help='ëª©í‘œ íŠ¹ì´ë„ (ê¸°ë³¸: 0.80)')
    args = parser_args.parse_args()
    
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    print("=" * 70)
    print("ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œìŠ¤í…œ")
    print("=" * 70)
    print(f"âš¡ ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ:")
    print(f"   - ìµœëŒ€ íŒŒì‹± ë¼ì¸: {args.max_total_lines:,}ê°œ")
    print(f"   - í•™ìŠµ ë°ì´í„° ìƒ˜í”Œë§: {args.sample_size:,}ê°œ")
    print(f"   - Epochs: {args.epochs}")
    print(f"   - Batch Size: {args.batch_size}")
    print("=" * 70)
    
    # 2. ëª¨ë¸ ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë¨¼ì € ì´ˆê¸°í™”)
    comparator = LogSpecificModelComparator()
    
    # 1. ë¡œê·¸ íŒŒì‹± ë˜ëŠ” ë¡œë“œ
    print("\n1ë‹¨ê³„: ë¡œê·¸ ë°ì´í„° ì¤€ë¹„")
    parser = SpringBootLogParser()
    
    # ìë™ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° ì¬ì‚¬ìš© (ì˜µì…˜ì´ ì—†ì„ ë•Œ)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    default_split_dir = os.path.join(script_dir, 'split_data')
    default_parsed_file = os.path.join(script_dir, 'parsed_data.parquet')
    
    # ë¶„í• ëœ ë°ì´í„°ì—ì„œ ì§ì ‘ ë¡œë“œ (ê°€ì¥ ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    if args.load_split:
        print(f"ğŸ“‚ ë¶„í• ëœ ë°ì´í„°ì—ì„œ ë¡œë“œ: {args.load_split}")
        data = comparator.prepare_data_from_files(args.load_split)
        logs_df = None  # ë¶„í• ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ë¡œë“œë¨
    elif os.path.exists(default_split_dir) and os.path.isdir(default_split_dir):
        # ê¸°ë³¸ split_data ë””ë ‰í† ë¦¬ê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
        print(f"ğŸ“‚ ê¸°ì¡´ ë¶„í•  ë°ì´í„° ìë™ ë¡œë“œ: {default_split_dir}")
        data = comparator.prepare_data_from_files(default_split_dir)
        logs_df = None  # ë¶„í• ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ë¡œë“œë¨
    elif args.load_chunks:
        # ì²­í¬ íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
        print(f"ğŸ“‚ ì²­í¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ: {args.load_chunks}")
        logs_df = parser.load_from_chunks(args.load_chunks)
    elif args.load_parsed:
        # ì €ì¥ëœ íŒŒì‹± ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“‚ ì €ì¥ëœ íŒŒì‹± ë°ì´í„° ë¡œë“œ: {args.load_parsed}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ì˜µì…˜ì´ ìˆìœ¼ë©´ ë¶„í•  ìˆ˜í–‰
        if args.streaming_split:
            split_output_dir = args.split_output_dir
            if split_output_dir is None:
                split_output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'split_data')
            
            print(f"   ğŸ’¡ ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ëª¨ë“œ: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„í•  ì¤‘...")
            chunk_size = args.chunk_read_size if args.chunk_read_size else 100000
            split_files = parser.prepare_data_streaming(
                args.load_parsed,
                split_output_dir,
                train_ratio=0.8,
                valid_ratio=0.2,
                chunk_size=chunk_size
            )
            
            # ë¶„í• ëœ ë°ì´í„° ë¡œë“œ
            data = comparator.prepare_data_from_files(split_output_dir)
            logs_df = None  # ë¶„í• ëœ ë°ì´í„°ëŠ” ì´ë¯¸ ë¡œë“œë¨
        else:
            if args.chunk_read_size:
                print(f"   ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ: ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (ì²­í¬ í¬ê¸°: {args.chunk_read_size:,}ê°œ)")
            logs_df = parser.load_parsed_data(args.load_parsed, chunk_size=args.chunk_read_size)
    elif os.path.exists(default_parsed_file):
        # ê¸°ë³¸ parsed_data.parquet íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì‚¬ìš©
        print(f"ğŸ“‚ ê¸°ì¡´ íŒŒì‹± ë°ì´í„° ìë™ ë¡œë“œ: {default_parsed_file}")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ìˆ˜í–‰ (split_dataê°€ ì—†ìœ¼ë©´)
        if not os.path.exists(default_split_dir):
            print(f"   ğŸ’¡ ë¶„í•  ë°ì´í„°ê°€ ì—†ì–´ ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ìˆ˜í–‰ ì¤‘...")
            split_output_dir = default_split_dir
            chunk_size = args.chunk_read_size if args.chunk_read_size else 100000
            split_files = parser.prepare_data_streaming(
                default_parsed_file,
                split_output_dir,
                train_ratio=0.8,
                valid_ratio=0.2,
                chunk_size=chunk_size
            )
            # ë¶„í• ëœ ë°ì´í„° ë¡œë“œ
            data = comparator.prepare_data_from_files(split_output_dir)
            logs_df = None
        else:
            # ë¶„í•  ë°ì´í„°ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëƒ¥ ë¡œë“œ
            if args.chunk_read_size:
                print(f"   ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ: ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (ì²­í¬ í¬ê¸°: {args.chunk_read_size:,}ê°œ)")
            logs_df = parser.load_parsed_data(default_parsed_file, chunk_size=args.chunk_read_size)
    else:
        # ìƒˆë¡œ íŒŒì‹±
        print("ğŸ“ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘...")
        print(f"   ğŸ’¡ ê¸°ì¡´ íŒŒì‹± ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.")
        # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
        # í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥:
        # - max_files: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ ìˆ˜
        # - sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ ë¼ì¸ ìˆ˜
        # - chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ 10,000)
        # - max_total_lines: ì „ì²´ ìµœëŒ€ ë¼ì¸ ìˆ˜
        logs_df = parser.parse_directory(
            log_directory,
            max_files=args.max_files,        # ì „ì²´ íŒŒì¼ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìˆ«ìë¡œ ì œí•œ)
            sample_lines=None,     # ì „ì²´ ë¼ì¸ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ìˆ«ìë¡œ ì œí•œ)
            chunk_size=5000,       # ì²­í¬ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½, ê¸°ë³¸ê°’ë³´ë‹¤ ì‘ê²Œ)
            max_total_lines=args.max_total_lines,  # ì „ì²´ ìµœëŒ€ ë¼ì¸ ìˆ˜ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì„¤ì •)
            save_chunks_to_disk=True,  # íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            keep_chunks=args.keep_chunks  # ì²­í¬ íŒŒì¼ ìœ ì§€ ì—¬ë¶€
        )
        
        # íŒŒì‹± ë°ì´í„° ì €ì¥
        if args.save_parsed:
            parser.save_parsed_data(logs_df, args.save_parsed)
        elif args.parse_only:
            # ê¸°ë³¸ ì €ì¥ ê²½ë¡œ ì‚¬ìš©
            parser.save_parsed_data(logs_df, default_parsed_file)
            print(f"\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ íŒŒì‹± ë°ì´í„° ì¬ì‚¬ìš©:")
            print(f"   python log_specific_model_comparison.py --load-parsed {default_parsed_file}")
            return
        else:
            # íŒŒì‹±ë§Œ í•˜ê³  ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œì— ì €ì¥ (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ì¬ì‚¬ìš©)
            parser.save_parsed_data(logs_df, default_parsed_file)
            print(f"ğŸ’¾ íŒŒì‹± ë°ì´í„° ì €ì¥: {default_parsed_file} (ë‹¤ìŒ ì‹¤í–‰ ì‹œ ìë™ ì¬ì‚¬ìš©)")
    
    # 3. ë°ì´í„° ì¤€ë¹„
    print("\n3ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„")
    
    # ë¶„í• ëœ ë°ì´í„°ê°€ ì´ë¯¸ ë¡œë“œëœ ê²½ìš°
    split_data_dir = None
    if logs_df is None:
        # ë¶„í• ëœ ë°ì´í„°ì—ì„œ ë¡œë“œí•œ ê²½ìš°
        if args.load_split:
            split_data_dir = args.load_split
        elif args.streaming_split:
            split_data_dir = split_output_dir if 'split_output_dir' in locals() else default_split_dir
        else:
            # ê¸°ë³¸ split_data ë””ë ‰í† ë¦¬ í™•ì¸
            if os.path.exists(default_split_dir):
                split_data_dir = default_split_dir
        
        # í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” trainë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if 'data' not in locals() or data is None:
            print("   ğŸ’¡ ë©”ëª¨ë¦¬ ì ˆì•½: í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” Trainë§Œ ë¡œë“œí•©ë‹ˆë‹¤.")
            data = comparator.prepare_data_from_files(split_data_dir, load_only_train=True)
        
        # ë°ì´í„° ìƒ˜í”Œë§ (ë¹ ë¥¸ í•™ìŠµìš©)
        if args.sample_size and data.get('train_normal') is not None and len(data['train_normal']) > args.sample_size:
            print(f"\nğŸ“Š í•™ìŠµ ë°ì´í„° ìƒ˜í”Œë§ ì¤‘: {len(data['train_normal']):,}ê°œ â†’ {args.sample_size:,}ê°œ")
            data['train_normal'] = data['train_normal'].sample(n=args.sample_size, random_state=42).reset_index(drop=True)
            data['train_normal'] = data['train_normal'].sort_values('timestamp').reset_index(drop=True)
            print(f"   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(data['train_normal']):,}ê°œ")
    else:
        if logs_df.empty:
            print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(logs_df):,}ê°œ ë¡œê·¸ ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
        
        # ë°ì´í„° ìƒ˜í”Œë§ (ë¹ ë¥¸ í•™ìŠµìš©)
        if args.sample_size and len(logs_df) > args.sample_size:
            print(f"\nğŸ“Š ë°ì´í„° ìƒ˜í”Œë§ ì¤‘: {len(logs_df):,}ê°œ â†’ {args.sample_size:,}ê°œ")
            logs_df = logs_df.sample(n=args.sample_size, random_state=42).reset_index(drop=True)
            logs_df = logs_df.sort_values('timestamp').reset_index(drop=True)
            print(f"   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(logs_df):,}ê°œ")
        
        data = comparator.prepare_data(logs_df, train_ratio=0.8, valid_ratio=0.2)
    
    # 4. ëª¨ë¸ í•™ìŠµ
    print("\n4ë‹¨ê³„: ë¡œê·¸ íŠ¹í™” ëª¨ë¸ í•™ìŠµ")
    print(f"âš¡ ë¹ ë¥¸ í•™ìŠµ ì„¤ì •:")
    print(f"   - Epochs: {args.epochs}")
    print(f"   - Batch Size: {args.batch_size}")
    if data.get('train_normal') is not None:
        print(f"   - í•™ìŠµ ë°ì´í„°: {len(data['train_normal']):,}ê°œ")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í•™ìŠµ (LogRobust ì œì™¸ - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ê³  OOM ë°œìƒ ê°€ëŠ¥)
    available_models = ['deeplog', 'loganomaly']
    print("   âœ… ì‚¬ìš© ëª¨ë¸: DeepLog, LogAnomaly (LogRobust ì œì™¸)")
    
    # í•™ìŠµ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì„¤ì •
    log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs', 'training')
    
    comparator.train_models(
        data['train_normal'], 
        valid_normal_logs=data.get('valid_normal'),
        model_types=available_models,
        log_dir=log_dir,
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    if not comparator.trained_systems:
        print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•™ìŠµ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
    del data['train_normal']
    del data['train_error']
    if 'valid_normal' in data:
        del data['valid_normal']
    
    # 5. ëª¨ë¸ í‰ê°€
    print("\n5ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    
    # í‰ê°€ ë‹¨ê³„ì—ì„œ test ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    if split_data_dir and (data.get('test_logs', pd.DataFrame()).empty or len(data.get('test_logs', pd.DataFrame())) == 0):
        print("   ğŸ’¡ í‰ê°€ìš© ë°ì´í„° ë¡œë“œ ì¤‘...")
        test_data = comparator.load_test_data(split_data_dir)
        data['test_logs'] = test_data['test_logs']
        data['y_test'] = test_data['y_test']
        if not test_data['valid_logs'].empty:
            data['valid_logs'] = test_data['valid_logs']
            data['y_valid'] = test_data['y_valid']
    
    # í‰ê°€ ë°ì´í„° ìƒ˜í”Œë§ (ë¹ ë¥¸ í‰ê°€ìš©)
    test_logs = data['test_logs']
    y_test = data['y_test']
    
    if args.eval_sample_size and len(test_logs) > args.eval_sample_size:
        print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ìƒ˜í”Œë§ ì¤‘: {len(test_logs):,}ê°œ â†’ {args.eval_sample_size:,}ê°œ")
        print(f"   âš¡ ë¹ ë¥¸ í‰ê°€ ëª¨ë“œ: ìƒ˜í”Œë§ìœ¼ë¡œ í‰ê°€ ì‹œê°„ ë‹¨ì¶•")
        # ì‹œê°„ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒ˜í”Œë§
        sample_indices = np.linspace(0, len(test_logs) - 1, args.eval_sample_size, dtype=int)
        test_logs = test_logs.iloc[sample_indices].reset_index(drop=True)
        y_test = y_test[sample_indices]
        print(f"   âœ… ìƒ˜í”Œë§ ì™„ë£Œ: {len(test_logs):,}ê°œ")
    
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„°: {len(test_logs):,}ê°œ ë¡œê·¸")
    print(f"   ì˜ˆìƒ í‰ê°€ ì‹œê°„: ëª¨ë¸ë‹¹ ì•½ {len(test_logs) // 1000}ì´ˆ (ëŒ€ëµì  ì¶”ì •)")
    
    # ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ ì„¤ì •
    target_metrics = {
        'accuracy': args.target_accuracy,
        'precision': args.target_precision,
        'recall': args.target_recall,
        'f1_score': args.target_f1,
        'specificity': args.target_specificity
    }
    
    results = comparator.evaluate_models(
        test_logs, 
        y_test,
        optimize_threshold=args.optimize_threshold,
        target_metrics=target_metrics
    )
    
    if not results:
        print("âš ï¸ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 6. ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ë° ê·¸ë˜í”„ ìƒì„±
    print("\n6ë‹¨ê³„: ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±")
    
    # ê²°ê³¼ í´ë” ì§€ì • (log_specific_comparison_4)
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results/log_specific_comparison_4"
    
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ í´ë”: {output_dir}")
    os.makedirs(output_dir, exist_ok=True)
    
    comparison_df, best_model = comparator.generate_comparison_report(output_dir=output_dir)
    
    # 7. ìµœì  ëª¨ë¸ ì„ ì •
    print("\n7ë‹¨ê³„: ìµœì  ëª¨ë¸ ì„ ì •")
    best_model_name, best_system = comparator.get_best_model()
    
    print(f"\n{'='*70}")
    print(f"ğŸ† ìµœì¢… ì„ ì •ëœ ëª¨ë¸: {best_model_name.upper()}")
    print(f"{'='*70}")
    
    best_metrics = results[best_model_name]
    print(f"\nì„±ëŠ¥ ì§€í‘œ:")
    print(f"   ì •í™•ë„: {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)")
    print(f"   ì •ë°€ë„: {best_metrics['precision']:.4f} ({best_metrics['precision']*100:.2f}%)")
    print(f"   ì¬í˜„ìœ¨: {best_metrics['recall']:.4f} ({best_metrics['recall']*100:.2f}%)")
    print(f"   F1 ì ìˆ˜: {best_metrics['f1_score']:.4f}")
    if best_metrics['roc_auc']:
        print(f"   ROC-AUC: {best_metrics['roc_auc']:.4f}")
    
    # ì‹¬ê°ë„ ì •ë³´
    if best_metrics.get('severity_info'):
        severity_info = best_metrics['severity_info']
        print(f"\nì‹¬ê°ë„ ë¶„ì„:")
        print(f"   íƒì§€ëœ ì´ìƒ ì‹œí€€ìŠ¤: {severity_info.get('total_anomalies', 0)}ê°œ")
        if 'by_severity' in severity_info:
            print(f"   ì‹¬ê°ë„ ë¶„í¬:")
            for level, count in severity_info['by_severity'].items():
                print(f"      {level}: {count}ê°œ")
    
    # 8. ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥
    print("\n8ë‹¨ê³„: ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥")
    report_path = os.path.join(output_dir, "comparison_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("ë¡œê·¸ íŠ¹í™” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸\n")
        f.write("=" * 70 + "\n\n")
        
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("ë°ì´í„° ì •ë³´:\n")
        # ë°ì´í„°ê°€ ì´ë¯¸ ì‚­ì œë˜ì—ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        train_normal_count = len(data.get('train_normal', pd.DataFrame()))
        train_error_count = len(data.get('train_error', pd.DataFrame()))
        valid_normal_count = len(data.get('valid_normal', pd.DataFrame())) if not data.get('valid_normal', pd.DataFrame()).empty else 0
        valid_error_count = len(data.get('valid_error', pd.DataFrame())) if not data.get('valid_error', pd.DataFrame()).empty else 0
        test_normal_count = len(data.get('test_normal', pd.DataFrame()))
        test_error_count = len(data.get('test_error', pd.DataFrame()))
        
        total_logs = train_normal_count + train_error_count + valid_normal_count + valid_error_count + test_normal_count + test_error_count
        if total_logs > 0:
            f.write(f"  - ì „ì²´ ë¡œê·¸: {total_logs}ê°œ\n")
            f.write(f"  - í•™ìŠµ ì •ìƒ ë¡œê·¸: {train_normal_count}ê°œ ({train_normal_count/total_logs*100:.1f}%)\n")
            if valid_normal_count > 0:
                f.write(f"  - ê²€ì¦ ì •ìƒ ë¡œê·¸: {valid_normal_count}ê°œ ({valid_normal_count/total_logs*100:.1f}%)\n")
            f.write(f"  - í…ŒìŠ¤íŠ¸ ì •ìƒ ë¡œê·¸: {test_normal_count}ê°œ ({test_normal_count/total_logs*100:.1f}%)\n")
            f.write(f"  - í…ŒìŠ¤íŠ¸ ì—ëŸ¬ ë¡œê·¸: {test_error_count}ê°œ ({test_error_count/total_logs*100:.1f}%)\n\n")
        
        f.write("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n")
        f.write(comparison_df.to_string(index=False))
        f.write("\n\n")
        
        f.write(f"ìµœì¢… ì„ ì •ëœ ëª¨ë¸: {best_model_name.upper()}\n")
        f.write(f"ì¢…í•© ì ìˆ˜: {best_metrics['f1_score']*0.4 + best_metrics['accuracy']*0.3 + best_metrics['recall']*0.2 + best_metrics['precision']*0.1:.4f}\n\n")
        
        for model_name, metrics in results.items():
            f.write(f"[{model_name.upper()}] ìƒì„¸ ê²°ê³¼:\n")
            f.write(f"  ì •í™•ë„: {metrics['accuracy']:.4f}\n")
            f.write(f"  ì •ë°€ë„: {metrics['precision']:.4f}\n")
            f.write(f"  ì¬í˜„ìœ¨: {metrics['recall']:.4f}\n")
            f.write(f"  F1 ì ìˆ˜: {metrics['f1_score']:.4f}\n")
            if metrics['roc_auc']:
                f.write(f"  ROC-AUC: {metrics['roc_auc']:.4f}\n")
            f.write(f"  íŠ¹ì´ë„: {metrics['specificity']:.4f}\n")
            f.write(f"  í˜¼ë™ í–‰ë ¬:\n")
            f.write(f"    [ì •ìƒâ†’ì •ìƒ: {metrics['tn']:4d}  ì •ìƒâ†’ì´ìƒ: {metrics['fp']:4d}]\n")
            f.write(f"    [ì´ìƒâ†’ì •ìƒ: {metrics['fn']:4d}  ì´ìƒâ†’ì´ìƒ: {metrics['tp']:4d}]\n")
            
            # ì‹¬ê°ë„ ì •ë³´
            if metrics.get('severity_info'):
                severity_info = metrics['severity_info']
                f.write(f"\n  ì‹¬ê°ë„ ë¶„ì„:\n")
                f.write(f"    íƒì§€ëœ ì´ìƒ ì‹œí€€ìŠ¤: {severity_info.get('total_anomalies', 0)}ê°œ\n")
                if 'by_severity' in severity_info:
                    f.write(f"    ì‹¬ê°ë„ ë¶„í¬:\n")
                    for level, count in severity_info['by_severity'].items():
                        f.write(f"      {level}: {count}ê°œ\n")
                if 'avg_severity_score' in severity_info:
                    f.write(f"    í‰ê·  ì‹¬ê°ë„ ì ìˆ˜: {severity_info['avg_severity_score']:.2f}\n")
            f.write("\n")
    
    print(f"ğŸ’¾ ìƒì„¸ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    print("\n" + "=" * 70)
    print("âœ… ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ!")
    print("=" * 70)
    print(f"\nìµœì¢… ì„ ì • ëª¨ë¸: {best_model_name.upper()}")
    print(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main()

