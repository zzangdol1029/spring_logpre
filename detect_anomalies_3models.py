#!/usr/bin/env python3
"""
3ê°œ ëª¨ë¸(DeepLog, LogAnomaly, LogRobust)ì„ ì‚¬ìš©í•œ ì´ìƒ íƒì§€ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python detect_anomalies_3models.py --log-file <ë¡œê·¸íŒŒì¼ê²½ë¡œ>
    python detect_anomalies_3models.py --log-dir <ë¡œê·¸ë””ë ‰í† ë¦¬ê²½ë¡œ>
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
from datetime import datetime
from log_anomaly_detector import SpringBootLogParser
from log_specific_anomaly_detectors import LogSpecificAnomalySystem


def load_trained_models(results_dir=None):
    """
    í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    
    Args:
        results_dir: ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (Noneì´ë©´ ìë™ìœ¼ë¡œ ì°¾ê¸°)
    
    Returns:
        dict: ëª¨ë¸ íƒ€ì…ë³„ ì‹œìŠ¤í…œ ë”•ì…”ë„ˆë¦¬
    """
    if results_dir is None:
        # ìë™ìœ¼ë¡œ ìµœì‹  ê²°ê³¼ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        base_dir = os.path.dirname(os.path.abspath(__file__))
        results_base = os.path.join(base_dir, 'results', 'log_specific_comparison')
        
        # ë²ˆí˜¸ê°€ ê°€ì¥ í° ë””ë ‰í† ë¦¬ ì°¾ê¸°
        max_num = -1
        latest_dir = None
        
        if os.path.exists(results_base):
            latest_dir = results_base
            max_num = 0
        
        for i in range(1, 100):
            check_dir = f"{results_base}_{i}"
            if os.path.exists(check_dir):
                latest_dir = check_dir
                max_num = i
            else:
                break
        
        if latest_dir:
            results_dir = latest_dir
            print(f"ğŸ“‚ ìµœì‹  í•™ìŠµ ê²°ê³¼ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {results_dir}")
        else:
            print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¨¼ì € log_specific_model_comparison.pyë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
            return None
    
    # ëª¨ë¸ì€ í•™ìŠµ ì‹œì ì— ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, 
    # LogSpecificAnomalySystemì„ ìƒˆë¡œ ìƒì„±í•˜ê³  í•™ìŠµëœ ìƒíƒœë¥¼ ê°€ì •
    # ì‹¤ì œë¡œëŠ” ëª¨ë¸ì„ ì €ì¥/ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ì´ í•„ìš”í•¨
    
    print("âš ï¸ í˜„ì¬ëŠ” ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("   ì´ìƒ íƒì§€ë¥¼ í•˜ë ¤ë©´ ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.")
    print("   í•™ìŠµëœ ëª¨ë¸ì€ ë©”ëª¨ë¦¬ì—ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.")
    
    return None


def detect_with_3models(logs_df, trained_systems=None):
    """
    3ê°œ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€ ìˆ˜í–‰
    
    Args:
        logs_df: ë¡œê·¸ DataFrame
        trained_systems: í•™ìŠµëœ ì‹œìŠ¤í…œ ë”•ì…”ë„ˆë¦¬ (Noneì´ë©´ ìƒˆë¡œ í•™ìŠµ)
    
    Returns:
        dict: ëª¨ë¸ë³„ íƒì§€ ê²°ê³¼
    """
    print("=" * 70)
    print("3ê°œ ëª¨ë¸ ì´ìƒ íƒì§€ ì‹œì‘")
    print("=" * 70)
    print(f"ğŸ“Š ë¶„ì„í•  ë¡œê·¸: {len(logs_df):,}ê°œ")
    print()
    
    results = {}
    
    # 3ê°œ ëª¨ë¸ íƒ€ì…
    model_types = ['deeplog', 'loganomaly', 'logrobust']
    
    # PyTorch í™•ì¸
    try:
        import torch
    except ImportError:
        print("âš ï¸ PyTorchê°€ ì—†ì–´ LogRobustë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        model_types = ['deeplog', 'loganomaly']
    
    if trained_systems is None:
        print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤.")
        print("   log_specific_model_comparison.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        return None
    
    # ê° ëª¨ë¸ë¡œ ì´ìƒ íƒì§€
    for model_type in model_types:
        if model_type not in trained_systems:
            print(f"âš ï¸ {model_type.upper()} ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        print(f"\n[{model_type.upper()}] ì´ìƒ íƒì§€ ì¤‘...")
        system = trained_systems[model_type]
        
        try:
            # ì´ìƒ íƒì§€
            detection_results = system.detect_anomalies(logs_df)
            
            if detection_results and not detection_results.get('anomalies', pd.DataFrame()).empty:
                anomalies_df = detection_results['anomalies']
                summary = detection_results.get('summary', {})
                
                print(f"   âœ… {len(anomalies_df)}ê°œ ì´ìƒ ì‹œí€€ìŠ¤ íƒì§€")
                
                if 'by_severity' in summary:
                    print(f"   ì‹¬ê°ë„ ë¶„í¬:")
                    for level, count in summary['by_severity'].items():
                        print(f"      {level}: {count}ê°œ")
                
                results[model_type] = {
                    'anomalies': anomalies_df,
                    'summary': summary,
                    'total_detected': len(anomalies_df)
                }
            else:
                print(f"   âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                results[model_type] = {
                    'anomalies': pd.DataFrame(),
                    'summary': {},
                    'total_detected': 0
                }
        except Exception as e:
            print(f"   âŒ ì´ìƒ íƒì§€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            results[model_type] = {
                'anomalies': pd.DataFrame(),
                'summary': {},
                'total_detected': 0,
                'error': str(e)
            }
    
    return results


def compare_results(results):
    """
    3ê°œ ëª¨ë¸ì˜ íƒì§€ ê²°ê³¼ ë¹„êµ
    
    Args:
        results: ëª¨ë¸ë³„ íƒì§€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "=" * 70)
    print("3ê°œ ëª¨ë¸ íƒì§€ ê²°ê³¼ ë¹„êµ")
    print("=" * 70)
    
    comparison_data = []
    for model_type, result in results.items():
        total = result.get('total_detected', 0)
        summary = result.get('summary', {})
        by_severity = summary.get('by_severity', {})
        
        comparison_data.append({
            'ëª¨ë¸': model_type.upper(),
            'íƒì§€ëœ ì´ìƒ': f"{total}ê°œ",
            'CRITICAL': by_severity.get('CRITICAL', 0),
            'HIGH': by_severity.get('HIGH', 0),
            'MEDIUM': by_severity.get('MEDIUM', 0),
            'LOW': by_severity.get('LOW', 0),
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print("\nğŸ“Š íƒì§€ ê²°ê³¼ ë¹„êµ:")
    print(comparison_df.to_string(index=False))
    
    # ê³µí†µìœ¼ë¡œ íƒì§€ëœ ì´ìƒ ì°¾ê¸°
    print("\nğŸ” ê³µí†µ íƒì§€ ë¶„ì„:")
    all_anomalies = {}
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        if not anomalies_df.empty and 'sequence_index' in anomalies_df.columns:
            detected_indices = set(anomalies_df['sequence_index'].values)
            all_anomalies[model_type] = detected_indices
    
    if len(all_anomalies) >= 2:
        # 2ê°œ ì´ìƒ ëª¨ë¸ì´ ê³µí†µìœ¼ë¡œ íƒì§€í•œ ì‹œí€€ìŠ¤
        common_indices = set.intersection(*all_anomalies.values())
        print(f"   ê³µí†µ íƒì§€: {len(common_indices)}ê°œ ì‹œí€€ìŠ¤")
        
        # ê° ëª¨ë¸ë§Œ íƒì§€í•œ ì‹œí€€ìŠ¤
        for model_type, indices in all_anomalies.items():
            unique = indices - set.union(*[v for k, v in all_anomalies.items() if k != model_type])
            print(f"   {model_type.upper()}ë§Œ íƒì§€: {len(unique)}ê°œ ì‹œí€€ìŠ¤")


def save_results(results, output_dir):
    """
    íƒì§€ ê²°ê³¼ ì €ì¥
    
    Args:
        results: ëª¨ë¸ë³„ íƒì§€ ê²°ê³¼
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘: {output_dir}")
    
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        
        if not anomalies_df.empty:
            output_path = os.path.join(output_dir, f"{model_type}_anomalies_{timestamp}.csv")
            anomalies_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"   âœ… {model_type.upper()}: {output_path}")
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    comparison_data = []
    for model_type, result in results.items():
        total = result.get('total_detected', 0)
        summary = result.get('summary', {})
        by_severity = summary.get('by_severity', {})
        
        comparison_data.append({
            'ëª¨ë¸': model_type.upper(),
            'íƒì§€ëœ_ì´ìƒ': total,
            'CRITICAL': by_severity.get('CRITICAL', 0),
            'HIGH': by_severity.get('HIGH', 0),
            'MEDIUM': by_severity.get('MEDIUM', 0),
            'LOW': by_severity.get('LOW', 0),
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_path = os.path.join(output_dir, f"comparison_{timestamp}.csv")
    comparison_df.to_csv(comparison_path, index=False, encoding='utf-8-sig')
    print(f"   âœ… ë¹„êµ ê²°ê³¼: {comparison_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='3ê°œ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€')
    parser.add_argument('--log-file', type=str, default=None,
                       help='ë¶„ì„í•  ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--log-dir', type=str, default=None,
                       help='ë¶„ì„í•  ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: results/detection_YYYYMMDD_HHMMSS)')
    parser.add_argument('--models-dir', type=str, default=None,
                       help='í•™ìŠµëœ ëª¨ë¸ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ìµœì‹  ê²°ê³¼ ë””ë ‰í† ë¦¬)')
    args = parser.parse_args()
    
    # ë¡œê·¸ íŒŒì¼/ë””ë ‰í† ë¦¬ í™•ì¸
    if not args.log_file and not args.log_dir:
        print("âŒ --log-file ë˜ëŠ” --log-dir ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        parser.print_help()
        return
    
    # ë¡œê·¸ íŒŒì‹±
    print("=" * 70)
    print("ë¡œê·¸ íŒŒì¼ íŒŒì‹±")
    print("=" * 70)
    
    parser_obj = SpringBootLogParser()
    
    if args.log_file:
        if not os.path.exists(args.log_file):
            print(f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.log_file}")
            return
        print(f"ğŸ“„ ë¡œê·¸ íŒŒì¼: {args.log_file}")
        logs_df = parser_obj.parse_log_file(args.log_file)
    else:
        if not os.path.exists(args.log_dir):
            print(f"âŒ ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.log_dir}")
            return
        print(f"ğŸ“ ë¡œê·¸ ë””ë ‰í† ë¦¬: {args.log_dir}")
        logs_df = parser_obj.parse_directory(args.log_dir, max_files=None, sample_lines=None)
    
    if logs_df.empty:
        print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… {len(logs_df):,}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
    
    # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ (í˜„ì¬ëŠ” í•™ìŠµ ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ ê²½ê³ ë§Œ)
    print("\n" + "=" * 70)
    print("âš ï¸ ì¤‘ìš”: í˜„ì¬ëŠ” ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
    print("=" * 70)
    print("3ê°œ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€ë¥¼ í•˜ë ¤ë©´:")
    print("  1. log_specific_model_comparison.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ í•™ìŠµ")
    print("  2. í•™ìŠµëœ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€í•œ ìƒíƒœì—ì„œ")
    print("  3. ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê°™ì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰")
    print()
    print("ë˜ëŠ” log_specific_model_comparison.pyì˜ evaluate_models ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = args.output_dir
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(base_dir, 'results', f'detection_{timestamp}')
    
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ì‚¬ìš© ë°©ë²•")
    print("=" * 70)
    print("""
í˜„ì¬ëŠ” ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•˜ì„¸ìš”:

ë°©ë²• 1: log_specific_model_comparison.py ì‚¬ìš© (ê¶Œì¥)
  - ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í•™ìŠµê³¼ í‰ê°€ë¥¼ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤
  - í‰ê°€ ë‹¨ê³„ì—ì„œ test ë°ì´í„°ì— ëŒ€í•´ 3ê°œ ëª¨ë¸ë¡œ ì´ìƒ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤
  - ê²°ê³¼ëŠ” results/log_specific_comparison_*/ í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤

ë°©ë²• 2: ì§ì ‘ ì½”ë“œ ì‘ì„±
  from log_specific_anomaly_detectors import LogSpecificAnomalySystem
  from log_anomaly_detector import SpringBootLogParser
  
  # 1. ë¡œê·¸ íŒŒì‹±
  parser = SpringBootLogParser()
  logs_df = parser.parse_directory("logs/backup")
  
  # 2. ê° ëª¨ë¸ í•™ìŠµ
  systems = {}
  for model_type in ['deeplog', 'loganomaly', 'logrobust']:
      system = LogSpecificAnomalySystem(model_type=model_type)
      system.load_logs(logs_df)
      system.train()
      systems[model_type] = system
  
  # 3. ìƒˆë¡œìš´ ë¡œê·¸ì— ëŒ€í•´ ì´ìƒ íƒì§€
  new_logs_df = parser.parse_log_file("new_log.log")
  for model_type, system in systems.items():
      results = system.detect_anomalies(new_logs_df)
      print(f"{model_type}: {len(results['anomalies'])}ê°œ íƒì§€")
""")


if __name__ == "__main__":
    main()

