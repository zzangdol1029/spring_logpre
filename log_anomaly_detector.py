"""
Spring Boot ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸
backup í´ë”ì˜ ë¡œê·¸ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ì´ìƒ íŒ¨í„´ì„ íƒì§€í•©ë‹ˆë‹¤.
"""

import re
import os
import glob
import pickle
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from pyod.models.auto_encoder import AutoEncoder
from pyod.models.iforest import IForest
from pyod.models.lof import LOF
import warnings
warnings.filterwarnings('ignore')


class SpringBootLogParser:
    """Spring Boot ë¡œê·¸ íŒŒì„œ"""
    
    # Spring Boot ë¡œê·¸ íŒ¨í„´: 2025-07-02 15:59:36.514  INFO 12185 --- [           main] k.r.b.f.c.Application : Starting...
    LOG_PATTERN = re.compile(
        r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\.\d{3})\s+'
        r'(\w+)\s+'
        r'(\d+)\s+'
        r'---\s+'
        r'\[([^\]]+)\]\s+'
        r'([^\s:]+)\s*:?\s*'
        r'(.*)'
    )
    
    ERROR_KEYWORDS = [
        'Exception', 'Error', 'Failed', 'Fatal', 'Critical',
        'Timeout', 'Connection refused', 'OutOfMemoryError',
        'NullPointerException', 'StackOverflowError',
        'ClassNotFoundException', 'NoClassDefFoundError',
        'SQLException', 'IOException', 'SocketException'
    ]
    
    def __init__(self):
        self.parsed_logs = []
        
    def parse_log_line(self, line):
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        match = self.LOG_PATTERN.match(line.strip())
        if match:
            timestamp_str, level, pid, thread, class_path, message = match.groups()
            try:
                timestamp = pd.to_datetime(timestamp_str, format='%Y-%m-%d %H:%M:%S.%f')
            except:
                timestamp = pd.to_datetime(timestamp_str, errors='coerce')
            
            # ì—ëŸ¬ í‚¤ì›Œë“œ í™•ì¸
            is_error = level in ['ERROR', 'FATAL'] or any(
                keyword.lower() in message.lower() for keyword in self.ERROR_KEYWORDS
            )
            
            return {
                'timestamp': timestamp,
                'level': level,
                'pid': pid,
                'thread': thread.strip(),
                'class_path': class_path,
                'message': message,
                'is_error': is_error,
                'message_length': len(message),
                'has_exception': 'Exception' in message or 'Error' in message
            }
        return None
    
    def parse_log_file(self, file_path, max_lines=None):
        """
        ë¡œê·¸ íŒŒì¼ íŒŒì‹±
        
        Args:
            file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            max_lines: ìµœëŒ€ íŒŒì‹±í•  ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        """
        logs = []
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line_num, line in enumerate(f, 1):
                    if max_lines and line_num > max_lines:
                        break
                    parsed = self.parse_log_line(line)
                    if parsed:
                        parsed['file_path'] = os.path.basename(file_path)
                        parsed['line_number'] = line_num
                        logs.append(parsed)
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
        return logs
    
    def parse_directory(self, directory_path, max_files=None, sample_lines=None, 
                       chunk_size=10000, max_total_lines=None, 
                       save_chunks_to_disk=True, chunk_dir=None, keep_chunks=False):
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë¡œê·¸ íŒŒì¼ íŒŒì‹± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            directory_path: ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_files: ìµœëŒ€ íŒŒì‹±í•  íŒŒì¼ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ íŒŒì‹±í•  ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            chunk_size: ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•  ë¡œê·¸ ë¼ì¸ ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
            max_total_lines: ì „ì²´ ìµœëŒ€ íŒŒì‹±í•  ë¼ì¸ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            save_chunks_to_disk: ì²­í¬ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ True, ë©”ëª¨ë¦¬ ì ˆì•½)
            chunk_dir: ì²­í¬ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ í”„ë¡œì íŠ¸ í´ë”/chunks)
            keep_chunks: ì²­í¬ íŒŒì¼ì„ ìœ ì§€í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ False, ë³‘í•© í›„ ì‚­ì œ)
        """
        import tempfile
        import shutil
        
        log_files = glob.glob(os.path.join(directory_path, '*.log'))
        
        if max_files:
            log_files = log_files[:max_files]
        
        print(f"ì´ {len(log_files)}ê°œ ë¡œê·¸ íŒŒì¼ ë°œê²¬")
        if max_files:
            print(f"  (ìµœëŒ€ {max_files}ê°œ íŒŒì¼ë§Œ ì²˜ë¦¬)")
        if max_total_lines:
            print(f"  (ì „ì²´ ìµœëŒ€ {max_total_lines:,}ê°œ ë¼ì¸ë§Œ ì²˜ë¦¬)")
        print(f"  (ì²­í¬ í¬ê¸°: {chunk_size:,}ê°œ ë¼ì¸)")
        
        # ì²­í¬ ì €ì¥ ë°©ì‹ ê²°ì •
        if save_chunks_to_disk:
            if chunk_dir is None:
                # í”„ë¡œì íŠ¸ í´ë” ë°‘ì— chunks ë””ë ‰í† ë¦¬ ìƒì„±
                # log_anomaly_detector.pyê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€
                current_dir = os.path.dirname(os.path.abspath(__file__))
                chunk_dir = os.path.join(current_dir, 'chunks')
            os.makedirs(chunk_dir, exist_ok=True)
            print(f"  ğŸ“ ì²­í¬ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {chunk_dir}")
            chunk_files = []  # íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        else:
            chunk_dfs = []  # ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸
        
        total_parsed = 0
        file_count = 0
        chunk_count = 0
        
        for file_path in log_files:
            if max_total_lines and total_parsed >= max_total_lines:
                print(f"\nâš ï¸ ì „ì²´ ìµœëŒ€ ë¼ì¸ ìˆ˜({max_total_lines:,})ì— ë„ë‹¬í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
                
            file_count += 1
            print(f"\n[{file_count}/{len(log_files)}] íŒŒì‹± ì¤‘: {os.path.basename(file_path)}")
            
            # íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ íŒŒì‹±
            file_logs = []
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    for line_num, line in enumerate(f, 1):
                        if max_total_lines and total_parsed >= max_total_lines:
                            break
                        if sample_lines and line_num > sample_lines:
                            break
                            
                        parsed = self.parse_log_line(line)
                        if parsed:
                            parsed['file_path'] = os.path.basename(file_path)
                            parsed['line_number'] = line_num
                            file_logs.append(parsed)
                            total_parsed += 1
                            
                            # ì²­í¬ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
                            if len(file_logs) >= chunk_size:
                                chunk_df = pd.DataFrame(file_logs)
                                
                                if save_chunks_to_disk:
                                    # íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                                    chunk_count += 1
                                    chunk_file = os.path.join(chunk_dir, f'chunk_{chunk_count:06d}.parquet')
                                    try:
                                        chunk_df.to_parquet(chunk_file, compression='snappy', engine='pyarrow')
                                        chunk_files.append(chunk_file)
                                        print(f"  - ì²­í¬ íŒŒì¼ ì €ì¥: {chunk_file} ({len(file_logs):,}ê°œ ë¼ì¸, ëˆ„ì : {total_parsed:,}ê°œ)")
                                    except ImportError:
                                        # pyarrowê°€ ì—†ìœ¼ë©´ pickle ì‚¬ìš©
                                        chunk_file = os.path.join(chunk_dir, f'chunk_{chunk_count:06d}.pkl')
                                        chunk_df.to_pickle(chunk_file)
                                        chunk_files.append(chunk_file)
                                        print(f"  - ì²­í¬ íŒŒì¼ ì €ì¥: {chunk_file} ({len(file_logs):,}ê°œ ë¼ì¸, ëˆ„ì : {total_parsed:,}ê°œ)")
                                    del chunk_df  # ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
                                else:
                                    # ë©”ëª¨ë¦¬ì— ì €ì¥
                                    chunk_dfs.append(chunk_df)
                                    print(f"  - ì²­í¬ ì €ì¥: {len(file_logs):,}ê°œ ë¼ì¸ (ëˆ„ì : {total_parsed:,}ê°œ)")
                                
                                file_logs = []  # ë©”ëª¨ë¦¬ í•´ì œ
                                
            except Exception as e:
                print(f"  âš ï¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                continue
            
            # ë‚¨ì€ ë¡œê·¸ ì²˜ë¦¬
            if file_logs:
                chunk_df = pd.DataFrame(file_logs)
                
                if save_chunks_to_disk:
                    chunk_count += 1
                    chunk_file = os.path.join(chunk_dir, f'chunk_{chunk_count:06d}.parquet')
                    try:
                        chunk_df.to_parquet(chunk_file, compression='snappy', engine='pyarrow')
                        chunk_files.append(chunk_file)
                        print(f"  - ì²­í¬ íŒŒì¼ ì €ì¥: {chunk_file} ({len(file_logs):,}ê°œ ë¼ì¸, ëˆ„ì : {total_parsed:,}ê°œ)")
                    except ImportError:
                        chunk_file = os.path.join(chunk_dir, f'chunk_{chunk_count:06d}.pkl')
                        chunk_df.to_pickle(chunk_file)
                        chunk_files.append(chunk_file)
                        print(f"  - ì²­í¬ íŒŒì¼ ì €ì¥: {chunk_file} ({len(file_logs):,}ê°œ ë¼ì¸, ëˆ„ì : {total_parsed:,}ê°œ)")
                    del chunk_df
                else:
                    chunk_dfs.append(chunk_df)
                    print(f"  - ì²­í¬ ì €ì¥: {len(file_logs):,}ê°œ ë¼ì¸ (ëˆ„ì : {total_parsed:,}ê°œ)")
            
            print(f"  âœ… íŒŒì¼ ì™„ë£Œ: {total_parsed:,}ê°œ ë¼ì¸ íŒŒì‹±")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del file_logs
        
        # ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë³‘í•©
        if save_chunks_to_disk:
            if not chunk_files:
                print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                if os.path.exists(chunk_dir) and chunk_dir.startswith(tempfile.gettempdir()):
                    os.rmdir(chunk_dir)
                return pd.DataFrame()
            
            print(f"\nğŸ“Š ì²­í¬ íŒŒì¼ ë³‘í•© ì¤‘... (ì´ {len(chunk_files)}ê°œ íŒŒì¼)")
            
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³‘í•© (ë©”ëª¨ë¦¬ ì ˆì•½)
            result_dfs = []
            for i, chunk_file in enumerate(chunk_files):
                try:
                    if chunk_file.endswith('.parquet'):
                        chunk_df = pd.read_parquet(chunk_file, engine='pyarrow')
                    else:
                        chunk_df = pd.read_pickle(chunk_file)
                    
                    result_dfs.append(chunk_df)
                    
                    # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ë³‘í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                    if len(result_dfs) >= 10:  # 10ê°œë§ˆë‹¤ ë³‘í•©
                        temp_df = pd.concat(result_dfs, ignore_index=True)
                        result_dfs = [temp_df]  # ë³‘í•©ëœ ê²°ê³¼ë§Œ ìœ ì§€
                    
                    # íŒŒì¼ ì‚­ì œëŠ” ë‚˜ì¤‘ì— keep_chunks ì˜µì…˜ì— ë”°ë¼ ê²°ì •
                    # ì—¬ê¸°ì„œëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ (ë³‘í•© í›„ ì¼ê´„ ì²˜ë¦¬)
                    
                    if (i + 1) % 10 == 0:
                        print(f"  - {i + 1}/{len(chunk_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
                        
                except Exception as e:
                    print(f"  âš ï¸ ì²­í¬ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {chunk_file}: {e}")
                    continue
            
            # ìµœì¢… ë³‘í•©
            if result_dfs:
                result_df = pd.concat(result_dfs, ignore_index=True)
            else:
                result_df = pd.DataFrame()
            
            # ì²­í¬ íŒŒì¼ ì •ë¦¬
            if not keep_chunks:
                # ì²­í¬ íŒŒì¼ ì‚­ì œ
                print(f"\nğŸ—‘ï¸  ì²­í¬ íŒŒì¼ ì‚­ì œ ì¤‘...")
                try:
                    for chunk_file in chunk_files:
                        if os.path.exists(chunk_file):
                            os.remove(chunk_file)
                    # ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
                    if os.path.exists(chunk_dir) and not os.listdir(chunk_dir):
                        os.rmdir(chunk_dir)
                        print(f"  âœ… ì²­í¬ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {chunk_dir}")
                    else:
                        print(f"  âœ… ì²­í¬ íŒŒì¼ ì‚­ì œ ì™„ë£Œ ({len(chunk_files)}ê°œ)")
                except Exception as e:
                    print(f"  âš ï¸ ì²­í¬ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            else:
                # ì²­í¬ íŒŒì¼ ìœ ì§€
                print(f"\nğŸ’¾ ì²­í¬ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {chunk_dir}")
                print(f"  - ì´ {len(chunk_files)}ê°œ ì²­í¬ íŒŒì¼")
                print(f"  ğŸ’¡ ì²­í¬ íŒŒì¼ ì‚­ì œ: shutil.rmtree('{chunk_dir}')")
            
        else:
            if not chunk_dfs:
                print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
            
            print(f"\nğŸ“Š ì²­í¬ ë³‘í•© ì¤‘... (ì´ {len(chunk_dfs)}ê°œ ì²­í¬)")
            result_df = pd.concat(chunk_dfs, ignore_index=True)
            del chunk_dfs
        
        print(f"âœ… ì´ {len(result_df):,}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        return result_df
    
    def save_parsed_data(self, logs_df: pd.DataFrame, output_path: str):
        """
        íŒŒì‹±ëœ ë°ì´í„°ë¥¼ Parquet íŒŒì¼ë¡œ ì €ì¥ (pyarrow ì—†ìœ¼ë©´ pickle ì‚¬ìš©)
        
        Args:
            logs_df: ì €ì¥í•  DataFrame
            output_path: ì €ì¥ ê²½ë¡œ (íŒŒì¼ëª… í¬í•¨)
        """
        # ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ì¶œ (íŒŒì¼ëª…ë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬)
        dir_path = os.path.dirname(output_path)
        if dir_path:  # ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
            os.makedirs(dir_path, exist_ok=True)
        # ë””ë ‰í† ë¦¬ ê²½ë¡œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥ (ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”)
        
        # Parquet ì €ì¥ ì‹œë„ (pyarrow ì‚¬ìš©)
        try:
            logs_df.to_parquet(output_path, compression='snappy', engine='pyarrow')
            file_format = "Parquet"
        except ImportError:
            # pyarrowê°€ ì—†ìœ¼ë©´ pickleë¡œ ì €ì¥
            if output_path.endswith('.parquet'):
                # í™•ì¥ìë¥¼ .pklë¡œ ë³€ê²½
                output_path = output_path.replace('.parquet', '.pkl')
            logs_df.to_pickle(output_path)
            file_format = "Pickle"
            print("âš ï¸ pyarrowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Pickle í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
            print("   ğŸ’¡ Parquet í˜•ì‹ì„ ì‚¬ìš©í•˜ë ¤ë©´: pip install pyarrow")
        
        file_size = os.path.getsize(output_path) / 1024 / 1024  # MB
        print(f"âœ… íŒŒì‹± ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path} ({file_format}, {file_size:.2f} MB)")
    
    def load_parsed_data(self, input_path: str, chunk_size: int = None) -> pd.DataFrame:
        """
        ì €ì¥ëœ íŒŒì‹± ë°ì´í„°ë¥¼ ë¡œë“œ (Parquet ë˜ëŠ” Pickle)
        
        Args:
            input_path: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            chunk_size: ì²­í¬ ë‹¨ìœ„ë¡œ ì½ì„ í¬ê¸° (Noneì´ë©´ ì „ì²´ ë¡œë“œ, ë©”ëª¨ë¦¬ ì ˆì•½ ì‹œ ì‚¬ìš©)
            
        Returns:
            ë¡œë“œëœ DataFrame
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        
        # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        if chunk_size and input_path.endswith('.parquet'):
            try:
                import pyarrow.parquet as pq
                parquet_file = pq.ParquetFile(input_path)
                
                print(f"ğŸ“‚ Parquet íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ëŠ” ì¤‘... (ì²­í¬ í¬ê¸°: {chunk_size:,}ê°œ)")
                print(f"   ì´ í–‰ ìˆ˜: {parquet_file.metadata.num_rows:,}ê°œ")
                
                result_dfs = []
                total_rows = 0
                
                for i, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):
                    batch_df = batch.to_pandas()
                    result_dfs.append(batch_df)
                    total_rows += len(batch_df)
                    
                    # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ë³‘í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                    if len(result_dfs) >= 10:
                        temp_df = pd.concat(result_dfs, ignore_index=True)
                        result_dfs = [temp_df]
                    
                    if (i + 1) % 10 == 0:
                        print(f"   - {i + 1}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({total_rows:,}ê°œ í–‰)")
                
                # ìµœì¢… ë³‘í•©
                if result_dfs:
                    logs_df = pd.concat(result_dfs, ignore_index=True)
                else:
                    logs_df = pd.DataFrame()
                
                print(f"âœ… íŒŒì‹± ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(logs_df):,}ê°œ ë¡œê·¸ ë¼ì¸ (ì²­í¬ ë°©ì‹)")
                return logs_df
            except ImportError:
                print("âš ï¸ pyarrowê°€ ì—†ì–´ ì²­í¬ ì½ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¡œë“œí•©ë‹ˆë‹¤.")
                chunk_size = None  # ì „ì²´ ë¡œë“œë¡œ ì „í™˜
        
        # ì „ì²´ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹)
        if input_path.endswith('.parquet'):
            try:
                logs_df = pd.read_parquet(input_path, engine='pyarrow')
            except ImportError:
                raise ImportError(
                    "Parquet íŒŒì¼ì„ ì½ìœ¼ë ¤ë©´ pyarrowê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
                    "ì„¤ì¹˜ ë°©ë²•: pip install pyarrow\n"
                    "ë˜ëŠ” Pickle íŒŒì¼(.pkl)ì„ ì‚¬ìš©í•˜ì„¸ìš”."
                )
        elif input_path.endswith('.pkl'):
            logs_df = pd.read_pickle(input_path)
        else:
            # í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì‹œë„í•´ë´„
            try:
                logs_df = pd.read_parquet(input_path, engine='pyarrow')
            except (ImportError, Exception):
                try:
                    logs_df = pd.read_pickle(input_path)
                except Exception:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {input_path}")
        
        print(f"âœ… íŒŒì‹± ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(logs_df):,}ê°œ ë¡œê·¸ ë¼ì¸")
        return logs_df
    
    def get_chunk_files(self, chunk_dir: str) -> list:
        """
        ì²­í¬ ë””ë ‰í† ë¦¬ì—ì„œ ì²­í¬ íŒŒì¼ ëª©ë¡ ë°˜í™˜
        
        Args:
            chunk_dir: ì²­í¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ì²­í¬ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ì •ë ¬ë¨)
        """
        if not os.path.exists(chunk_dir):
            return []
        
        chunk_files = []
        for ext in ['.parquet', '.pkl']:
            chunk_files.extend(glob.glob(os.path.join(chunk_dir, f'chunk_*{ext}')))
        
        # íŒŒì¼ëª… ê¸°ì¤€ ì •ë ¬
        chunk_files.sort()
        return chunk_files
    
    def load_from_chunks(self, chunk_dir: str, max_chunks: int = None) -> pd.DataFrame:
        """
        ì²­í¬ íŒŒì¼ì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        
        Args:
            chunk_dir: ì²­í¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_chunks: ìµœëŒ€ ë¡œë“œí•  ì²­í¬ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ë¡œë“œëœ DataFrame
        """
        chunk_files = self.get_chunk_files(chunk_dir)
        
        if not chunk_files:
            print(f"âš ï¸ ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {chunk_dir}")
            return pd.DataFrame()
        
        if max_chunks:
            chunk_files = chunk_files[:max_chunks]
        
        print(f"ğŸ“‚ ì²­í¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘... (ì´ {len(chunk_files)}ê°œ íŒŒì¼)")
        
        # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë¡œë“œ
        result_dfs = []
        for i, chunk_file in enumerate(chunk_files):
            try:
                if chunk_file.endswith('.parquet'):
                    chunk_df = pd.read_parquet(chunk_file, engine='pyarrow')
                else:
                    chunk_df = pd.read_pickle(chunk_file)
                
                result_dfs.append(chunk_df)
                
                # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ë³‘í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                if len(result_dfs) >= 10:
                    temp_df = pd.concat(result_dfs, ignore_index=True)
                    result_dfs = [temp_df]
                
                if (i + 1) % 10 == 0:
                    print(f"  - {i + 1}/{len(chunk_files)}ê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
                    
            except Exception as e:
                print(f"  âš ï¸ ì²­í¬ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {chunk_file}: {e}")
                continue
        
        # ìµœì¢… ë³‘í•©
        if result_dfs:
            result_df = pd.concat(result_dfs, ignore_index=True)
            print(f"âœ… ì´ {len(result_df):,}ê°œ ë¡œê·¸ ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
        else:
            result_df = pd.DataFrame()
        
        return result_df
    
    def prepare_data_streaming(self, input_path: str, output_dir: str, 
                               train_ratio=0.8, valid_ratio=0.2, 
                               chunk_size=100000):
        """
        Parquet íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ train/valid/testë¡œ ìŠ¤íŠ¸ë¦¬ë° ë¶„í• 
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê° ë¶„í• ì„ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            input_path: ì…ë ¥ Parquet íŒŒì¼ ê²½ë¡œ
            output_dir: ë¶„í• ëœ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.8 = 80%)
            valid_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (trainì˜ ë¹„ìœ¨, ê¸°ë³¸ 0.2 = 20%)
            chunk_size: ì²­í¬ ë‹¨ìœ„ë¡œ ì½ì„ í¬ê¸°
            
        Returns:
            ë¶„í• ëœ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë”•ì…”ë„ˆë¦¬
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        
        if not input_path.endswith('.parquet'):
            raise ValueError("ìŠ¤íŠ¸ë¦¬ë° ë¶„í• ì€ Parquet íŒŒì¼ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
        
        try:
            import pyarrow.parquet as pq
        except ImportError:
            raise ImportError("ìŠ¤íŠ¸ë¦¬ë° ë¶„í• ì„ ìœ„í•´ pyarrowê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pyarrow")
        
        os.makedirs(output_dir, exist_ok=True)
        
        parquet_file = pq.ParquetFile(input_path)
        total_rows = parquet_file.metadata.num_rows
        
        print("=" * 60)
        print("ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ë°ì´í„° ë¶„í• ")
        print("=" * 60)
        print(f"   ì…ë ¥ íŒŒì¼: {input_path}")
        print(f"   ì´ í–‰ ìˆ˜: {total_rows:,}ê°œ")
        print(f"   ì²­í¬ í¬ê¸°: {chunk_size:,}ê°œ")
        print(f"   ë¶„í•  ë¹„ìœ¨: Train {train_ratio*100:.0f}% / Valid {valid_ratio*100:.0f}% (trainì˜) / Test {100-train_ratio*100:.0f}%")
        print(f"   ì €ì¥ ë””ë ‰í† ë¦¬: {output_dir}")
        
        # ë¶„í•  ê²½ê³„ ê³„ì‚°
        train_end_idx = int(total_rows * train_ratio)
        valid_start_idx = int(train_end_idx * (1 - valid_ratio))
        
        # ê° ë¶„í• ì˜ ì„ì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
        temp_dir = os.path.join(output_dir, 'temp')
        os.makedirs(temp_dir, exist_ok=True)
        
        # ê° ë¶„í• ì˜ ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
        train_temp_files = []
        valid_temp_files = []
        test_temp_files = []
        
        # ì²­í¬ë¥¼ ì¼ì • ê°œìˆ˜ë§ˆë‹¤ íŒŒì¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
        merge_chunk_count = 5  # 5ê°œ ì²­í¬ë§ˆë‹¤ ë³‘í•©í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥
        
        # ê° ë¶„í• ì˜ í˜„ì¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        train_chunks = []
        valid_chunks = []
        test_chunks = []
        
        # í†µê³„
        train_count = 0
        valid_count = 0
        test_count = 0
        current_idx = 0
        
        print(f"\nğŸ“Š ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ ë¶„í•  ì¤‘... (ë©”ëª¨ë¦¬ íš¨ìœ¨ ëª¨ë“œ)")
        print(f"   ğŸ’¡ {merge_chunk_count}ê°œ ì²­í¬ë§ˆë‹¤ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½")
        
        def save_chunks_to_file(chunks, temp_files, prefix):
            """ì²­í¬ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ"""
            if not chunks:
                return temp_files
            
            # ë³‘í•©
            merged_df = pd.concat(chunks, ignore_index=True)
            chunks.clear()  # ë©”ëª¨ë¦¬ í•´ì œ
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            temp_file = os.path.join(temp_dir, f'{prefix}_{len(temp_files):06d}.parquet')
            merged_df.to_parquet(temp_file, compression='snappy', engine='pyarrow')
            temp_files.append(temp_file)
            del merged_df  # ë©”ëª¨ë¦¬ í•´ì œ
            
            return temp_files
        
        for batch_idx, batch in enumerate(parquet_file.iter_batches(batch_size=chunk_size)):
            batch_df = batch.to_pandas()
            batch_size = len(batch_df)
            batch_start = current_idx
            batch_end = current_idx + batch_size
            
            # ì‹œê°„ ìˆœì„œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì–´ì•¼ í•˜ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
            batch_df = batch_df.sort_values('timestamp').reset_index(drop=True)
            
            # ë¶„í• 
            if batch_end <= train_end_idx:
                # Train ì˜ì—­
                if batch_start < valid_start_idx:
                    # Train (valid ì œì™¸)
                    train_chunks.append(batch_df)
                    train_count += batch_size
                    
                    # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ íŒŒì¼ë¡œ ì €ì¥
                    if len(train_chunks) >= merge_chunk_count:
                        train_temp_files = save_chunks_to_file(train_chunks, train_temp_files, 'train')
                else:
                    # Valid ì˜ì—­
                    valid_chunks.append(batch_df)
                    valid_count += batch_size
                    
                    # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ íŒŒì¼ë¡œ ì €ì¥
                    if len(valid_chunks) >= merge_chunk_count:
                        valid_temp_files = save_chunks_to_file(valid_chunks, valid_temp_files, 'valid')
            else:
                # Test ì˜ì—­
                if batch_start < train_end_idx:
                    # ì¼ë¶€ëŠ” train/valid, ì¼ë¶€ëŠ” test
                    split_in_batch = train_end_idx - batch_start
                    train_valid_part = batch_df.iloc[:split_in_batch]
                    test_part = batch_df.iloc[split_in_batch:]
                    
                    # Train/Valid ë¶„í• 
                    if batch_start < valid_start_idx:
                        valid_split = valid_start_idx - batch_start
                        train_part = train_valid_part.iloc[:valid_split]
                        valid_part = train_valid_part.iloc[valid_split:]
                        train_chunks.append(train_part)
                        valid_chunks.append(valid_part)
                        train_count += len(train_part)
                        valid_count += len(valid_part)
                        
                        # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ íŒŒì¼ë¡œ ì €ì¥
                        if len(train_chunks) >= merge_chunk_count:
                            train_temp_files = save_chunks_to_file(train_chunks, train_temp_files, 'train')
                        if len(valid_chunks) >= merge_chunk_count:
                            valid_temp_files = save_chunks_to_file(valid_chunks, valid_temp_files, 'valid')
                    else:
                        valid_chunks.append(train_valid_part)
                        valid_count += len(train_valid_part)
                        
                        if len(valid_chunks) >= merge_chunk_count:
                            valid_temp_files = save_chunks_to_file(valid_chunks, valid_temp_files, 'valid')
                    
                    test_chunks.append(test_part)
                    test_count += len(test_part)
                    
                    if len(test_chunks) >= merge_chunk_count:
                        test_temp_files = save_chunks_to_file(test_chunks, test_temp_files, 'test')
                else:
                    # ì „ì²´ Test
                    test_chunks.append(batch_df)
                    test_count += batch_size
                    
                    if len(test_chunks) >= merge_chunk_count:
                        test_temp_files = save_chunks_to_file(test_chunks, test_temp_files, 'test')
            
            current_idx = batch_end
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (batch_idx + 1) % 10 == 0:
                print(f"   - {batch_idx + 1}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({current_idx:,}/{total_rows:,}ê°œ í–‰, {current_idx/total_rows*100:.1f}%)")
                print(f"      ë©”ëª¨ë¦¬: Train ì²­í¬ {len(train_chunks)}ê°œ, Valid ì²­í¬ {len(valid_chunks)}ê°œ, Test ì²­í¬ {len(test_chunks)}ê°œ")
                print(f"      ì €ì¥ëœ ì„ì‹œ íŒŒì¼: Train {len(train_temp_files)}ê°œ, Valid {len(valid_temp_files)}ê°œ, Test {len(test_temp_files)}ê°œ")
        
        print(f"\nğŸ“ ë‚¨ì€ ì²­í¬ ì €ì¥ ë° ìµœì¢… ë³‘í•© ì¤‘...")
        
        # ë‚¨ì€ ì²­í¬ë“¤ë„ íŒŒì¼ë¡œ ì €ì¥
        if train_chunks:
            train_temp_files = save_chunks_to_file(train_chunks, train_temp_files, 'train')
        if valid_chunks:
            valid_temp_files = save_chunks_to_file(valid_chunks, valid_temp_files, 'valid')
        if test_chunks:
            test_temp_files = save_chunks_to_file(test_chunks, test_temp_files, 'test')
        
        # ê° ë¶„í• ì˜ ì„ì‹œ íŒŒì¼ë“¤ì„ ìµœì¢… íŒŒì¼ë¡œ ë³‘í•© (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
        output_files = {}
        
        def merge_temp_files(temp_files, output_file, split_name):
            """ì„ì‹œ íŒŒì¼ë“¤ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³‘í•©"""
            if not temp_files:
                return None
            
            print(f"   ğŸ“¦ {split_name} ë³‘í•© ì¤‘... ({len(temp_files)}ê°œ ì„ì‹œ íŒŒì¼)")
            
            # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³‘í•© (ë©”ëª¨ë¦¬ ì ˆì•½)
            result_dfs = []
            for i, temp_file in enumerate(temp_files):
                temp_df = pd.read_parquet(temp_file, engine='pyarrow')
                result_dfs.append(temp_df)
                
                # ì¼ì • ê°œìˆ˜ë§ˆë‹¤ ë³‘í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                if len(result_dfs) >= 5:
                    merged = pd.concat(result_dfs, ignore_index=True)
                    result_dfs = [merged]
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_file):
                    os.remove(temp_file)
            
            # ìµœì¢… ë³‘í•©
            if result_dfs:
                final_df = pd.concat(result_dfs, ignore_index=True)
                final_df = final_df.sort_values('timestamp').reset_index(drop=True)
                final_df.to_parquet(output_file, compression='snappy', engine='pyarrow')
                del final_df, result_dfs
                return output_file
            return None
        
        # Train ë³‘í•©
        train_file = os.path.join(output_dir, 'train.parquet')
        if merge_temp_files(train_temp_files, train_file, 'Train'):
            output_files['train'] = train_file
            print(f"   âœ… Train ì €ì¥: {train_count:,}ê°œ ({train_count/total_rows*100:.1f}%)")
        
        # Valid ë³‘í•©
        valid_file = os.path.join(output_dir, 'valid.parquet')
        if merge_temp_files(valid_temp_files, valid_file, 'Valid'):
            output_files['valid'] = valid_file
            print(f"   âœ… Valid ì €ì¥: {valid_count:,}ê°œ ({valid_count/total_rows*100:.1f}%)")
        
        # Test ë³‘í•©
        test_file = os.path.join(output_dir, 'test.parquet')
        if merge_temp_files(test_temp_files, test_file, 'Test'):
            output_files['test'] = test_file
            print(f"   âœ… Test ì €ì¥: {test_count:,}ê°œ ({test_count/total_rows*100:.1f}%)")
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
        try:
            if os.path.exists(temp_dir) and not os.listdir(temp_dir):
                os.rmdir(temp_dir)
        except:
            pass
        
        print(f"\nâœ… ìŠ¤íŠ¸ë¦¬ë° ë¶„í•  ì™„ë£Œ!")
        print(f"   ì €ì¥ ìœ„ì¹˜: {output_dir}")
        
        return output_files


class LogAnomalyDetector:
    """ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.baseline_stats = {}
        self.scaler = StandardScaler()
        self.models = {}
        
    def extract_features(self, df):
        """ë¡œê·¸ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if df.empty:
            return pd.DataFrame()
        
        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        df['hour'] = df['timestamp'].dt.hour
        df['minute'] = df['timestamp'].dt.minute
        df['date'] = df['timestamp'].dt.date
        
        # ì‹œê°„ ìœˆë„ìš°ë³„ ì§‘ê³„ (10ë¶„ ë‹¨ìœ„)
        df['time_window'] = df['timestamp'].dt.floor('10T')
        
        # ì§‘ê³„
        features = []
        for window in df['time_window'].unique():
            window_df = df[df['time_window'] == window]
            
            feature = {
                'time_window': window,
                'total_logs': len(window_df),
                'error_count': window_df['is_error'].sum(),
                'warn_count': (window_df['level'] == 'WARN').sum(),
                'error_rate': window_df['is_error'].mean(),
                'warn_rate': (window_df['level'] == 'WARN').mean(),
                'unique_classes': window_df['class_path'].nunique(),
                'unique_threads': window_df['thread'].nunique(),
                'avg_message_length': window_df['message_length'].mean(),
                'exception_count': window_df['has_exception'].sum(),
                'exception_rate': window_df['has_exception'].mean(),
                'unique_files': window_df['file_path'].nunique(),
            }
            
            # ë ˆë²¨ë³„ ì¹´ìš´íŠ¸
            level_counts = window_df['level'].value_counts()
            for level in ['ERROR', 'WARN', 'INFO', 'DEBUG']:
                feature[f'{level.lower()}_count'] = level_counts.get(level, 0)
            
            # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í´ë˜ìŠ¤
            top_class = window_df['class_path'].value_counts().head(1)
            if not top_class.empty:
                feature['top_class'] = top_class.index[0]
                feature['top_class_count'] = top_class.values[0]
            else:
                feature['top_class'] = ''
                feature['top_class_count'] = 0
            
            features.append(feature)
        
        features_df = pd.DataFrame(features)
        
        # í´ë˜ìŠ¤ ê²½ë¡œë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹œ)
        if 'top_class' in features_df.columns:
            features_df['top_class_hash'] = features_df['top_class'].apply(
                lambda x: hash(x) % 1000 if x else 0
            )
        
        return features_df
    
    def calculate_baseline(self, features_df):
        """ê¸°ì¤€ì„  í†µê³„ ê³„ì‚°"""
        if features_df.empty:
            return {}
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        baseline = {}
        
        for col in numeric_cols:
            baseline[f'{col}_mean'] = features_df[col].mean()
            baseline[f'{col}_std'] = features_df[col].std()
            baseline[f'{col}_median'] = features_df[col].median()
            baseline[f'{col}_q25'] = features_df[col].quantile(0.25)
            baseline[f'{col}_q75'] = features_df[col].quantile(0.75)
            baseline[f'{col}_q95'] = features_df[col].quantile(0.95)
        
        return baseline
    
    def detect_statistical_anomalies(self, features_df, threshold=3.0):
        """í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (Z-score ê¸°ë°˜)"""
        if features_df.empty or not self.baseline_stats:
            return pd.DataFrame()
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        anomalies = []
        
        for idx, row in features_df.iterrows():
            anomaly_score = 0
            reasons = []
            
            for col in numeric_cols:
                mean_key = f'{col}_mean'
                std_key = f'{col}_std'
                
                if mean_key in self.baseline_stats and std_key in self.baseline_stats:
                    mean_val = self.baseline_stats[mean_key]
                    std_val = self.baseline_stats[std_key]
                    
                    if std_val > 0:
                        z_score = abs((row[col] - mean_val) / std_val)
                        if z_score > threshold:
                            anomaly_score += z_score
                            reasons.append(f"{col}: Z-score={z_score:.2f}")
            
            if anomaly_score > 0:
                anomalies.append({
                    'time_window': row['time_window'],
                    'anomaly_score': anomaly_score,
                    'reasons': '; '.join(reasons),
                    'features': row.to_dict()
                })
        
        return pd.DataFrame(anomalies)
    
    def detect_error_spikes(self, features_df, threshold_multiplier=5.0):
        """ì—ëŸ¬ ê¸‰ì¦ íƒì§€"""
        if features_df.empty or not self.baseline_stats:
            return pd.DataFrame()
        
        baseline_error_rate = self.baseline_stats.get('error_rate_mean', 0)
        baseline_error_std = self.baseline_stats.get('error_rate_std', 0)
        
        if baseline_error_rate == 0:
            baseline_error_rate = 0.01  # ìµœì†Œê°’
        
        spikes = []
        for idx, row in features_df.iterrows():
            current_error_rate = row['error_rate']
            
            if current_error_rate > baseline_error_rate * threshold_multiplier:
                spikes.append({
                    'time_window': row['time_window'],
                    'baseline_error_rate': baseline_error_rate,
                    'current_error_rate': current_error_rate,
                    'multiplier': current_error_rate / baseline_error_rate,
                    'error_count': row['error_count'],
                    'total_logs': row['total_logs']
                })
        
        return pd.DataFrame(spikes)
    
    def detect_unusual_patterns(self, df):
        """ë¹„ì •ìƒì ì¸ íŒ¨í„´ íƒì§€"""
        anomalies = []
        
        # 1. íŠ¹ì • í´ë˜ìŠ¤ì—ì„œ ì—ëŸ¬ ì§‘ì¤‘
        error_by_class = df[df['is_error']].groupby('class_path').size()
        if not error_by_class.empty:
            top_error_class = error_by_class.idxmax()
            error_count = error_by_class.max()
            total_errors = error_by_class.sum()
            
            if error_count > total_errors * 0.5:  # ì „ì²´ ì—ëŸ¬ì˜ 50% ì´ìƒì´ í•œ í´ë˜ìŠ¤ì—ì„œ
                anomalies.append({
                    'type': 'error_concentration',
                    'class': top_error_class,
                    'error_count': error_count,
                    'total_errors': total_errors,
                    'percentage': (error_count / total_errors) * 100
                })
        
        # 2. ë¡œê·¸ ë¹ˆë„ ì´ìƒ (ë„ˆë¬´ ë§ê±°ë‚˜ ì ìŒ)
        if 'time_window' in df.columns:
            log_frequency = df.groupby('time_window').size()
            if not log_frequency.empty:
                mean_freq = log_frequency.mean()
                std_freq = log_frequency.std()
                
                for window, count in log_frequency.items():
                    if std_freq > 0:
                        z_score = abs((count - mean_freq) / std_freq)
                        if z_score > 3:
                            anomalies.append({
                                'type': 'frequency_anomaly',
                                'time_window': window,
                                'log_count': count,
                                'mean': mean_freq,
                                'z_score': z_score
                            })
        
        # 3. ìƒˆë¡œìš´ ì˜ˆì™¸ íƒ€ì… íƒì§€
        exception_patterns = df[df['has_exception']]['message'].apply(
            lambda x: re.search(r'(\w+Exception|\w+Error)', x)
        )
        exception_types = exception_patterns.dropna().apply(lambda x: x.group(1))
        
        if not exception_types.empty:
            exception_counts = exception_types.value_counts()
            # ì „ì²´ì˜ 1% ë¯¸ë§Œì´ë©´ ìƒˆë¡œìš´ ì˜ˆì™¸ë¡œ ê°„ì£¼
            total_exceptions = len(exception_types)
            for exc_type, count in exception_counts.items():
                if count < total_exceptions * 0.01 and count > 0:
                    anomalies.append({
                        'type': 'new_exception_type',
                        'exception_type': exc_type,
                        'count': count,
                        'percentage': (count / total_exceptions) * 100
                    })
        
        return pd.DataFrame(anomalies)
    
    def train_ml_model(self, features_df, model_type='isolation_forest'):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ"""
        if features_df.empty:
            return None
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        X = features_df[numeric_cols].fillna(0)
        
        # ì •ê·œí™”
        X_scaled = self.scaler.fit_transform(X)
        
        # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
        if model_type == 'isolation_forest':
            model = IForest(contamination=0.1, random_state=42)
        elif model_type == 'autoencoder':
            # AutoEncoderëŠ” ë°ì´í„° í¬ê¸°ì— ë”°ë¼ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”
            n_samples, n_features = X_scaled.shape
            
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ AutoEncoder ì‚¬ìš© ë¶ˆê°€
            if n_samples < 10:
                print(f"âš ï¸ AutoEncoder í•™ìŠµ ì‹¤íŒ¨: ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({n_samples}ê°œ ìƒ˜í”Œ)")
                print(f"   ìµœì†Œ 10ê°œ ì´ìƒì˜ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return None
            
            # íŠ¹ì§• ìˆ˜ì— ë”°ë¼ hidden layer í¬ê¸° ì¡°ì •
            if n_features <= 5:
                hidden_neurons = [max(4, n_features), max(2, n_features//2), max(4, n_features)]
            elif n_features <= 10:
                hidden_neurons = [16, 8, 16]
            else:
                hidden_neurons = [64, 32, 16, 32, 64]
            
            # ìƒ˜í”Œ ìˆ˜ì— ë”°ë¼ epochì™€ batch_size ì¡°ì •
            if n_samples < 50:
                epoch_num = 20
                batch_size = min(8, n_samples)
            elif n_samples < 100:
                epoch_num = 30
                batch_size = 16
            else:
                epoch_num = 50
                batch_size = 32
            
            try:
                model = AutoEncoder(
                    contamination=0.1,
                    hidden_neurons=hidden_neurons,
                    epochs=epoch_num,
                    batch_size=batch_size,
                    dropout_rate=0.2,
                    verbose=0,  # ì§„í–‰ ìƒí™© ì¶œë ¥ ë¹„í™œì„±í™”
                    random_state=42
                )
            except TypeError:
                # íŒŒë¼ë¯¸í„° ì´ë¦„ì´ ë‹¤ë¥¸ ë²„ì „ì˜ pyodì¼ ìˆ˜ ìˆìŒ
                try:
                    model = AutoEncoder(
                        contamination=0.1,
                        hidden_neuron_list=hidden_neurons,
                        epoch_num=epoch_num,
                        batch_size=batch_size,
                        dropout_rate=0.2,
                        verbose=0,
                        random_state=42
                    )
                except Exception as e:
                    print(f"âš ï¸ AutoEncoder ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    return None
        elif model_type == 'lof':
            model = LOF(contamination=0.1)
        else:
            model = IForest(contamination=0.1, random_state=42)
        
        try:
            model.fit(X_scaled)
            self.models[model_type] = model
            return model
        except Exception as e:
            print(f"âš ï¸ {model_type} ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
            print(f"   ë°ì´í„° í¬ê¸°: {X_scaled.shape}")
            print(f"   ëª¨ë¸ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
    
    def predict_anomalies_ml(self, features_df, model_type='isolation_forest'):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì´ìƒì¹˜ ì˜ˆì¸¡"""
        if features_df.empty or model_type not in self.models:
            return pd.DataFrame()
        
        numeric_cols = features_df.select_dtypes(include=[np.number]).columns
        X = features_df[numeric_cols].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        model = self.models[model_type]
        predictions = model.predict(X_scaled)
        scores = model.decision_function(X_scaled)
        
        anomalies = features_df[predictions == 1].copy()
        anomalies['anomaly_score'] = -scores[predictions == 1]  # ìŒìˆ˜ ì ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œ
        
        return anomalies


class LogAnomalyDetectionSystem:
    """í†µí•© ë¡œê·¸ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_directory, max_files=None, sample_lines=None):
        self.log_directory = log_directory
        self.max_files = max_files
        self.sample_lines = sample_lines
        self.parser = SpringBootLogParser()
        self.detector = LogAnomalyDetector()
        self.logs_df = None
        self.features_df = None
        
    def load_logs(self):
        """ë¡œê·¸ íŒŒì¼ ë¡œë“œ ë° íŒŒì‹±"""
        print("=" * 60)
        print("ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘...")
        print("=" * 60)
        
        self.logs_df = self.parser.parse_directory(
            self.log_directory, 
            max_files=self.max_files,
            sample_lines=self.sample_lines
        )
        
        if self.logs_df.empty:
            print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"\nâœ… ì´ {len(self.logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        print(f"   - ê¸°ê°„: {self.logs_df['timestamp'].min()} ~ {self.logs_df['timestamp'].max()}")
        print(f"   - ì—ëŸ¬ ë¡œê·¸: {self.logs_df['is_error'].sum()}ê°œ")
        print(f"   - ê²½ê³  ë¡œê·¸: {(self.logs_df['level'] == 'WARN').sum()}ê°œ")
        
        return True
    
    def extract_features(self):
        """íŠ¹ì§• ì¶”ì¶œ"""
        print("\n" + "=" * 60)
        print("íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        print("=" * 60)
        
        self.features_df = self.detector.extract_features(self.logs_df)
        
        if self.features_df.empty:
            print("âš ï¸ ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"âœ… {len(self.features_df)}ê°œ ì‹œê°„ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        print(f"\níŠ¹ì§• í†µê³„:")
        print(self.features_df.describe())
        
        return True
    
    def train_baseline(self, train_ratio=0.8, validation_ratio=0.1):
        """
        ê¸°ì¤€ì„  í•™ìŠµ (80% í•™ìŠµ, 10% ê²€ì¦, 10% í…ŒìŠ¤íŠ¸)
        
        Args:
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.8 = 80%)
            validation_ratio: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ 0.1 = 10%, ë‚˜ë¨¸ì§€ 10%ëŠ” í…ŒìŠ¤íŠ¸)
        """
        print("\n" + "=" * 60)
        print("ê¸°ì¤€ì„  í•™ìŠµ ì¤‘...")
        print("=" * 60)
        
        if self.features_df.empty:
            print("âš ï¸ íŠ¹ì§• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ë¶„í• 
        self.features_df = self.features_df.sort_values('time_window')
        total_samples = len(self.features_df)
        
        # 80% í•™ìŠµ, 10% ê²€ì¦, 10% í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• 
        train_end = int(total_samples * train_ratio)
        val_end = train_end + int(total_samples * validation_ratio)
        
        train_df = self.features_df.iloc[:train_end]
        val_df = self.features_df.iloc[train_end:val_end]
        test_df = self.features_df.iloc[val_end:]
        
        # ê¸°ì¤€ì„  í†µê³„ ê³„ì‚° (í•™ìŠµ ë°ì´í„°ë§Œ ì‚¬ìš©)
        self.detector.baseline_stats = self.detector.calculate_baseline(train_df)
        
        print("âœ… ê¸°ì¤€ì„  í†µê³„ ê³„ì‚° ì™„ë£Œ")
        print(f"   - ì „ì²´ ë°ì´í„°: {total_samples}ê°œ ìœˆë„ìš°")
        print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ ìœˆë„ìš° ({len(train_df)/total_samples*100:.1f}%)")
        print(f"   - ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ ìœˆë„ìš° ({len(val_df)/total_samples*100:.1f}%)")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ ìœˆë„ìš° ({len(test_df)/total_samples*100:.1f}%)")
        
        # ML ëª¨ë¸ í•™ìŠµ
        print("\në¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        models_trained = {}
        
        if self.detector.train_ml_model(train_df, model_type='isolation_forest'):
            print("   âœ… Isolation Forest í•™ìŠµ ì™„ë£Œ")
            models_trained['isolation_forest'] = True
        else:
            print("   âš ï¸ Isolation Forest í•™ìŠµ ì‹¤íŒ¨")
            models_trained['isolation_forest'] = False
        
        if self.detector.train_ml_model(train_df, model_type='autoencoder'):
            print("   âœ… AutoEncoder í•™ìŠµ ì™„ë£Œ")
            models_trained['autoencoder'] = True
        else:
            print("   âš ï¸ AutoEncoder í•™ìŠµ ì‹¤íŒ¨ (ê±´ë„ˆëœ€)")
            models_trained['autoencoder'] = False
        
        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
        print("\n" + "=" * 60)
        print("ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 60)
        
        validation_results = self._evaluate_models(val_df, models_trained)
        self._print_validation_results(validation_results)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ (ë‚˜ì¤‘ì— ì‚¬ìš©)
        self.test_df = test_df
        
        return True
    
    def _evaluate_models(self, val_df, models_trained):
        """ê²€ì¦ ë°ì´í„°ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        results = {}
        
        # ì •ìƒ/ì´ìƒ ë¼ë²¨ ìƒì„± (ì—ëŸ¬ìœ¨ ê¸°ì¤€)
        baseline_error_rate = self.detector.baseline_stats.get('error_rate_mean', 0)
        threshold = baseline_error_rate * 2  # ê¸°ì¤€ ì—ëŸ¬ìœ¨ì˜ 2ë°° ì´ìƒì´ë©´ ì´ìƒ
        
        val_df = val_df.copy()
        val_df['true_label'] = (val_df['error_rate'] > threshold).astype(int)
        
        # ê° ëª¨ë¸ë³„ í‰ê°€
        for model_type, is_trained in models_trained.items():
            if not is_trained or model_type not in self.detector.models:
                continue
            
            try:
                # ì˜ˆì¸¡
                predictions = self.detector.predict_anomalies_ml(val_df, model_type=model_type)
                
                if predictions.empty:
                    continue
                
                # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„± (ì´ìƒì¹˜ë¡œ íƒì§€ëœ ê²ƒ)
                val_df['pred_label'] = 0
                val_df.loc[predictions.index, 'pred_label'] = 1
                
                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                true_labels = val_df['true_label'].values
                pred_labels = val_df['pred_label'].values
                
                # ë¼ë²¨ì´ ëª¨ë‘ ê°™ìœ¼ë©´ í‰ê°€ ë¶ˆê°€
                if len(set(true_labels)) == 1 and len(set(pred_labels)) == 1:
                    continue
                
                accuracy = accuracy_score(true_labels, pred_labels)
                precision = precision_score(true_labels, pred_labels, zero_division=0)
                recall = recall_score(true_labels, pred_labels, zero_division=0)
                f1 = f1_score(true_labels, pred_labels, zero_division=0)
                cm = confusion_matrix(true_labels, pred_labels)
                
                results[model_type] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1_score': f1,
                    'confusion_matrix': cm,
                    'true_anomalies': int(true_labels.sum()),
                    'predicted_anomalies': int(pred_labels.sum())
                }
            except Exception as e:
                print(f"   âš ï¸ {model_type} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        return results
    
    def _print_validation_results(self, validation_results):
        """ê²€ì¦ ê²°ê³¼ ì¶œë ¥"""
        if not validation_results:
            print("âš ï¸ í‰ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        for model_type, metrics in validation_results.items():
            print(f"\nğŸ“Š {model_type.upper()} ëª¨ë¸ ì„±ëŠ¥:")
            print(f"   ì •í™•ë„ (Accuracy): {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            print(f"   ì •ë°€ë„ (Precision): {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
            print(f"   ì¬í˜„ìœ¨ (Recall): {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
            print(f"   F1 ì ìˆ˜: {metrics['f1_score']:.4f}")
            print(f"   ì‹¤ì œ ì´ìƒì¹˜: {metrics['true_anomalies']}ê°œ")
            print(f"   ì˜ˆì¸¡ ì´ìƒì¹˜: {metrics['predicted_anomalies']}ê°œ")
            
            cm = metrics['confusion_matrix']
            print(f"   í˜¼ë™ í–‰ë ¬:")
            print(f"      ì •ìƒâ†’ì •ìƒ: {cm[0][0]}, ì •ìƒâ†’ì´ìƒ: {cm[0][1]}")
            print(f"      ì´ìƒâ†’ì •ìƒ: {cm[1][0]}, ì´ìƒâ†’ì´ìƒ: {cm[1][1]}")
    
    def detect_all_anomalies(self, use_test_data=True):
        """
        ëª¨ë“  ì´ìƒì¹˜ íƒì§€
        
        Args:
            use_test_data: Trueë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©, Falseë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©
        """
        print("\n" + "=" * 60)
        print("ì´ìƒì¹˜ íƒì§€ ì¤‘...")
        print("=" * 60)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë§Œ ì‚¬ìš©
        if use_test_data and hasattr(self, 'test_df') and not self.test_df.empty:
            print("ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€ ìˆ˜í–‰")
            test_features_df = self.test_df
        else:
            print("ğŸ“ ì „ì²´ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€ ìˆ˜í–‰")
            test_features_df = self.features_df
        
        results = {}
        
        # 1. í†µê³„ì  ì´ìƒì¹˜
        print("\n1. í†µê³„ì  ì´ìƒì¹˜ íƒì§€...")
        stat_anomalies = self.detector.detect_statistical_anomalies(test_features_df)
        results['statistical'] = stat_anomalies
        print(f"   âœ… {len(stat_anomalies)}ê°œ ì´ìƒì¹˜ ë°œê²¬")
        
        # 2. ì—ëŸ¬ ê¸‰ì¦
        print("\n2. ì—ëŸ¬ ê¸‰ì¦ íƒì§€...")
        error_spikes = self.detector.detect_error_spikes(test_features_df)
        results['error_spikes'] = error_spikes
        print(f"   âœ… {len(error_spikes)}ê°œ ì—ëŸ¬ ê¸‰ì¦ ë°œê²¬")
        
        # 3. ë¹„ì •ìƒ íŒ¨í„´ (ì „ì²´ ë¡œê·¸ ë°ì´í„° ì‚¬ìš©)
        print("\n3. ë¹„ì •ìƒ íŒ¨í„´ íƒì§€...")
        unusual_patterns = self.detector.detect_unusual_patterns(self.logs_df)
        results['unusual_patterns'] = unusual_patterns
        print(f"   âœ… {len(unusual_patterns)}ê°œ ë¹„ì •ìƒ íŒ¨í„´ ë°œê²¬")
        
        # 4. ML ê¸°ë°˜ ì´ìƒì¹˜
        print("\n4. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€...")
        ml_anomalies_if = self.detector.predict_anomalies_ml(
            test_features_df, model_type='isolation_forest'
        )
        results['ml_isolation_forest'] = ml_anomalies_if
        print(f"   âœ… Isolation Forest: {len(ml_anomalies_if)}ê°œ ì´ìƒì¹˜")
        
        # AutoEncoderëŠ” í•™ìŠµì´ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì‹¤í–‰
        if 'autoencoder' in self.detector.models:
            ml_anomalies_ae = self.detector.predict_anomalies_ml(
                test_features_df, model_type='autoencoder'
            )
            results['ml_autoencoder'] = ml_anomalies_ae
            print(f"   âœ… AutoEncoder: {len(ml_anomalies_ae)}ê°œ ì´ìƒì¹˜")
        else:
            results['ml_autoencoder'] = pd.DataFrame()
            print(f"   âš ï¸ AutoEncoder: í•™ìŠµë˜ì§€ ì•Šì•„ ê±´ë„ˆëœ€")
        
        return results
    
    def save_model(self, model_path):
        """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¤€ì„  ì €ì¥"""
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        model_data = {
            'baseline_stats': self.detector.baseline_stats,
            'scaler': self.detector.scaler,
            'models': self.detector.models
        }
        
        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    
    def load_model(self, model_path):
        """ì €ì¥ëœ ëª¨ë¸ê³¼ ê¸°ì¤€ì„  ë¡œë“œ"""
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.detector.baseline_stats = model_data['baseline_stats']
        self.detector.scaler = model_data['scaler']
        self.detector.models = model_data['models']
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    def detect_anomalies_on_new_data(self, new_log_directory, max_files=None, sample_lines=None):
        """
        ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„°ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€
        
        Args:
            new_log_directory: ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
            max_files: ìµœëŒ€ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
            sample_lines: íŒŒì¼ë‹¹ ìµœëŒ€ ì²˜ë¦¬í•  ë¼ì¸ ìˆ˜
        
        Returns:
            dict: ì´ìƒì¹˜ íƒì§€ ê²°ê³¼
        """
        print("=" * 60)
        print("ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„° ì´ìƒì¹˜ íƒì§€")
        print("=" * 60)
        
        # ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì‹±
        print("\nìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘...")
        new_logs_df = self.parser.parse_directory(
            new_log_directory,
            max_files=max_files,
            sample_lines=sample_lines
        )
        
        if new_logs_df.empty:
            print("âš ï¸ íŒŒì‹±ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(new_logs_df)}ê°œ ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì™„ë£Œ")
        
        # íŠ¹ì§• ì¶”ì¶œ
        print("\níŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        new_features_df = self.detector.extract_features(new_logs_df)
        
        if new_features_df.empty:
            print("âš ï¸ ì¶”ì¶œëœ íŠ¹ì§•ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"âœ… {len(new_features_df)}ê°œ ì‹œê°„ ìœˆë„ìš° íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ")
        
        # ì´ìƒì¹˜ íƒì§€
        print("\nì´ìƒì¹˜ íƒì§€ ì¤‘...")
        results = {}
        
        # 1. í†µê³„ì  ì´ìƒì¹˜
        stat_anomalies = self.detector.detect_statistical_anomalies(new_features_df)
        results['statistical'] = stat_anomalies
        print(f"   âœ… í†µê³„ì  ì´ìƒì¹˜: {len(stat_anomalies)}ê°œ")
        
        # 2. ì—ëŸ¬ ê¸‰ì¦
        error_spikes = self.detector.detect_error_spikes(new_features_df)
        results['error_spikes'] = error_spikes
        print(f"   âœ… ì—ëŸ¬ ê¸‰ì¦: {len(error_spikes)}ê°œ")
        
        # 3. ë¹„ì •ìƒ íŒ¨í„´
        unusual_patterns = self.detector.detect_unusual_patterns(new_logs_df)
        results['unusual_patterns'] = unusual_patterns
        print(f"   âœ… ë¹„ì •ìƒ íŒ¨í„´: {len(unusual_patterns)}ê°œ")
        
        # 4. ML ê¸°ë°˜ ì´ìƒì¹˜
        if 'isolation_forest' in self.detector.models:
            ml_anomalies_if = self.detector.predict_anomalies_ml(
                new_features_df, model_type='isolation_forest'
            )
            results['ml_isolation_forest'] = ml_anomalies_if
            print(f"   âœ… ML ì´ìƒì¹˜ (IF): {len(ml_anomalies_if)}ê°œ")
        else:
            results['ml_isolation_forest'] = pd.DataFrame()
            print(f"   âš ï¸ ML ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•ŠìŒ")
        
        if 'autoencoder' in self.detector.models:
            ml_anomalies_ae = self.detector.predict_anomalies_ml(
                new_features_df, model_type='autoencoder'
            )
            results['ml_autoencoder'] = ml_anomalies_ae
            print(f"   âœ… ML ì´ìƒì¹˜ (AE): {len(ml_anomalies_ae)}ê°œ")
        else:
            results['ml_autoencoder'] = pd.DataFrame()
        
        return results
    
    def generate_report(self, results):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("=" * 60)
        
        # í†µê³„ì  ì´ìƒì¹˜
        if not results['statistical'].empty:
            print("\nğŸ“Š í†µê³„ì  ì´ìƒì¹˜:")
            for idx, row in results['statistical'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ì´ìƒ ì ìˆ˜: {row['anomaly_score']:.2f}")
                print(f"   ì´ìœ : {row['reasons']}")
                print()
        
        # ì—ëŸ¬ ê¸‰ì¦
        if not results['error_spikes'].empty:
            print("\nğŸš¨ ì—ëŸ¬ ê¸‰ì¦:")
            for idx, row in results['error_spikes'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ê¸°ì¤€ ì—ëŸ¬ìœ¨: {row['baseline_error_rate']:.2%}")
                print(f"   í˜„ì¬ ì—ëŸ¬ìœ¨: {row['current_error_rate']:.2%}")
                print(f"   ë°°ìˆ˜: {row['multiplier']:.1f}ë°°")
                print(f"   ì—ëŸ¬ ìˆ˜: {row['error_count']}ê°œ / ì´ {row['total_logs']}ê°œ")
                print()
        
        # ë¹„ì •ìƒ íŒ¨í„´
        if not results['unusual_patterns'].empty:
            print("\nâš ï¸ ë¹„ì •ìƒ íŒ¨í„´:")
            for idx, row in results['unusual_patterns'].iterrows():
                if row['type'] == 'error_concentration':
                    print(f"   ì—ëŸ¬ ì§‘ì¤‘: {row['class']}ì—ì„œ {row['error_count']}ê°œ ({row['percentage']:.1f}%)")
                elif row['type'] == 'frequency_anomaly':
                    print(f"   ë¡œê·¸ ë¹ˆë„ ì´ìƒ: {row['time_window']} (Z-score: {row['z_score']:.2f})")
                elif row['type'] == 'new_exception_type':
                    print(f"   ìƒˆë¡œìš´ ì˜ˆì™¸: {row['exception_type']} ({row['count']}íšŒ)")
                print()
        
        # ML ê¸°ë°˜ ì´ìƒì¹˜
        if not results['ml_isolation_forest'].empty:
            print("\nğŸ¤– ML ê¸°ë°˜ ì´ìƒì¹˜ (Isolation Forest):")
            for idx, row in results['ml_isolation_forest'].head(10).iterrows():
                print(f"   ì‹œê°„: {row['time_window']}")
                print(f"   ì´ìƒ ì ìˆ˜: {row['anomaly_score']:.2f}")
                print(f"   ì—ëŸ¬ ìˆ˜: {row['error_count']}ê°œ")
                print()
        
        # ìš”ì•½
        print("\n" + "=" * 60)
        print("ìš”ì•½")
        print("=" * 60)
        print(f"í†µê³„ì  ì´ìƒì¹˜: {len(results['statistical'])}ê°œ")
        print(f"ì—ëŸ¬ ê¸‰ì¦: {len(results['error_spikes'])}ê°œ")
        print(f"ë¹„ì •ìƒ íŒ¨í„´: {len(results['unusual_patterns'])}ê°œ")
        print(f"ML ì´ìƒì¹˜ (IF): {len(results['ml_isolation_forest'])}ê°œ")
        print(f"ML ì´ìƒì¹˜ (AE): {len(results['ml_autoencoder'])}ê°œ")
        
        return results


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    # ìƒ˜í”Œë§ ì˜µì…˜ (ì „ì²´ ë¶„ì„ì„ ì›í•˜ë©´ Noneìœ¼ë¡œ ì„¤ì •)
    MAX_FILES = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´: 5ê°œë§Œ ì²˜ë¦¬)
    SAMPLE_LINES = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì „ì²´ ë¼ì¸ ì²˜ë¦¬ (ê¸°ì¡´: 10000ì¤„ë§Œ ì²˜ë¦¬)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = LogAnomalyDetectionSystem(
        log_directory,
        max_files=MAX_FILES,
        sample_lines=SAMPLE_LINES
    )
    
    # ë¡œê·¸ ë¡œë“œ
    if not system.load_logs():
        return
    
    # íŠ¹ì§• ì¶”ì¶œ
    if not system.extract_features():
        return
    
    # ê¸°ì¤€ì„  í•™ìŠµ
    if not system.train_baseline():
        return
    
    # ì´ìƒì¹˜ íƒì§€ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©)
    results = system.detect_all_anomalies(use_test_data=True)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    system.generate_report(results)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results"
    os.makedirs(output_dir, exist_ok=True)
    
    for name, df in results.items():
        if not df.empty:
            output_path = os.path.join(output_dir, f"anomalies_{name}.csv")
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ëª¨ë¸ ì €ì¥
    model_path = os.path.join(output_dir, "trained_model.pkl")
    system.save_model(model_path)


def test_new_logs():
    """ìƒˆë¡œìš´ ë¡œê·¸ íŒŒì¼ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
    # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    model_path = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results/trained_model.pkl"
    
    # ìƒˆë¡œìš´ ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì˜ˆì‹œ)
    new_log_directory = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/logs/backup"
    
    # ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¨¼ì € í•™ìŠµ í•„ìš”
    if not os.path.exists(model_path):
        print("âš ï¸ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € main()ì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”.")
        return
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = LogAnomalyDetectionSystem(new_log_directory)
    
    # ëª¨ë¸ ë¡œë“œ
    system.load_model(model_path)
    
    # ìƒˆë¡œìš´ ë¡œê·¸ ë°ì´í„°ë¡œ ì´ìƒì¹˜ íƒì§€
    # ì˜ˆ: ìµœê·¼ 3ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
    results = system.detect_anomalies_on_new_data(
        new_log_directory,
        max_files=3,  # ì²˜ìŒ 3ê°œ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
        sample_lines=5000  # íŒŒì¼ë‹¹ 5000ì¤„ë§Œ ì²˜ë¦¬
    )
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    system.generate_report(results)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = "/Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog/results"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    for name, df in results.items():
        if not df.empty:
            output_path = os.path.join(output_dir, f"test_anomalies_{name}_{timestamp}.csv")
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒˆë¡œìš´ ë¡œê·¸ì— ëŒ€í•´ ì´ìƒì¹˜ íƒì§€
        test_new_logs()
    else:
        # í•™ìŠµ ëª¨ë“œ: ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        main()

