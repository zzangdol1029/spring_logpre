"""
ì•™ìƒë¸” ì´ìƒì¹˜ íƒì§€ í´ë˜ìŠ¤
ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ ì´ìƒì¹˜ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings('ignore')


class EnsembleAnomalyDetector:
    """ì•™ìƒë¸” ì´ìƒì¹˜ íƒì§€ í´ë˜ìŠ¤"""
    
    def __init__(self, models_dict: Dict, method: str = 'majority', weights: Optional[Dict] = None):
        """
        Args:
            models_dict: {ëª¨ë¸ëª…: ëª¨ë¸ê°ì²´} ë”•ì…”ë„ˆë¦¬
            method: ì•™ìƒë¸” ë°©ë²• ('majority', 'weighted', 'stacking', 'max', 'min')
            weights: ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬ (weighted ë°©ë²• ì‚¬ìš© ì‹œ)
        """
        self.models = models_dict
        self.method = method
        self.weights = weights or {}
        self.meta_model = None
        
        # ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë©´ ê· ë“± ê°€ì¤‘ì¹˜
        if method == 'weighted' and not self.weights:
            n_models = len(self.models)
            self.weights = {name: 1.0 / n_models for name in self.models.keys()}
    
    def predict(self, X):
        """
        ì•™ìƒë¸” ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
        
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼ (0=ì •ìƒ, 1=ì´ìƒ)
        """
        predictions = {}
        scores = {}
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
        for name, model in self.models.items():
            try:
                predictions[name] = model.predict(X)
                scores[name] = model.decision_function(X)
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ {name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                continue
        
        if not predictions:
            raise ValueError("ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ì•™ìƒë¸” ë°©ë²•ì— ë”°ë¼ ê²°í•©
        if self.method == 'majority':
            return self._majority_vote(predictions)
        elif self.method == 'weighted':
            return self._weighted_vote(scores)
        elif self.method == 'stacking':
            if self.meta_model is None:
                raise ValueError("ìŠ¤íƒœí‚¹ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € fit_meta_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")
            return self._stacking_predict(X, scores)
        elif self.method == 'max':
            return self._max_vote(predictions)
        elif self.method == 'min':
            return self._min_vote(predictions)
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì•™ìƒë¸” ë°©ë²•: {self.method}")
    
    def decision_function(self, X):
        """
        ì•™ìƒë¸” ì´ìƒ ì ìˆ˜
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
        
        Returns:
            ì´ìƒ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì´ìƒ)
        """
        scores = {}
        
        # ê° ëª¨ë¸ì˜ ì ìˆ˜ ìˆ˜ì§‘
        for name, model in self.models.items():
            try:
                score = model.decision_function(X)
                # ì ìˆ˜ ì •ê·œí™” (ëª¨ë¸ ê°„ ìŠ¤ì¼€ì¼ í†µì¼)
                if score.min() < 0:
                    score = -score
                score = (score - score.min()) / (score.max() - score.min() + 1e-8)
                scores[name] = score
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ {name} ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        if not scores:
            raise ValueError("ëª¨ë“  ëª¨ë¸ì˜ ì ìˆ˜ ê³„ì‚°ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # ê°€ì¤‘ í‰ê· 
        if self.method == 'weighted':
            total_weight = sum(self.weights.get(name, 1.0) for name in scores.keys())
            ensemble_score = np.zeros(len(scores[list(scores.keys())[0]]))
            
            for name, score in scores.items():
                weight = self.weights.get(name, 1.0) / total_weight
                ensemble_score += weight * score
            
            return ensemble_score
        else:
            # ë‹¨ìˆœ í‰ê· 
            return np.mean(list(scores.values()), axis=0)
    
    def _majority_vote(self, predictions: Dict) -> np.ndarray:
        """
        ë‹¤ìˆ˜ê²° íˆ¬í‘œ
        
        Args:
            predictions: {ëª¨ë¸ëª…: ì˜ˆì¸¡ê²°ê³¼} ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        model_names = list(predictions.keys())
        n_samples = len(predictions[model_names[0]])
        
        ensemble_pred = []
        for i in range(n_samples):
            votes = sum(pred[i] for pred in predictions.values())
            # ê³¼ë°˜ìˆ˜ ì´ìƒì´ë©´ ì´ìƒì¹˜
            threshold = len(model_names) / 2
            ensemble_pred.append(1 if votes >= threshold else 0)
        
        return np.array(ensemble_pred)
    
    def _weighted_vote(self, scores: Dict) -> np.ndarray:
        """
        ê°€ì¤‘ íˆ¬í‘œ
        
        Args:
            scores: {ëª¨ë¸ëª…: ì´ìƒì ìˆ˜} ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        # ì ìˆ˜ ì •ê·œí™”
        normalized_scores = {}
        for name, score in scores.items():
            if score.min() < 0:
                score = -score
            normalized_scores[name] = (score - score.min()) / (score.max() - score.min() + 1e-8)
        
        # ê°€ì¤‘ í‰ê· 
        total_weight = sum(self.weights.get(name, 1.0) for name in scores.keys())
        ensemble_score = np.zeros(len(scores[list(scores.keys())[0]]))
        
        for name, score in normalized_scores.items():
            weight = self.weights.get(name, 1.0) / total_weight
            ensemble_score += weight * score
        
        # ì„ê³„ê°’ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜
        threshold = 0.5
        return (ensemble_score > threshold).astype(int)
    
    def _max_vote(self, predictions: Dict) -> np.ndarray:
        """
        ìµœëŒ€ íˆ¬í‘œ (í•˜ë‚˜ë¼ë„ ì´ìƒì´ë©´ ì´ìƒ)
        
        Args:
            predictions: {ëª¨ë¸ëª…: ì˜ˆì¸¡ê²°ê³¼} ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        model_names = list(predictions.keys())
        n_samples = len(predictions[model_names[0]])
        
        ensemble_pred = []
        for i in range(n_samples):
            # í•˜ë‚˜ë¼ë„ ì´ìƒì´ë©´ ì´ìƒ
            max_vote = max(pred[i] for pred in predictions.values())
            ensemble_pred.append(max_vote)
        
        return np.array(ensemble_pred)
    
    def _min_vote(self, predictions: Dict) -> np.ndarray:
        """
        ìµœì†Œ íˆ¬í‘œ (ëª¨ë‘ ì´ìƒì´ì–´ì•¼ ì´ìƒ)
        
        Args:
            predictions: {ëª¨ë¸ëª…: ì˜ˆì¸¡ê²°ê³¼} ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        model_names = list(predictions.keys())
        n_samples = len(predictions[model_names[0]])
        
        ensemble_pred = []
        for i in range(n_samples):
            # ëª¨ë‘ ì´ìƒì´ì–´ì•¼ ì´ìƒ
            min_vote = min(pred[i] for pred in predictions.values())
            ensemble_pred.append(min_vote)
        
        return np.array(ensemble_pred)
    
    def _stacking_predict(self, X, scores: Dict) -> np.ndarray:
        """
        ìŠ¤íƒœí‚¹ ì˜ˆì¸¡
        
        Args:
            X: ì…ë ¥ ë°ì´í„°
            scores: {ëª¨ë¸ëª…: ì´ìƒì ìˆ˜} ë”•ì…”ë„ˆë¦¬
        
        Returns:
            ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        # ì ìˆ˜ë¥¼ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜
        features = np.column_stack(list(scores.values()))
        return self.meta_model.predict(features)
    
    def fit_meta_model(self, X_train, y_train):
        """
        ë©”íƒ€ ëª¨ë¸ í•™ìŠµ (ìŠ¤íƒœí‚¹ìš©)
        
        Args:
            X_train: í•™ìŠµ ë°ì´í„°
            y_train: í•™ìŠµ ë¼ë²¨
        """
        # ê° ëª¨ë¸ì˜ ì ìˆ˜ ìˆ˜ì§‘
        scores_list = []
        for name, model in self.models.items():
            try:
                score = model.decision_function(X_train)
                # ì ìˆ˜ ì •ê·œí™”
                if score.min() < 0:
                    score = -score
                score = (score - score.min()) / (score.max() - score.min() + 1e-8)
                scores_list.append(score)
            except Exception as e:
                print(f"âš ï¸ ëª¨ë¸ {name} ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
                continue
        
        if not scores_list:
            raise ValueError("ëª¨ë“  ëª¨ë¸ì˜ ì ìˆ˜ ê³„ì‚°ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # íŠ¹ì§• í–‰ë ¬ ìƒì„±
        X_meta = np.column_stack(scores_list)
        
        # ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
        self.meta_model = LogisticRegression(random_state=42, max_iter=1000)
        self.meta_model.fit(X_meta, y_train)
        
        print(f"âœ… ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (íŠ¹ì§• ìˆ˜: {X_meta.shape[1]})")
    
    def evaluate(self, X_test, y_test):
        """
        ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        
        Args:
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            y_test: í…ŒìŠ¤íŠ¸ ë¼ë²¨
        
        Returns:
            ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        y_pred = self.predict(X_test)
        y_scores = self.decision_function(X_test)
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        try:
            roc_auc = roc_auc_score(y_test, y_scores)
        except:
            roc_auc = None
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'y_pred': y_pred,
            'y_scores': y_scores
        }


def compare_models_and_ensemble(
    comparator,
    X_train, y_train, X_test, y_test,
    selected_models=None,
    ensemble_methods=['majority', 'weighted']
):
    """
    ëª¨ë¸ ë¹„êµ ë° ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
    
    Args:
        comparator: LogAnomalyModelComparator ê°ì²´
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        selected_models: ì„ íƒí•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        ensemble_methods: ì•™ìƒë¸” ë°©ë²• ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # 1. ê°œë³„ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    if selected_models:
        comparator.train_models(X_train, selected_models=selected_models)
    else:
        comparator.train_models(X_train)
    
    individual_results = comparator.evaluate_models(X_test, y_test)
    
    # 2. ì•™ìƒë¸” ìƒì„± ë° í‰ê°€
    ensemble_results = {}
    
    for method in ensemble_methods:
        print(f"\n{'='*60}")
        print(f"ì•™ìƒë¸” ë°©ë²•: {method}")
        print(f"{'='*60}")
        
        try:
            if method == 'stacking':
                # ìŠ¤íƒœí‚¹ì€ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ í•„ìš”
                ensemble = EnsembleAnomalyDetector(
                    models_dict=comparator.trained_models,
                    method=method
                )
                ensemble.fit_meta_model(X_train, y_train)
            elif method == 'weighted':
                # ê°€ì¤‘ì¹˜ëŠ” ê°œë³„ ëª¨ë¸ì˜ F1 ì ìˆ˜ ê¸°ë°˜
                weights = {}
                total_f1 = sum(metrics['f1_score'] for metrics in individual_results.values())
                for name, metrics in individual_results.items():
                    weights[name] = metrics['f1_score'] / total_f1 if total_f1 > 0 else 1.0 / len(individual_results)
                
                ensemble = EnsembleAnomalyDetector(
                    models_dict=comparator.trained_models,
                    method=method,
                    weights=weights
                )
                print(f"ê°€ì¤‘ì¹˜: {weights}")
            else:
                ensemble = EnsembleAnomalyDetector(
                    models_dict=comparator.trained_models,
                    method=method
                )
            
            # ì•™ìƒë¸” í‰ê°€
            ensemble_metrics = ensemble.evaluate(X_test, y_test)
            ensemble_results[method] = ensemble_metrics
            
            print(f"ì •í™•ë„: {ensemble_metrics['accuracy']:.4f}")
            print(f"ì •ë°€ë„: {ensemble_metrics['precision']:.4f}")
            print(f"ì¬í˜„ìœ¨: {ensemble_metrics['recall']:.4f}")
            print(f"F1 ì ìˆ˜: {ensemble_metrics['f1_score']:.4f}")
            if ensemble_metrics['roc_auc']:
                print(f"ROC-AUC: {ensemble_metrics['roc_auc']:.4f}")
        
        except Exception as e:
            print(f"âš ï¸ ì•™ìƒë¸” {method} ì‹¤íŒ¨: {e}")
            continue
    
    # 3. ì¢…í•© ë¹„êµ
    print(f"\n{'='*60}")
    print("ì¢…í•© ë¹„êµ")
    print(f"{'='*60}")
    
    comparison_data = []
    
    # ê°œë³„ ëª¨ë¸
    for name, metrics in individual_results.items():
        comparison_data.append({
            'ëª¨ë¸': name,
            'ë°©ë²•': 'ê°œë³„',
            'ì •í™•ë„': f"{metrics['accuracy']:.4f}",
            'F1 ì ìˆ˜': f"{metrics['f1_score']:.4f}",
            'ì¬í˜„ìœ¨': f"{metrics['recall']:.4f}",
            'ì •ë°€ë„': f"{metrics['precision']:.4f}",
        })
    
    # ì•™ìƒë¸”
    for method, metrics in ensemble_results.items():
        comparison_data.append({
            'ëª¨ë¸': f'ì•™ìƒë¸” ({method})',
            'ë°©ë²•': 'ì•™ìƒë¸”',
            'ì •í™•ë„': f"{metrics['accuracy']:.4f}",
            'F1 ì ìˆ˜': f"{metrics['f1_score']:.4f}",
            'ì¬í˜„ìœ¨': f"{metrics['recall']:.4f}",
            'ì •ë°€ë„': f"{metrics['precision']:.4f}",
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    print(comparison_df.to_string(index=False))
    
    # ìµœê³  ì„±ëŠ¥
    all_results = {**individual_results, **{f'ì•™ìƒë¸”_{k}': v for k, v in ensemble_results.items()}}
    best_f1 = max(all_results.items(), key=lambda x: x[1]['f1_score'])
    print(f"\nğŸ† ìµœê³  F1 ì ìˆ˜: {best_f1[0]} ({best_f1[1]['f1_score']:.4f})")
    
    return {
        'individual': individual_results,
        'ensemble': ensemble_results,
        'comparison': comparison_df
    }














