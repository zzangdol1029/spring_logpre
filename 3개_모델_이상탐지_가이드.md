# 3ê°œ ëª¨ë¸ ì´ìƒ íƒì§€ ê°€ì´ë“œ

## ê°œìš”

prelog í”„ë¡œì íŠ¸ì—ì„œëŠ” **3ê°œì˜ ë¡œê·¸ íŠ¹í™” ì´ìƒ íƒì§€ ëª¨ë¸**ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤:
1. **DeepLog**: LSTM ê¸°ë°˜ ì‹¤í–‰ ê²½ë¡œ ì˜ˆì¸¡
2. **LogAnomaly**: Template2Vec ê¸°ë°˜ ì˜ë¯¸ ë²¡í„°í™”
3. **LogRobust**: Attention + Bi-LSTM

## ë°©ë²• 1: í•™ìŠµ ë° í‰ê°€ ë™ì‹œ ìˆ˜í–‰ (ê¶Œì¥)

`log_specific_model_comparison.py`ë¥¼ ì‹¤í–‰í•˜ë©´ 3ê°œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.

### ì‹¤í–‰ ë°©ë²•

```bash
cd /Users/zzangdol/PycharmProjects/zzangdol/pattern/prelog

python log_specific_model_comparison.py \
    --load-split split_data \
    --optimize-threshold \
    --sample-size 500000 \
    --epochs 20 \
    --batch-size 64
```

### ê²°ê³¼ í™•ì¸

ì‹¤í–‰ ì™„ë£Œ í›„ `results/log_specific_comparison_*/` í´ë”ì— ë‹¤ìŒ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
- `log_specific_model_comparison.csv`: 3ê°œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- `results_deeplog.csv`: DeepLog íƒì§€ ê²°ê³¼
- `results_loganomaly.csv`: LogAnomaly íƒì§€ ê²°ê³¼
- `results_logrobust.csv`: LogRobust íƒì§€ ê²°ê³¼
- `severity_deeplog.csv`: DeepLog ì‹¬ê°ë„ ë¶„ì„
- `severity_loganomaly.csv`: LogAnomaly ì‹¬ê°ë„ ë¶„ì„
- `comparison_report.txt`: ìƒì„¸ ë¦¬í¬íŠ¸

## ë°©ë²• 2: ì§ì ‘ ì½”ë“œ ì‘ì„±

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from log_specific_anomaly_detectors import LogSpecificAnomalySystem
from log_anomaly_detector import SpringBootLogParser

# 1. ë¡œê·¸ íŒŒì‹±
parser = SpringBootLogParser()
logs_df = parser.parse_directory("logs/backup")

# 2. ê° ëª¨ë¸ í•™ìŠµ
systems = {}
for model_type in ['deeplog', 'loganomaly', 'logrobust']:
    print(f"\n[{model_type.upper()}] ëª¨ë¸ í•™ìŠµ ì¤‘...")
    system = LogSpecificAnomalySystem(model_type=model_type)
    system.load_logs(logs_df)
    system.train(epochs=20, batch_size=64)
    systems[model_type] = system

# 3. ìƒˆë¡œìš´ ë¡œê·¸ì— ëŒ€í•´ ì´ìƒ íƒì§€
new_logs_df = parser.parse_log_file("new_log.log")

results = {}
for model_type, system in systems.items():
    print(f"\n[{model_type.upper()}] ì´ìƒ íƒì§€ ì¤‘...")
    detection_results = system.detect_anomalies(new_logs_df)
    results[model_type] = detection_results
    
    if detection_results and not detection_results.get('anomalies', pd.DataFrame()).empty:
        anomalies_df = detection_results['anomalies']
        summary = detection_results.get('summary', {})
        
        print(f"   âœ… {len(anomalies_df)}ê°œ ì´ìƒ ì‹œí€€ìŠ¤ íƒì§€")
        print(f"   ì‹¬ê°ë„ ë¶„í¬: {summary.get('by_severity', {})}")
    else:
        print(f"   âœ… ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
```

### ê²°ê³¼ ë¶„ì„

```python
# ê° ëª¨ë¸ë³„ íƒì§€ ê²°ê³¼ ë¹„êµ
for model_type, result in results.items():
    anomalies_df = result.get('anomalies', pd.DataFrame())
    summary = result.get('summary', {})
    
    print(f"\n[{model_type.upper()}]")
    print(f"   íƒì§€ëœ ì´ìƒ: {len(anomalies_df)}ê°œ")
    print(f"   ì‹¬ê°ë„ ë¶„í¬: {summary.get('by_severity', {})}")
    
    # ì‹¬ê°ë„ê°€ ë†’ì€ ì´ìƒë§Œ í•„í„°ë§
    if not anomalies_df.empty:
        critical_anomalies = anomalies_df[
            anomalies_df['max_severity_level'].isin(['CRITICAL', 'HIGH'])
        ]
        print(f"   CRITICAL/HIGH: {len(critical_anomalies)}ê°œ")

# ê³µí†µìœ¼ë¡œ íƒì§€ëœ ì´ìƒ ì°¾ê¸°
all_anomalies = {}
for model_type, result in results.items():
    anomalies_df = result.get('anomalies', pd.DataFrame())
    if not anomalies_df.empty and 'sequence_index' in anomalies_df.columns:
        detected_indices = set(anomalies_df['sequence_index'].values)
        all_anomalies[model_type] = detected_indices

# 2ê°œ ì´ìƒ ëª¨ë¸ì´ ê³µí†µìœ¼ë¡œ íƒì§€í•œ ì‹œí€€ìŠ¤
if len(all_anomalies) >= 2:
    common_indices = set.intersection(*all_anomalies.values())
    print(f"\nê³µí†µ íƒì§€: {len(common_indices)}ê°œ ì‹œí€€ìŠ¤")
```

## ë°©ë²• 3: í•™ìŠµëœ ëª¨ë¸ ì¬ì‚¬ìš©

í˜„ì¬ëŠ” ëª¨ë¸ ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ, í•™ìŠµê³¼ íƒì§€ë¥¼ ê°™ì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

```python
import pandas as pd
from log_specific_anomaly_detectors import LogSpecificAnomalySystem
from log_anomaly_detector import SpringBootLogParser

# 1. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
parser = SpringBootLogParser()
train_logs_df = parser.parse_directory("logs/backup/training")

# 2. 3ê°œ ëª¨ë¸ í•™ìŠµ
systems = {}
for model_type in ['deeplog', 'loganomaly', 'logrobust']:
    system = LogSpecificAnomalySystem(model_type=model_type)
    system.load_logs(train_logs_df)
    system.train(epochs=20, batch_size=64)
    systems[model_type] = system

# 3. ìƒˆë¡œìš´ ë¡œê·¸ì— ëŒ€í•´ ì´ìƒ íƒì§€
new_logs_df = parser.parse_directory("logs/backup/new")

all_results = {}
for model_type, system in systems.items():
    results = system.detect_anomalies(new_logs_df)
    all_results[model_type] = results

# 4. ê²°ê³¼ ì €ì¥
output_dir = "results/detection"
os.makedirs(output_dir, exist_ok=True)

for model_type, result in all_results.items():
    anomalies_df = result.get('anomalies', pd.DataFrame())
    if not anomalies_df.empty:
        output_path = os.path.join(output_dir, f"{model_type}_anomalies.csv")
        anomalies_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ {model_type.upper()} ê²°ê³¼ ì €ì¥: {output_path}")
```

## ëª¨ë¸ë³„ íŠ¹ì§•

### DeepLog
- **ì¥ì **: ì¬í˜„ìœ¨ì´ ë†’ìŒ (ê±°ì˜ ëª¨ë“  ì´ìƒ íƒì§€)
- **ë‹¨ì **: ì˜¤íƒì´ ë§ìŒ (False Positive ë§ìŒ)
- **ì ìš©**: ì´ìƒì„ ë†“ì¹˜ë©´ ì•ˆ ë˜ëŠ” ê²½ìš°

### LogAnomaly
- **ì¥ì **: ì •í™•ë„ê°€ ë†’ìŒ, ì •ìƒì„ ì˜ êµ¬ë¶„
- **ë‹¨ì **: ì´ìƒ íƒì§€ìœ¨ì´ ë‚®ìŒ (False Negative ë§ìŒ)
- **ì ìš©**: ì˜¤íƒì„ ìµœì†Œí™”í•´ì•¼ í•˜ëŠ” ê²½ìš°

### LogRobust
- **ì¥ì **: ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
- **ë‹¨ì **: í•™ìŠµ ì‹œê°„ì´ ê¸¸ê³ , ê¸°ë³¸ ì„ê³„ê°’ì—ì„œ ì‹¤íŒ¨ ê°€ëŠ¥
- **ì ìš©**: ë³µì¡í•œ ì´ìƒ íŒ¨í„´ íƒì§€ê°€ í•„ìš”í•œ ê²½ìš°

## ì•™ìƒë¸” ë°©ë²•

3ê°œ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ë” ì •í™•í•œ íƒì§€ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ë°©ë²• 1: íˆ¬í‘œ ë°©ì‹ (2ê°œ ì´ìƒ ëª¨ë¸ì´ íƒì§€í•˜ë©´ ì´ìƒìœ¼ë¡œ íŒë‹¨)
def ensemble_vote(results):
    all_anomalies = {}
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        if not anomalies_df.empty and 'sequence_index' in anomalies_df.columns:
            all_anomalies[model_type] = set(anomalies_df['sequence_index'].values)
    
    # 2ê°œ ì´ìƒ ëª¨ë¸ì´ íƒì§€í•œ ì‹œí€€ìŠ¤
    from collections import Counter
    all_indices = []
    for indices in all_anomalies.values():
        all_indices.extend(indices)
    
    index_counts = Counter(all_indices)
    consensus_indices = [idx for idx, count in index_counts.items() if count >= 2]
    
    return consensus_indices

# ë°©ë²• 2: ì‹¬ê°ë„ ê¸°ë°˜ ìš°ì„ ìˆœìœ„
def ensemble_by_severity(results):
    all_anomalies = []
    for model_type, result in results.items():
        anomalies_df = result.get('anomalies', pd.DataFrame())
        if not anomalies_df.empty:
            for _, row in anomalies_df.iterrows():
                all_anomalies.append({
                    'model': model_type,
                    'sequence_index': row['sequence_index'],
                    'severity': row.get('max_severity_level', 'LOW'),
                    'score': row.get('anomaly_score', 0)
                })
    
    # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    severity_order = {'CRITICAL': 4, 'HIGH': 3, 'MEDIUM': 2, 'LOW': 1}
    all_anomalies.sort(key=lambda x: (
        severity_order.get(x['severity'], 0),
        x['score']
    ), reverse=True)
    
    return all_anomalies
```

## ì£¼ì˜ì‚¬í•­

1. **ëª¨ë¸ ì €ì¥/ë¡œë“œ**: í˜„ì¬ëŠ” ëª¨ë¸ ì €ì¥ ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ, í•™ìŠµê³¼ íƒì§€ë¥¼ ê°™ì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

2. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 3ê°œ ëª¨ë¸ì„ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤.

3. **ì‹¤í–‰ ì‹œê°„**: 3ê°œ ëª¨ë¸ì„ ëª¨ë‘ í•™ìŠµí•˜ê³  íƒì§€í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.

4. **ì„ê³„ê°’ ì¡°ì •**: ê° ëª¨ë¸ì˜ ì„ê³„ê°’ì„ ì¡°ì •í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì°¸ê³ 

- `log_specific_model_comparison.py`: 3ê°œ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
- `log_specific_anomaly_detectors.py`: ëª¨ë¸ êµ¬í˜„
- `detect_anomalies_3models.py`: ì´ìƒ íƒì§€ ìŠ¤í¬ë¦½íŠ¸ (ì‘ì„± ì¤‘)



